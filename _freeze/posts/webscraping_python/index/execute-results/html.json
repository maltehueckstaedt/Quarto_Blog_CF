{
  "hash": "485fe5f68fa9c7315cc6add82c9ccda7",
  "result": {
    "markdown": "---\ntitle: \"Webscraping with Python\"\nauthor: \"Malte Hückstädt\"\ndate: \"2023-03-30\"\ncategories: [Python, web-scraping]\nimage: \"image.jpg\"\nbibliography: Bibliothek.bib\n---\n\n# Introduction\n\nWeb scraping refers to the process of extracting, copying and storing web content or source code sections with the aim of analysing or otherwise exploiting them. With the help of web scraping procedures, many different types of information can be collected and analysed automatically: For example, contact, network or behavioural data. Web scraping is no longer a novelty in empirical research. There are numerous reasons why web scraping methods are used in empirical research: For example, repeated collection of cross-sectional web data, the desire for replicability of data collection processes or the desire for time-efficient and less error-prone collection of online data [@munzertNutzungWebdatenSozialwissenschaften2019].\n\n# Web scraping with Python\n\nAs a low-threshold introduction to webscraping, we want to use [Python](https://www.python.org/) to collect various online-accessible articles from the art magazine [_Monopol_](https://www.monopol-magazin.de/) from the section _Art Market_. For this we need the Pythons libaries `BeautifulSoup`, `requests`, `pandas` and for the later backup of our collected data the library `openpyxl`. Since I myself use [RStudio](https://www.rstudio.com/) as IDE ((Integrated Development Environment)) for Python, I load the R package `reticulate` in an R environment and turn off the (to me annoying) notification function of the package. Finally, I use the function `use_python()` to tell R under which path my Python binary can be found. Users who work directly in Python can skip this step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\noptions(reticulate.repl.quiet = TRUE)\nuse_python(\"~/Library/r-miniconda/bin/python\")\n```\n:::\n\n\nIn a further step, the Python libaries mentioned above are loaded into the working environment. If the libraries have not yet been installed, this can be done from R with the `reticulate` function `py_install()`. In Python, libraries can be installed directly with the function `pip install`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#py_install(\"pandas\")\n#py_install(\"bs4\")\n#py_install(\"openpyxl\")\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport openpyxl\n```\n:::\n\n\nOnce this is done, we define the URL where the links to the ten newest articles of the section \"Art Market\" are stored. The defined URL are fed to the function `requests.get()`, which should display the status code 200: HTTP status code 200 indicates that the request was successfully processed. All requested data has been located on the web server and is being transferred to the client\n\n\n::: {.cell}\n\n```{.python .cell-code}\n#load url\nurl = \"https://www.monopol-magazin.de/ressort/kunstmarkt\"\nres = requests.get(url)\nprint(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Response [200]>\n```\n:::\n:::\n\nIf we pass the object `text` from the list `res` generated by the `requests.get()` function to the BeautifulSoup function, we get a BeautifulSoup object that transforms the HTML document into an easily readable nested data structure.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsoup = BeautifulSoup(res.text)\n```\n:::\n\nWith the help of the [selector gadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=de) we select the CSS selector that leads to the URLs of the first ten articles of the _art market_ section. In this case it is: `#block-kf-bootstrap-content header`.\n\n::: {.cell}\n\n```{.python .cell-code}\narticles = soup.select(\"#block-kf-bootstrap-content header\")\n```\n:::\n\nFrom the list `articles` created in this way, all titles of the ten articles are now extracted as pure text elements with the help of a for-loop and the list `articles_titles` is written.\n\n\n::: {.cell}\n\n```{.python .cell-code}\narticles_titles = [i.text for i in articles]\n```\n:::\n\nFurthermore, another for-loop iterates over the `soup` object to extract the links to the full texts of the ten articles as simple text elements. An empty list called `articles_urls` is created in which all partial URLs are written that can be found in the HTML elements of the class `article__title` and there in the HTML element `a`. Since only the respective paths are stored in the `a` elements of the HTML code, but not the main domain of the URL, this is finally merged with the paths to form a single string.\n\n\n::: {.cell}\n\n```{.python .cell-code}\narticles_urls = []\nfor article__title in soup.find_all(attrs={'class':'article__title'}):\n  for link in article__title.find_all('a'):\n    url = \"https://www.monopol-magazin.de\"+link.get('href')\n    articles_urls.append(url)\ntype(articles_urls)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'list'>\n```\n:::\n:::\n\n\nNow the text elements of the ten articles can be collected with a for-loop. Again, an empty list is created, this time with the name `data`. The for-loop iterates over the ten URLs extracted above that are in the list `articles_urls`. The `requests.get()` function is called first on each of the ten URLs. If the respective web page of the articles of the status code reflects 200, we create - as above - a soup object with the function `BeautifulSoup()`, which puts the HTML code of the respective page into a clear structure for us. Then, with the help of the [selector gadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=de), we select the CSS selector that leads to the articles of each section of the _art market_. In this case it is: `.field--type-text-long.field--label-hidden`. Finally, the collected text elements are adapted to the unicode standard using the function `normalize` of the library `unicodedata` and a for-loop. This is done with the help of the _NFKD_ algorithm.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndata = []\n\nfor url in articles_urls:\n    res = requests.get(url)\n    if res.status_code == 200:\n        soup = BeautifulSoup(res.text)\n        for element in soup.select('.field--type-text-long.field--label-hidden'):\n            data.append((element.get_text()))\n\n#clean text\nimport unicodedata\ndata = [unicodedata.normalize(\"NFKD\",i) for i in data]\n```\n:::\n\n\nVoilà! As we could see, our code created _little spiders_ that roam the URLs we specified and reliably collect all the information we instructed them to collect in our code. In this way, the _little spiders_ that we brought to life with a few lines of Python code can do the work in seconds with an accuracy and speed that would take us humans a quarter of an hour.\n\nWe now merge the three lists with the titles of the articles, the URLs and the actual text corpus first into a list called `d` and then into a data-frame called `df`. Furthermore, we use the `replace()` function and the `strip()` function to remove all line breaks and whitespace from all columns of the type `string`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndata_sub = data[0:10]\nd = {'titles':articles_titles,'urls':articles_urls,'texts':data_sub}\ndf = pd.DataFrame(d)\n\ndf[df.columns] = df.apply(lambda x: x.str.replace(\"\\n\", \"\"))\ndf[df.columns] = df.apply(lambda x: x.str.strip())\n\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                              titles  ...                                              texts\n0  Messetipps      7 Stände, die sich auf der Art...  ...  Auf der Art Düsseldorf zeigen 95 Galerien noc...\n1  Leitplanken und Uecker-Nägel      Art Düsseldo...  ...           Zeller van Almsick, Art Düsseldorf 2023\n2  Abschied von Marian Goodman      Künstlerin Na...  ...  Copperfield GalleryAuf einer Messe, wo leicht ...\n3  Kunstmesse      Art Düsseldorf feiert fünfte A...  ...  Stand der Copperfield Gallery aus London auf d...\n4                Art Düsseldorf      Gut aufgestellt  ...  Rebekka Benzenberg und Monika Grabuschnigg bei...\n5  Talkreihe auf der Art Düsseldorf      Wie könn...  ...  Stand von Anton Janizewski auf der Art Düssel...\n6  Auktion in Kalifornien      Banksy-Werk bringt...  ...  Inge Mahn bei Kadel WillbornEine gelungene Ins...\n7  Auktion in Köln      Richter-Bild für halbe Mi...  ...  Stand von Kadel Willborn auf der Art Düsseldo...\n8  Kunstmesse Art Düsseldorf      Extrarunde in d...  ...  Alwin Lay bei Koenig2 by Robby GreifEin Kaffee...\n9  Eröffnung der Grove-Galerie in Berlin      \"Ge...  ...  Stand von Koenig2 by Robby Grief c/o Galerie C...\n\n[10 rows x 3 columns]\n```\n:::\n:::\n\nFinally, we export the data-frame created in this way into the `.xlsx` format with the function `ExcelWriter()`. To do this, we define the desired output-format, write the object `df` into an Excel sheet and export this with the function `writer.save()`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwriter = pd.ExcelWriter('output.xlsx')\ndf.to_excel(writer)\nwriter.save()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<string>:1: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n```\n:::\n:::\n\nThere are many possibilities for analysing the data obtained, some of which will be presented here in the medium term.\n\n# References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}