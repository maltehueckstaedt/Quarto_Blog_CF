[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Malte. I was born in 1983 in Schwedt/Oder. I studied fine art in Hannover (FH Hannover) and Berlin (KH-Berlin) from 2004 to 2009 and sociology in Berlin (HU Berlin) and in Bielefeld (Bielefeld University) from 2014 to 2018. I have been working at the German Centre for Science and Higher Education Research (DZHW) since 2018 in various research projects and contexts as a (Social) Data Scientist.\nMy main research interests are programming with R and Python, computational social science, machine learning, research collaborations, interdisciplinarity and sociology of art and culture."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Malte Hückstädt was born in Schwedt/Oder in 1983. He studied fine arts in Hanover (FH Hanover) and Berlin (KH-Berlin) from 2004 to 2009, social sciences in Berlin (HU Berlin) from 2014 to 2016 and sociology with a quantitative-methodological profile in Bielefeld (Bielefeld University) from 2016 to 2018.\nFrom November 2018 to January 2019, Malte Hückstädt worked at the Research Data Centre of the German Centre for Science and Higher Education Research (DZHW) on the preparation of qualitative data for the creation of an SUF. Since February 2019, he has been working in the Governance in Science and Higher Education department in the project Determinants and Effects of Collaboration in Research Collaborations (DEKiF). There he is responsible for the preparation, implementation and analysis of the large-scale web-survey carried out as part of the project."
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "About",
    "section": "2023",
    "text": "2023\nHückstädt, M. (2023). ‘Ten reasons why research collaborations succeed—a random forest approach’, Scientometrics, 128/3: 1923–50. DOI: https://doi.org/10.1007/s11192-022-04629-7\nWeinmann, C., Hückstädt, M., Meißner, F., & Vowe, G. (2023). ‘How do researchers perceive problems in research collaboration? Results from a large-scale study of German scientists’, Frontiers in Research Metrics and Analytics, 8. DOI: https://doi.org/10.3389/frma.2023.1106482\nHückstädt, M., Leisten, L.M. (2023). Input, process, output - inhibitors and promoters of collaboration Problems (under Review)."
  },
  {
    "objectID": "about.html#section-1",
    "href": "about.html#section-1",
    "title": "About",
    "section": "2022",
    "text": "2022\nHückstädt, M. (2022). ‘Coopetition between frenemies–interrelations and effects of seven collaboration problems in research clusters’, Scientometrics, 127/9: 5191–224. DOI: 10.1007/s11192-022-04472-w. DOI: https://doi.org/10.1007/s11192-022-04472-w"
  },
  {
    "objectID": "about.html#section-2",
    "href": "about.html#section-2",
    "title": "About",
    "section": "2021",
    "text": "2021\nKleimann, B., & Hückstädt, M. (2021). Selection criteria in professorial recruiting as indicators of institutional similarity? A comparison of German universities and universities of applied sciences.Quality in Higher Education (online first). https://doi.org/10.1080/13538322.2021.1889760"
  },
  {
    "objectID": "about.html#section-3",
    "href": "about.html#section-3",
    "title": "About",
    "section": "2019",
    "text": "2019\nKleimann, B., İkiz-Akıncı, D., & Hückstädt, M. (2019). Leistungsbewertung in Berufungsverfahren. Traditionswandel in der akademischen Personalselektion. Daten- und Methodenbericht zur qualitativen Erhebung der DZHW-Studie LiBerTas 2016. Version 1.0.0. Hannover: fdz.DZHW.\nHückstädt, M. (2019). Determinanten der subjektiv wahrgenommenen Wichtigkeit von Hochschulrankings bei der Studienortwahl? In L. Ringel & T. Werron (Hrsg.), Rankings - Soziologische Fallstudien (S. 181-202). Wiesbaden: Springer. https://doi.org/10.1007/978-3-658-26366-9_8"
  },
  {
    "objectID": "about.html#section-4",
    "href": "about.html#section-4",
    "title": "About",
    "section": "2018",
    "text": "2018\nKleimann, B., & Hückstädt, M. (2018). Auswahlkriterien in Berufungsverfahren: Universitäten und Fachhochschulen im Vergleich. Beiträge zur Hochschulforschung, 40 (2/2018), 20-47."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Hückstädt, M. (2023). ‘Ten reasons why research collaborations succeed—a random forest approach’, Scientometrics, 128/3: 1923–50. DOI: https://doi.org/10.1007/s11192-022-04629-7\nWeinmann, C., Hückstädt, M., Meißner, F., & Vowe, G. (2023). ‘How do researchers perceive problems in research collaboration? Results from a large-scale study of German scientists’, Frontiers in Research Metrics and Analytics, 8. DOI: https://doi.org/10.3389/frma.2023.1106482\nHückstädt, M., Leisten, L.M. (2023). Input, process, output - inhibitors and promoters of collaboration Problems (under Review).\nHückstädt, M. (2022). ‘Coopetition between frenemies–interrelations and effects of seven collaboration problems in research clusters’, Scientometrics, 127/9: 5191–224. DOI: 10.1007/s11192-022-04472-w. DOI: https://doi.org/10.1007/s11192-022-04472-w\nKleimann, B., & Hückstädt, M. (2021). Selection criteria in professorial recruiting as indicators of institutional similarity? A comparison of German universities and universities of applied sciences.Quality in Higher Education (online first). https://doi.org/10.1080/13538322.2021.1889760\nKleimann, B., & Hückstädt, M. (2018). Auswahlkriterien in Berufungsverfahren: Universitäten und Fachhochschulen im Vergleich. Beiträge zur Hochschulforschung, 40 (2/2018), 20-47."
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "2018",
    "text": "2018\nKleimann, B., & Hückstädt, M. (2018). Auswahlkriterien in Berufungsverfahren: Universitäten und Fachhochschulen im Vergleich. Beiträge zur Hochschulforschung, 40 (2/2018), 20-47."
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "2021",
    "text": "2021\nKleimann, B., & Hückstädt, M. (2021). Selection criteria in professorial recruiting as indicators of institutional similarity? A comparison of German universities and universities of applied sciences.Quality in Higher Education (online first). https://doi.org/10.1080/13538322.2021.1889760"
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications",
    "section": "2019",
    "text": "2019\nKleimann, B., İkiz-Akıncı, D., & Hückstädt, M. (2019). Leistungsbewertung in Berufungsverfahren. Traditionswandel in der akademischen Personalselektion. Daten- und Methodenbericht zur qualitativen Erhebung der DZHW-Studie LiBerTas 2016. Version 1.0.0. Hannover: fdz.DZHW.\nHückstädt, M. (2019). Determinanten der subjektiv wahrgenommenen Wichtigkeit von Hochschulrankings bei der Studienortwahl? In L. Ringel & T. Werron (Hrsg.), Rankings - Soziologische Fallstudien (S. 181-202). Wiesbaden: Springer. https://doi.org/10.1007/978-3-658-26366-9_8"
  },
  {
    "objectID": "publications.html#section-4",
    "href": "publications.html#section-4",
    "title": "Publications",
    "section": "2018",
    "text": "2018\nKleimann, B., & Hückstädt, M. (2018). Auswahlkriterien in Berufungsverfahren: Universitäten und Fachhochschulen im Vergleich. Beiträge zur Hochschulforschung, 40 (2/2018), 20-47."
  },
  {
    "objectID": "3d.html",
    "href": "3d.html",
    "title": "3D Modelling",
    "section": "",
    "text": "Sieh dir diesen Beitrag auf Instagram an\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEin Beitrag geteilt von Chernoff Faces (@chernoff_faces)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSieh dir diesen Beitrag auf Instagram an\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEin Beitrag geteilt von Chernoff Faces (@chernoff_faces)"
  },
  {
    "objectID": "acomplishments.html",
    "href": "acomplishments.html",
    "title": "Acomplishments",
    "section": "",
    "text": "As a data enthusiast, I continue to educate myself. Most of the knowledge I have learned in the field of data science in recent years has been acquired informally through practical applications or trail and error. However, when the opportunity arises, I also like to take part in training courses, either digital or analogue. In the following you will find my certified trainings in decreasing order of actuality.\n\n\n\n\n\n\n\nOrganiser\n\nMode\nTopic\nDate\nCertificate\n\n\n\n\n\nJohannes Gutenberg-Universität Mainz\nmanuel\nWeb Scraping und Data Mining mit Python\n3/22\nCertifcate\n\n\n\nDataCamp\nonline\nWeb Scraping in Python\n3/22\nCertifcate\n\n\n\nDataCamp\nonline\nIntroduction to\nPython\n4/21\nCertifcate\n\n\n\nDataCamp\nonline\nIntermediate Python\n4/21\nCertifcate\n\n\n\nGESIS\nmanuel\nOnline-Befragungen – Planung\nund Durchführung\n12/19\nCertifcate\n\n\n\nGESIS\nmanuel\nEinführung in die Logik der\nBayesschen Statistik\n11/19\nCertifcate\n\n\n\nDataCamp\nonline\nCleaning Data in R\n5/19\nCertifcate\n\n\n\nDataCamp\nonline\nIntroduction to R\n5/19\nCertifcate\n\n\n\nHumboldt Universität zu Berlin\nmanuel\nDatenanalyse mit Stata\n12/15\nCertifcate"
  },
  {
    "objectID": "acomplishments.html#web-scraping-und-data-mining-mit-python",
    "href": "acomplishments.html#web-scraping-und-data-mining-mit-python",
    "title": "Acomplishments",
    "section": "Web Scraping und Data Mining mit Python",
    "text": "Web Scraping und Data Mining mit Python\nJGU-Mainz Mar 2022\n\n\n\ncaption"
  },
  {
    "objectID": "experience.html",
    "href": "experience.html",
    "title": "Experience",
    "section": "",
    "text": "Employer:\nGerman Centre for Higher Education Research and Science Studies (DZHW)\n\n\nPeriod:\nSince 06/2022\n\n\nPlace:\nHanover, DE\n\n\nRole:\nResearch Associate\n\n\nProject:\nHierarchies in Cooperative Research (third-party funding-application preparation, DFG, Individual Research Grants)\n\n\nProject content:\nHierarchies have the potential to both enhance and effectiveness of research teams. The research project Hierarchies in Collaborative Research aims to identify the determinants of hierarchies in collaborative research. research, the determinants of whether, why and when different forms of and when different forms of hierarchy can impair the effectiveness of teams. effectiveness of teams (mediated by cooperation problems). impair the effectiveness of teams. The data basis for the study will be the survey data of data from staff members of Collaborative Research Centres and Excellence clusters will be analysed.\n\n\nResponsibilities:\nPhase 1: Preparation of the proposal\n\nDevelopment of a project idea\nDevelopment of a short concept\nReview of the international state of research\nDevelopment of a project concept\n\nPhase 2: Writing the proposal\n\nPresentation of the initial situation\nPresentation of the work programme\nPresentation of the state of research\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmployer:\nGerman Centre for Higher Education Research and Science Studies (DZHW)\n\n\nPeriod:\n01/2022 – 05/2022\n\n\nPlace:\nHanover, DE\n\n\nRole:\nResearch Associate\n\n\nProject:\nDomain Data Protocols for Empirical Educational Research a Contribution to Standardizing and Increasing the Quality of Research Data Management (DDP)\n\n\nProject content:\nThe collaborative project DDP aimed to develop so-called domain data protocols for handling research data in educational research. Domain data protocols are public and referencable sample standard protocols for data management. They are intended to support researchers in empirical educational research in producing quality-assured and re-usable data.\n\n\nResponsibilities:\n\nProofreading of domain data protocols\nPretests von Domain-Data-Protokollen\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmployer:\nGerman Centre for Higher Education Research and Science Studies (DZHW)\n\n\nPeriod:\n02/2019 – 05/2022\n\n\nPlace:\nHanover, DE\n\n\nRole:\nResearch Associate\n\n\nProject:\nDeterminants and effects of cooperation in homogeneous and heterogeneous research clusters (DEKiF)\n\n\nProject content:\nThe joint project DEKiF investigated the question of which internal cooperation problems occur in (inter-)disciplinary research clusters, which causes and framework conditions are decisive for this and how cooperation problems affect the success of research clusters. At the same time, it was determined which strategies research clusters use to solve or prevent problems that arise.\n\n\nResponsibilities:\n\nNarrowing down and reviewing the state of international research\nIdentification of relevant research gaps\nDevelopment and concretisation of research questions and hypothesis\nPreparation, implementation and evaluation of a large-scale web survey (survey of approx. 15,000 researchers participating in DFG research clusters)\nProgramming and chaining of R-functions for the automated collection of contact data from the target persons of the DEKiF web survey by means of web scraping\nDevelopment of measurement instruments\nCreation of programming templates\nPreparation and implementation of pre-tests\nImplementation and supervision of the field phase\nCleaning and preparation of survey- and metadata\nConducting survey error analyses\nCarrying out descriptive, explorative and advanced, inference statistical analyses\nPreparation of methodological reports\nAnonymisation and preparation of the survey- and metadata for publication of a scientific use file\nPreparation and publication of technical papers\nCompilation and framing of technical papers for a cumulative dissertation publication\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmployer:\nGerman Centre for Higher Education Research and Science Studies (DZHW)\n\n\nPeriod:\n11/2018 – 01/2019\n\n\nPlace:\nHanover, DE\n\n\nRole:\nResearch Associate\n\n\nProject:\nContinuous operation of the Research Data Centre for Higher Education and Science Research (FDZ-DZHW)\n\n\nResponsibilities:\nEditing and anonymisation of guided interview transcripts of the research project Performance Evaluation in Appointment Procedures - the changing tradition of hiring professors in academia (LiBerTas)\n\n\nPeriod:\n10/2010 – 09/2012\n\n\nRole:\nFreelance artist and graphic designer"
  },
  {
    "objectID": "posts/chernoff_faces/index.html",
    "href": "posts/chernoff_faces/index.html",
    "title": "Chernoff Faces",
    "section": "",
    "text": "The Chernoff-Faces (Chernoff 1973) developed by Herman Chernoff (* 1 July 1923) are a method of multivariate data visualisation. Within the framework of data visualisation using Chernoff Faces, the physiognomy of a cartoon-like simplified human face (e.g. size of the ears, shape of the mouth, inclination of the eyebrows, size of the nose, etc.) is shaped according to the characteristic values of a object. As a result, each row vector of a data set receives a Chernoff face structured according to its combination of column vector values. By means of the Chernoff faces, which may vary depending on the values on the column vectors, the feature profiles of the row vectors can now be compared with each other. A distinctive strength of the Chernoff Faces visualisation method is that it uses the human ability to register even small divergences or convergences in the physiognomy of human (or human-like) faces. Since the physiognomy of the Chernoff Faces is in a sense short-circuited with the feature expressions of the respective feature carriers, the method of the Chernoff Faces allows similarity or dissimilarity structures of different feature profiles of line vectors to be explored easily and intuitively.\nThe application of Chernoff Faces becomes problematic with data sets (cf. Livingstone 2009) with a very large number of row vectors: Of course, twenty faces are easier to distinguish than two hundred."
  },
  {
    "objectID": "posts/chernoff_faces/index.html#data-processing",
    "href": "posts/chernoff_faces/index.html#data-processing",
    "title": "Chernoff Faces",
    "section": "Data processing",
    "text": "Data processing\nIn a first step, the data set described above is loaded into the working environment for data preparation using the tidyverse packages (Wickham 2017). This is done here using the read_excel() function of the readxl package (Wickham and Bryan 2019).\n\n# Load data set\nlibrary(readxl)\ndf &lt;- read_excel(\"./Fine_Artfacts_Kunsthochschulen.xlsx\")\n\nWithin the framework of the data collection of the ranking positions, it was easy to determine a global ranking position for almost all professors. The collection of the national ranking position, which at the beginning of 2019 was still based on the birthplace of the respective artist, is more complicated. In order not to reduce the information content of the data during the survey, a new column vector was created for each nationality, which contains the respective national ranking position:\n\nhead(df[,14:36])\n\n# A tibble: 6 × 23\n  Rank_Germany Rank_USA Rank_France Rank_Netherlands Rank_Austria Rank_Turkey\n         &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1            7       NA          NA               NA           NA          NA\n2           24       NA          NA               NA           NA          NA\n3           33       NA          NA               NA           NA          NA\n4           34       NA          NA               NA           NA          NA\n5           40       NA          NA               NA           NA          NA\n6           NA       NA          NA               NA           NA          NA\n# ℹ 17 more variables: `Rank_United Kingdom` &lt;dbl&gt;, Rank_Italy &lt;dbl&gt;,\n#   Rank_Romania &lt;dbl&gt;, Rank_Sweden &lt;dbl&gt;, `Rank_South Africa` &lt;dbl&gt;,\n#   Rank_Norway &lt;dbl&gt;, Rank_Finland &lt;dbl&gt;, Rank_Switzerland &lt;dbl&gt;,\n#   `Rank_New Zealand` &lt;dbl&gt;, Rank_Mexico &lt;dbl&gt;, Rank_Japan &lt;dbl&gt;,\n#   Rank_Australia &lt;dbl&gt;, Rank_Israel &lt;dbl&gt;, Rank_Poland &lt;dbl&gt;,\n#   Rank_Canada &lt;dbl&gt;, `Rank_Korea (Republic of)` &lt;dbl&gt;, Rank_Iran &lt;dbl&gt;\n\n\nFor the following analyses, these column vectors must be combined into a single vector, which should be named Rank_Natio:\n\n#Set NA to \"\" and join columns together\nlibrary(tidyverse)\ncolnames(df)\n\n [1] \"Name\"                     \"Media\"                   \n [3] \"Gender\"                   \"Nationalitaet\"           \n [5] \"Alter\"                    \"Lives_Works\"             \n [7] \"Most_Exibit_in_1\"         \"Most_Exibit_in_2\"        \n [9] \"Most_Exibit_in_3\"         \"Kunsthochschule\"         \n[11] \"Fachbereich\"              \"Death\"                   \n[13] \"Rank_Global\"              \"Rank_Germany\"            \n[15] \"Rank_USA\"                 \"Rank_France\"             \n[17] \"Rank_Netherlands\"         \"Rank_Austria\"            \n[19] \"Rank_Turkey\"              \"Rank_United Kingdom\"     \n[21] \"Rank_Italy\"               \"Rank_Romania\"            \n[23] \"Rank_Sweden\"              \"Rank_South Africa\"       \n[25] \"Rank_Norway\"              \"Rank_Finland\"            \n[27] \"Rank_Switzerland\"         \"Rank_New Zealand\"        \n[29] \"Rank_Mexico\"              \"Rank_Japan\"              \n[31] \"Rank_Australia\"           \"Rank_Israel\"             \n[33] \"Rank_Poland\"              \"Rank_Canada\"             \n[35] \"Rank_Korea (Republic of)\" \"Rank_Iran\"               \n[37] \"Studienfach\"              \"Studienort\"              \n[39] \"Studium_bei\"             \n\ndf &lt;- df %&gt;% mutate_if(is.numeric,as.character) %&gt;% replace(., is.na(.), \"\") %&gt;% unite(\"Rank_Natio\", c(14:36))\n\n\n# Remove underscore caused by the unite_function.\ndf$Rank_Natio &lt;- str_replace_all(df$Rank_Natio, \"(_+)\", \"\") #Convert \"_\" (or more) into \"\"\n\n#replace empty string cells with NA data frame-wide\ndf &lt;- mutate_all(df, funs(na_if(.,\"\")))\n\nFurthermore, the ranking variables were loaded into the working environment as string variables; for the sake of simplicity, these are converted individually into numeric variables:\n\n#colnames(df)\ndf$Rank_Global &lt;- as.numeric(df$Rank_Global)\ndf$Rank_Natio &lt;- as.numeric(df$Rank_Natio)\ndf$Kunsthochschule &lt;- as.factor(df$Kunsthochschule)\n\nIn the following, aggregated variables are created at the art college level. These are, as already described above:\n\nmean value of the global ranking of all professors of an art academy\nmean value of the national ranking of all professors of an art academy\nnumber of professors per art academy in total\nnumber of female professors per art academy\nnumber of male professors per art academy\naverage age of all professors per art academy\n\n\n# Create aggregated variables at college level\n\n# Ranking average values per art college\nRank &lt;- df %&gt;% group_by(Kunsthochschule) %&gt;% mutate(Mean_Nat_Rank= mean(Rank_Natio,na.rm = T)) %&gt;% mutate(Mean_Global_Rank = mean(Rank_Global, na.rm = T)) %&gt;%\n  summarize(Sum_Nat = round(mean(Mean_Nat_Rank),2), Sum_Glob = round(mean(Mean_Global_Rank),2))\n\n# Number of professors per art college\nProfs &lt;- df %&gt;% group_by(Kunsthochschule) %&gt;% summarise (n_Profs = n())\n\n# Number of men/women per art college\nGender &lt;- df %&gt;% group_by(Kunsthochschule) %&gt;%\n  summarise(n_Frau = sum(Gender == \"F\"), n_Mann = sum(Gender == \"M\"))\n\n# Mean_age per art college\ndf$Alter &lt;- as.numeric(df$Alter)\ndf$Alter &lt;- 2019-df$Alter\n\nAlter &lt;- df %&gt;% group_by(Kunsthochschule) %&gt;% mutate(Mean_Alter= mean(Alter,na.rm = T))  %&gt;%  \n  summarize(Mean_Alter_Sum = round(mean(Mean_Alter),2))   \n\n# Data merging:\ndf1 &lt;- merge(Rank,Profs, by=\"Kunsthochschule\")\ndf2 &lt;- merge(df1,Gender, by=\"Kunsthochschule\")\nArtFac &lt;- merge(df2,Alter, by=\"Kunsthochschule\")\n\n# Remove all data frames except ArtFac + df\nrm(list=setdiff(ls(), c(\"ArtFac\", \"df\")))\n\nFinally, all rows with one or more missings on the column vectors are filtered out of the data frame. In addition, in order to make the Chernoff-Faces plot to be generated clearer, the names of the art colleges are replaced by acronyms and specified as rownames.\n\n# colnames(ArtFac)\n# filter out NA row\nArtFac &lt;- ArtFac %&gt;%  filter(!is.na(Sum_Nat))\n\n# Columns in rownames\nArtFac &lt;- ArtFac %&gt;% remove_rownames %&gt;% column_to_rownames(var=\"Kunsthochschule\")\n\n#rownames(ArtFac)\n# Change the names of the KHs to make the Faces plot clearer.\nrownames(ArtFac)[1]&lt;-\"AdBK M\"\nrownames(ArtFac)[2]&lt;-\"BU WE\"\nrownames(ArtFac)[3]&lt;-\"KH HAL\"\nrownames(ArtFac)[4]&lt;-\"HBK BS\"\nrownames(ArtFac)[5]&lt;-\"HfBK DD\"\nrownames(ArtFac)[6]&lt;-\"HfBK HH\"\nrownames(ArtFac)[7]&lt;-\"HFK HB\"\nrownames(ArtFac)[8]&lt;-\"HdBK SB\"\nrownames(ArtFac)[9]&lt;-\"HfG OF\"\nrownames(ArtFac)[10]&lt;-\"KA D\"\n\nrownames(ArtFac)[11]&lt;-\"KA MS\"\nrownames(ArtFac)[12]&lt;-\"KHB B\"\nrownames(ArtFac)[13]&lt;-\"KfM K\"\nrownames(ArtFac)[14]&lt;-\"KH KS\"\nrownames(ArtFac)[15]&lt;-\"KH KI\"\nrownames(ArtFac)[16]&lt;-\"KH L\"\nrownames(ArtFac)[17]&lt;-\"KH MZ\"\nrownames(ArtFac)[18]&lt;-\"KH N\"\nrownames(ArtFac)[19]&lt;-\"AdK KA\"\nrownames(ArtFac)[20]&lt;-\"AdK S\"\nrownames(ArtFac)[21]&lt;-\"HfBK FaM\"\nrownames(ArtFac)[22]&lt;-\"UdK Berlin\"\n\nThe finished dataframe finally takes the following form:\n\nlibrary(knitr)\nArtFac %&gt;%\n  kable(format = 'markdown')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSum_Nat\nSum_Glob\nn_Profs\nn_Frau\nn_Mann\nMean_Alter_Sum\n\n\n\n\nAdBK M\n509.13\n3845.47\n17\n6\n11\n58.76\n\n\nBU WE\n359.50\n2327.00\n4\n3\n1\n49.50\n\n\nKH HAL\n2223.50\n18192.20\n14\n6\n8\n54.36\n\n\nHBK BS\n747.67\n8430.00\n13\n5\n8\n58.92\n\n\nHfBK DD\n949.25\n7841.25\n12\n3\n9\n59.25\n\n\nHfBK HH\n251.27\n3525.25\n16\n8\n8\n51.94\n\n\nHFK HB\n1132.11\n9973.33\n9\n4\n5\n54.89\n\n\nHdBK SB\n853.25\n32832.40\n5\n2\n3\n56.80\n\n\nHfG OF\n1413.80\n11350.00\n5\n2\n3\n53.20\n\n\nKA D\n584.21\n6600.47\n19\n8\n11\n54.32\n\n\nKA MS\n1161.23\n12924.08\n13\n4\n9\n55.08\n\n\nKHB B\n624.50\n10828.83\n6\n3\n3\n56.50\n\n\nKfM K\n768.67\n17749.57\n8\n2\n5\n58.14\n\n\nKH KS\n599.60\n4157.80\n6\n1\n5\n51.00\n\n\nKH KI\n1420.25\n36768.60\n6\n3\n3\n62.00\n\n\nKH L\n1359.82\n16067.94\n18\n6\n12\n54.29\n\n\nKH MZ\n1080.70\n7809.40\n11\n6\n5\n55.55\n\n\nKH N\n2367.22\n27328.20\n10\n4\n6\n52.10\n\n\nAdK KA\n444.64\n3176.14\n14\n6\n8\n55.86\n\n\nAdK S\n1842.19\n13372.31\n16\n5\n11\n58.25\n\n\nHfBK FaM\n72.00\n604.57\n8\n3\n5\n49.50\n\n\nUdK Berlin\n596.65\n14717.55\n22\n12\n10\n54.70\n\n\n\n\n\nThe lower the value of a ranking profile, the better the professors of the respective art academy are ranked on average."
  },
  {
    "objectID": "posts/correspondence_analysis/index.html",
    "href": "posts/correspondence_analysis/index.html",
    "title": "Correspondence Analysis",
    "section": "",
    "text": "Correspondence analysis (ca) is a chi-square statistic-based, primarily geometric, descriptive or explorative method, that can be used to examine latent structures of multivariate data. Like cluster analysis, correspondence analysis is a data reduction procedure that, in contrast to cluster analysis and principal component analysis, is used in particular to describe categorical data that are grouped together in a contingency table (Blasius 2001). In contrast to cluster analysis, however, the characteristics or subjects are not assigned to a cluster, but are localized and related to each other in a space spanned by latent dimensions, which are represented by continuously scaled structural axes. The goal of ca, meanwhile, is to determine an optimal (for various, pragmatic reasons, usually two-dimensional) subspace of a multidimensional hyperspace. In the definition of this system the dimensions are to be chosen in such a way that with them a maximum of the variation of the data can be explained (Blasius 2001).\nThe ca is primarily interpreted graphically, while numerical interpretation tends to be of lower priority (Backhaus et al. 2016). The graphical representation possibilities are then probably also one of the main advantages of ca: Thus, ca can represent data structures clearly, graphically, reduce complexity on the basis of latent dimensions, and thus make complex, unknown relationships visible at a glance. The general maxim of exploratory data analysis “Let the data speak for themselves!” (Le Roux and Rouanet 2010, 4) is thus quite redeemed in the context of ca’s graphical representations. However, the comparatively catchy “geometric maps” (Lenger, Schneickert, and Schumacher 2013, 207) that result from the geometric solution of a ca, also harbor a number of interpretational hurdles that even Pierre Bourdieu was not always able to overcome without error (Blasius and Winkler 1989; Gollac 2015)."
  },
  {
    "objectID": "posts/correspondence_analysis/index.html#data-processing",
    "href": "posts/correspondence_analysis/index.html#data-processing",
    "title": "Correspondence Analysis",
    "section": "Data processing",
    "text": "Data processing\nTo do this, we first load the corresponding contingency table into our workspace using the read_excel() function of the readxl package and specify (with the help of a “pipe”) as rownames the variable of the social classes that already has the name rownames in our excel-sheet.\nIn a further step, we print the contingency table (in transposed form for clarity) using the kable() function of the kableExtra package.\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\ndf <- read_excel(\"Bourdieu_p822.xlsx\") %>% column_to_rownames(\"rownames\")\n\nknitr::kable(t(df), \"pipe\")\n\n\n\n\n\nworking classes\nmiddle classes\nupper classes\n\n\n\n\nsunset\n90\n84\n64\n\n\ncommunion\n50\n31\n18\n\n\nfolkloric dance\n63\n52\n37\n\n\ngirl with cat\n56\n60\n50\n\n\nlactating woman\n44\n55\n53\n\n\ntree bark\n17\n32\n48\n\n\nsteel frame\n6\n12\n23\n\n\npregnant woman\n11\n16\n19\n\n\ncabbages\n7\n11\n18\n\n\ncar accident\n0\n1\n4\n\n\nRaphael\n32\n27\n18\n\n\nBuffet\n8\n17\n9\n\n\nUtrillo\n20\n18\n20\n\n\nVlamink\n6\n12\n11\n\n\nWatteau\n16\n19\n16\n\n\nRenoir\n49\n51\n48\n\n\nVan Gogh\n48\n47\n49\n\n\nDali\n3\n4\n5\n\n\nBraque\n5\n7\n9\n\n\nGoya\n16\n19\n31\n\n\nBrueghel\n1\n12\n27\n\n\nKandinsky\n0\n2\n4"
  },
  {
    "objectID": "posts/webscraping_python/index.html",
    "href": "posts/webscraping_python/index.html",
    "title": "Webscraping with Python",
    "section": "",
    "text": "Introduction\nWeb scraping refers to the process of extracting, copying and storing web content or source code sections with the aim of analysing or otherwise exploiting them. With the help of web scraping procedures, many different types of information can be collected and analysed automatically: For example, contact, network or behavioural data. Web scraping is no longer a novelty in empirical research. There are numerous reasons why web scraping methods are used in empirical research: For example, repeated collection of cross-sectional web data, the desire for replicability of data collection processes or the desire for time-efficient and less error-prone collection of online data (Munzert and Nyhuis 2019).\n\n\nWeb scraping with Python\nAs a low-threshold introduction to webscraping, we want to use Python to collect various online-accessible articles from the art magazine Monopol from the section Art Market. For this we need the Pythons libaries BeautifulSoup, requests, pandas and for the later backup of our collected data the library openpyxl. Since I myself use RStudio as IDE ((Integrated Development Environment)) for Python, I load the R package reticulate in an R environment and turn off the (to me annoying) notification function of the package. Finally, I use the function use_python() to tell R under which path my Python binary can be found. Users who work directly in Python can skip this step.\n\nlibrary(reticulate)\noptions(reticulate.repl.quiet = TRUE)\nuse_python(\"~/Library/r-miniconda/bin/python\")\n\nIn a further step, the Python libaries mentioned above are loaded into the working environment. If the libraries have not yet been installed, this can be done from R with the reticulate function py_install(). In Python, libraries can be installed directly with the function pip install.\n\n#py_install(\"pandas\")\n#py_install(\"bs4\")\n#py_install(\"openpyxl\")\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport openpyxl\n\nOnce this is done, we define the URL where the links to the ten newest articles of the section “Art Market” are stored. The defined URL are fed to the function requests.get(), which should display the status code 200: HTTP status code 200 indicates that the request was successfully processed. All requested data has been located on the web server and is being transferred to the client\n\n#load url\nurl = \"https://www.monopol-magazin.de/ressort/kunstmarkt\"\nres = requests.get(url)\nprint(res)\n\n<Response [200]>\n\n\nIf we pass the object text from the list res generated by the requests.get() function to the BeautifulSoup function, we get a BeautifulSoup object that transforms the HTML document into an easily readable nested data structure.\n\nsoup = BeautifulSoup(res.text)\n\nWith the help of the selector gadget we select the CSS selector that leads to the URLs of the first ten articles of the art market section. In this case it is: #block-kf-bootstrap-content header.\n\narticles = soup.select(\"#block-kf-bootstrap-content header\")\n\nFrom the list articles created in this way, all titles of the ten articles are now extracted as pure text elements with the help of a for-loop and the list articles_titles is written.\n\narticles_titles = [i.text for i in articles]\n\nFurthermore, another for-loop iterates over the soup object to extract the links to the full texts of the ten articles as simple text elements. An empty list called articles_urls is created in which all partial URLs are written that can be found in the HTML elements of the class article__title and there in the HTML element a. Since only the respective paths are stored in the a elements of the HTML code, but not the main domain of the URL, this is finally merged with the paths to form a single string.\n\narticles_urls = []\nfor article__title in soup.find_all(attrs={'class':'article__title'}):\n  for link in article__title.find_all('a'):\n    url = \"https://www.monopol-magazin.de\"+link.get('href')\n    articles_urls.append(url)\ntype(articles_urls)\n\n<class 'list'>\n\n\nNow the text elements of the ten articles can be collected with a for-loop. Again, an empty list is created, this time with the name data. The for-loop iterates over the ten URLs extracted above that are in the list articles_urls. The requests.get() function is called first on each of the ten URLs. If the respective web page of the articles of the status code reflects 200, we create - as above - a soup object with the function BeautifulSoup(), which puts the HTML code of the respective page into a clear structure for us. Then, with the help of the selector gadget, we select the CSS selector that leads to the articles of each section of the art market. In this case it is: .field--type-text-long.field--label-hidden. Finally, the collected text elements are adapted to the unicode standard using the function normalize of the library unicodedata and a for-loop. This is done with the help of the NFKD algorithm.\n\ndata = []\n\nfor url in articles_urls:\n    res = requests.get(url)\n    if res.status_code == 200:\n        soup = BeautifulSoup(res.text)\n        for element in soup.select('.field--type-text-long.field--label-hidden'):\n            data.append((element.get_text()))\n\n#clean text\nimport unicodedata\ndata = [unicodedata.normalize(\"NFKD\",i) for i in data]\n\nVoilà! As we could see, our code created little spiders that roam the URLs we specified and reliably collect all the information we instructed them to collect in our code. In this way, the little spiders that we brought to life with a few lines of Python code can do the work in seconds with an accuracy and speed that would take us humans a quarter of an hour.\nWe now merge the three lists with the titles of the articles, the URLs and the actual text corpus first into a list called d and then into a data-frame called df. Furthermore, we use the replace() function and the strip() function to remove all line breaks and whitespace from all columns of the type string.\n\ndata_sub = data[0:10]\nd = {'titles':articles_titles,'urls':articles_urls,'texts':data_sub}\ndf = pd.DataFrame(d)\n\ndf[df.columns] = df.apply(lambda x: x.str.replace(\"\\n\", \"\"))\ndf[df.columns] = df.apply(lambda x: x.str.strip())\n\nprint(df)\n\n                                              titles  ...                                              texts\n0  Gallery Weekend Berlin      Wohin in der Potsd...  ...  Gallery Weekend Berlin, 28. bis 30. AprilWeite...\n1  Kunstmesse Paper Positions in Berlin      Stab...  ...  Parallel zum Gallery Weekend in Berlin zeigt d...\n2       Gallery Weekend Berlin      Wohin im Westen?  ...  Cave-Ayumi Gallery: Naoto Kumagai \"Integrate\",...\n3        Gallery Weekend Berlin      Wohin in Mitte?  ...  Kräftig und klar wirken dagegen die Siebdruck...\n4         Art Brussels      Eine Messe für Spürnasen  ...         Māksla XO: Helena Heinrihsone \"Red\", 2022\n5             Kunstmesse      20 Jahre Art Karlsruhe  ...  Auf der Messe werden mehrere Preise für die b...\n6  Auktionen im September      Privatsammlung von...  ...         Paper Positions, Berlin, 27. bis 30. April\n7  Sotheby's      Riesenspinne und Klimt-Premiere...  ...  Gallery Weekend Berlin, 28. bis 30. AprilWeite...\n8  Galeristin Xiaochan Hua      \"Freiheit ist mir...  ...  Gallery Weekend Berlin, 28. bis 30. AprilWeite...\n9  Kunstmesse Art Brussels      Das nächste große...  ...  Die gut informierte lokale Sammlerszene liebt ...\n\n[10 rows x 3 columns]\n\n\nFinally, we export the data-frame created in this way into the .xlsx format with the function ExcelWriter(). To do this, we define the desired output-format, write the object df into an Excel sheet and export this with the function writer.save().\n\nwriter = pd.ExcelWriter('output.xlsx')\ndf.to_excel(writer)\nwriter.save()\n\n<string>:1: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n\n\nThere are many possibilities for analysing the data obtained, some of which will be presented here in the medium term.\n\n\n\n\n\n\n\n\nReferences\n\nMunzert, Simon, and Dominic Nyhuis. 2019. “Die Nutzung von Webdaten in den Sozialwissenschaften.” In Handbuch Methoden der Politikwissenschaft, 373–97. Wiesbaden: Springer. https://doi.org/10.1007/978-3-658-16937-4_22-1."
  },
  {
    "objectID": "acomplishments.html#section",
    "href": "acomplishments.html#section",
    "title": "Acomplishments",
    "section": "",
    "text": "Organiser\n\nTopic\nDate\nCertificate\n\n\n\n\n\nDataCamp\nIntroduction to\nPython\n4/21\nCertifcate\n\n\n\nDataCamp\nIntermediate Python\n4/21\nCertifcate"
  },
  {
    "objectID": "acomplishments.html#section-1",
    "href": "acomplishments.html#section-1",
    "title": "Acomplishments",
    "section": "2019",
    "text": "2019\n\n\n\nOrganiser\n\nTopic\nDate\nCertificate\n\n\n\n\n\nGESIS\nOnline-Befragungen – Planung\nund Durchführung\n12/19\nCertifcate\n\n\n\nGESIS\nEinführung in die Logik der\nBayesschen Statistik\n11/19\nCertifcate"
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "2018",
    "text": "2018\nKleimann, B., & Hückstädt, M. (2018). Auswahlkriterien in Berufungsverfahren: Universitäten und Fachhochschulen im Vergleich. Beiträge zur Hochschulforschung, 40 (2/2018), 20-47."
  },
  {
    "objectID": "publications.html#hückstädt-m.-2023.-ten-reasons-why-research-collaborations-succeeda-random-forest-approach-scientometrics-1283-192350.-doi-httpsdoi.org10.1007s11192-022-04629-7",
    "href": "publications.html#hückstädt-m.-2023.-ten-reasons-why-research-collaborations-succeeda-random-forest-approach-scientometrics-1283-192350.-doi-httpsdoi.org10.1007s11192-022-04629-7",
    "title": "Publications",
    "section": "2019Hückstädt, M. (2023). ‘Ten reasons why research collaborations succeed—a random forest approach’, Scientometrics, 128/3: 1923–50. DOI: https://doi.org/10.1007/s11192-022-04629-7",
    "text": "2019Hückstädt, M. (2023). ‘Ten reasons why research collaborations succeed—a random forest approach’, Scientometrics, 128/3: 1923–50. DOI: https://doi.org/10.1007/s11192-022-04629-7\n\nWeinmann, C., Hückstädt, M., Meißner, F., & Vowe, G. (2023). ‘How do researchers perceive problems in research collaboration? Results from a large-scale study of German scientists’, Frontiers in Research Metrics and Analytics, 8. DOI: https://doi.org/10.3389/frma.2023.1106482\nHückstädt, M., Leisten, L.M. (2023). Input, process, output - inhibitors and promoters of collaboration Problems (under Review).\nHückstädt, M. (2022). ‘Coopetition between frenemies–interrelations and effects of seven collaboration problems in research clusters’, Scientometrics, 127/9: 5191–224. DOI: 10.1007/s11192-022-04472-w. DOI: https://doi.org/10.1007/s11192-022-04472-w\nKleimann, B., & Hückstädt, M. (2021). Selection criteria in professorial recruiting as indicators of institutional similarity? A comparison of German universities and universities of applied sciences.Quality in Higher Education (online first). https://doi.org/10.1080/13538322.2021.1889760"
  },
  {
    "objectID": "publications.html#articles-in-edited-anthologies",
    "href": "publications.html#articles-in-edited-anthologies",
    "title": "Publications",
    "section": "Articles in edited anthologies",
    "text": "Articles in edited anthologies\n\nHückstädt, M. (2019). Determinanten der subjektiv wahrgenommenen Wichtigkeit von Hochschulrankings bei der Studienortwahl? In L. Ringel & T. Werron (Hrsg.), Rankings - Soziologische Fallstudien (S. 181-202). Wiesbaden: Springer. https://doi.org/10.1007/978-3-658-26366-9_8"
  },
  {
    "objectID": "publications.html#workingpaper",
    "href": "publications.html#workingpaper",
    "title": "Publications",
    "section": "Workingpaper",
    "text": "Workingpaper\n\nHückstädt, M., Janßen, M., Oberschelp, A., Wagner, N., Weinmann, C. & Winde, M. (2022). Forschungskooperation im Verbund: Herausforderungen, Lösungsansätze, Empfehlungen. Stifterverband für die Deutsche Wissenschaft. Future Lab: Kooperationsgovernance – Diskussionspapier 6."
  },
  {
    "objectID": "publications.html#reports",
    "href": "publications.html#reports",
    "title": "Publications",
    "section": "Reports",
    "text": "Reports\n\nHückstädt, M. (2023). Determinanten und Effekte von Kooperation in Homogenen und Heterogenen Forschungsverbünden (DEKiF). Daten- und Methodenbericht des DEKiF-Websurveys. Version 1.0.0. Hannover: fdz.DZHW. Hannover.\nHückstädt, M., İkiz-Akıncı, D. & Kleimann, B. (2020). Leistungsbewertung in Berufungsverfahren. Traditionswandel in der Akademischen Personalselektion. Daten- und Methodenbericht zur qualitativen Erhebung der DZHW-Studie LiBerTas 2016. Version 1.0.0. Hannover: fdz.DZHW. doi:10.21249/DZHW:lib2016:1.0.0"
  },
  {
    "objectID": "skills.html",
    "href": "skills.html",
    "title": "Skills",
    "section": "",
    "text": "German\n\n\n    \n\n\n\n\n\n\n\nEnglish\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n    \n\n\n\n\n\n\n\nPython\n\n\n    \n\n\n\n\n\n\n\nMplus\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n(R/Quarto)Markdown\n\n\n    \n\n\n\n\n\n\n\nLaTex\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nGit(Hub)\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nreadxl\n\n\n    \n\n\n\n\n\n\n\ntidyverse\n\n\n    \n\n\n\n\n\n\n\nknitr\n\n\n    \n\n\n\n\n\n\n\nstringr\n\n\n    \n\n\n\n\n\n\n\nrvest\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nShiny\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnaconda\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\nVS Code\n\n\n    \n\n\n\n\n\n\n\nRstudio\n\n\n    \n\n\n\n\n\n\n\nAtom (†)\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nZotero\n\n\n    \n\n\n\n\n\n\n\nyEd\n\n\n    \n\n\n\n\n\n\n\nPandoc\n\n\n    \n\n\n\n\n\n\n\nMS Office\n\n\n    \n\n\n\n\n\n\n\nSQL\n\n\n    \n\n\n\n\n\n\n\nDocker\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n    \n\n\n\n\n\n\n\nOrdinal Regression\n\n\n    \n\n\n\n\n\n\n\nRegression Trees\n\n\n    \n\n\n\n\n\n\n\nMultilevel Analysis\n\n\n    \n\n\n\n\n\n\n\nTime Series Analysis\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n    \n\n\n\n\n\n\n\nClassification Trees\n\n\n    \n\n\n\n\n\n\n\nSupport Vector Machines\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nPartitioning Cluster Analysis\n\n\n    \n\n\n\n\n\n\n\nHierarchical Cluster Analysis\n\n\n    \n\n\n\n\n\n\n\nLatent Class Analysis\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n(Multiple) Corres-pondence Analysis\n\n\n    \n\n\n\n\n\n\n\nMultidimensional Scaling\n\n\n    \n\n\n\n\n\n\n\nExploratory/Confirmatory  Factor Analysis\n\n\n    \n\n\n\n\n\n\n\nPrincipal Component Analysis\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\nStructural Equation Modeling\n\n\n    \n\n\n\n\n\n\n\nWeb Scraping\n\n\n    \n\n\n\n\n\n\n\n(Permutational)Analysis of Variance\n\n\n    \n\n\n\n\n\n\n\nGeocoding\n\n\n    \n\n\n\n\n\n\n\nNetwork Analysis\n\n\n    \n\n\n\n\n\n\n\nNatural LanguageProcessing"
  },
  {
    "objectID": "acomplishments.html#section-2",
    "href": "acomplishments.html#section-2",
    "title": "Acomplishments",
    "section": "2019",
    "text": "2019\n\n\n\nOrganiser\n\nTopic\nDate\nCertificate\n\n\n\n\n\nGESIS\nOnline-Befragungen – Planung\nund Durchführung\n12/19\nCertifcate\n\n\n\nGESIS\nEinführung in die Logik der\nBayesschen Statistik\n11/19\nCertifcate"
  },
  {
    "objectID": "experience.html#german-centre-for-higher-education-research-and-science-studies-dzhw-since-062022-hanover-de-project-hierarchies-in-cooperative-research-third-party-funding-application-preparation-dfg-individual-research-grants-project-content-hierarchies-have-the-potential-to-both-enhance-and-effectiveness-of-research-teams.-the-research-project-hierarchies-in-collaborative-research-aims-to-identify-the-determinants-of-hierarchies-in-collaborative-research.-research-the-determinants-of-whether-why-and-when-different-forms-of-and-when-different-forms-of-hierarchy-can-impair-the-effectiveness-of-teams.-effectiveness-of-teams-mediated-by-cooperation-problems.-impair-the-effectiveness-of-teams.-the-data-basis-for-the-study-will-be-the-survey-data-of-data-from-staff-members-of-collaborative-research-centres-and-excellence-clusters-will-be-analysed.",
    "href": "experience.html#german-centre-for-higher-education-research-and-science-studies-dzhw-since-062022-hanover-de-project-hierarchies-in-cooperative-research-third-party-funding-application-preparation-dfg-individual-research-grants-project-content-hierarchies-have-the-potential-to-both-enhance-and-effectiveness-of-research-teams.-the-research-project-hierarchies-in-collaborative-research-aims-to-identify-the-determinants-of-hierarchies-in-collaborative-research.-research-the-determinants-of-whether-why-and-when-different-forms-of-and-when-different-forms-of-hierarchy-can-impair-the-effectiveness-of-teams.-effectiveness-of-teams-mediated-by-cooperation-problems.-impair-the-effectiveness-of-teams.-the-data-basis-for-the-study-will-be-the-survey-data-of-data-from-staff-members-of-collaborative-research-centres-and-excellence-clusters-will-be-analysed.",
    "title": "Experience",
    "section": "German Centre for Higher Education Research and Science Studies (DZHW) since 06/2022 – Hanover, DE Project: Hierarchies in Cooperative Research (third-party funding-application preparation, DFG, Individual Research Grants) Project content: Hierarchies have the potential to both enhance and effectiveness of research teams. The research project Hierarchies in Collaborative Research aims to identify the determinants of hierarchies in collaborative research. research, the determinants of whether, why and when different forms of and when different forms of hierarchy can impair the effectiveness of teams. effectiveness of teams (mediated by cooperation problems). impair the effectiveness of teams. The data basis for the study will be the survey data of data from staff members of Collaborative Research Centres and Excellence clusters will be analysed.",
    "text": "German Centre for Higher Education Research and Science Studies (DZHW) since 06/2022 – Hanover, DE Project: Hierarchies in Cooperative Research (third-party funding-application preparation, DFG, Individual Research Grants) Project content: Hierarchies have the potential to both enhance and effectiveness of research teams. The research project Hierarchies in Collaborative Research aims to identify the determinants of hierarchies in collaborative research. research, the determinants of whether, why and when different forms of and when different forms of hierarchy can impair the effectiveness of teams. effectiveness of teams (mediated by cooperation problems). impair the effectiveness of teams. The data basis for the study will be the survey data of data from staff members of Collaborative Research Centres and Excellence clusters will be analysed.\nResponsibilities:\nPhase 1: Preparation of the proposal\n\nDevelopment of a project idea\nDevelopment of a short concept\nReview of the international state of research\nDevelopment of a project concept\n\nPhase 2: Writing the proposal\n\nPresentation of the initial situation\nPresentation of the work programme\nPresentation of the state of research"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Recent Talks\n\nHückstädt, M. (2023, 8. June). Forschungskooperationen - Probleme und Erfolgsdeterminanten. Disputationsvortrag, Deutsches Zentrum für Hochschul- und Wissenschaftsforschung (DZHW), Hanover, DE. You can find the slides of the presentation here::\n\n\n\nHückstädt, M. (2022, 5. May). Entstehungskontexte von Kooperationsproblemen in Forschungsverbünden und ihre Effekte auf den Kooperationserfolg. Evaluation der Abteilung Governance in Wissenschaft und Hochschule durch den wissenschaftlichen Beirat, Deutsches Zentrum für Hochschul- und Wissenschaftsforschung (DZHW), Hanover, DE.\nHückstädt, M. & Janssen, M. (2022, 24. January). Was macht Forschungsverbünde erfolgreich? Der Beitrag des Forschungsverbundmanagements – Befunde und Empfehlungen aus den Teilstudien des DZHW. Praktischer Abschlussworkshop des Verbundprojektes DEKiF, Deutsches Zentrum für Hochschul- und Wissenschaftsforschung (DZHW), Stifterverband für die Deutsche Wissenschaft, Heinrich-Heine-Universität Düsseldorf, Hanover, DE.\nJungbauer-Gans, M., Kleimann, B., Janßen, M., Oberschelp, A. & Hückstädt, M. (2021, 22. November). Benefits and challenges of collaboration in research. Investigating the manifold aspects of mono-, inter- and transdisciplinary collaboration. Wissenschaftlicher Abschlussworkshop des Verbundprojektes DEKiF, Deutsches Zentrum für Hochschul- und Wissenschaftsforschung (DZHW), Stifterverband für die Deutsche Wissenschaft, Heinrich-Heine-Universität Düsseldorf, Hanover, DE.\n\n\n\nUpcoming Talks\n\ncurrently nothing"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Imprint",
    "section": "",
    "text": "Angaben gemäß § 5 TMG"
  },
  {
    "objectID": "contact.html#kontakt",
    "href": "contact.html#kontakt",
    "title": "Imprint",
    "section": "Kontakt",
    "text": "Kontakt\nMalte Hückstädt\nSchelfmarkt 1\n19055 Schwerin\nTelefon: +49 (0) 176215040464\nTelefax: +49 (0) 511405638\nE-Mail: deaddatascientists@gmail.com"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Open Source License",
    "section": "",
    "text": "Quarto is open source software licensed under the GNU GPL v2. We believe that it’s better for everyone if the tools used for research and science are free and open. Reproducibility, widespread sharing of knowledge and techniques, and the leveling of the playing field by eliminating cost barriers are but a few of the shared benefits of free software in science.\nThe Quarto source code is available at https://github.com/quarto-dev/\nQuarto is a registered trademark of Posit. Please see our trademark policy for guidelines on usage of the Quarto trademark."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "If you are an R Markdown user wondering about the relationship between Quarto and R Markdown, see also our FAQ for R Markdown Users.\n\nWhat can I use Quarto for?\nQuarto® is an open-source scientific and technical publishing system built on Pandoc. You can weave together narrative text and code to produce elegantly formatted output as documents, web pages, blog posts, books and more. \n\n\n\nHow do I install Quarto?\nVisit the Quarto.org Get Started page, which provides installation instructions for Windows, Mac OS, and Linux. \n\n\n\nIs Quarto free to use?\nYes! Quarto is open source with a GPL-2 license. You can use or disseminate it any way that you would any GPL-2 licensed open source software. \n\n\nWhat output formats can Quarto create?\nThere are many output formats available in Quarto. This includes all of the built in Pandoc formats (e.g. HTML, PDF, MS Word, Revealjs, ePub, etc.) as well as various ways to publish multiple documents (websites, blogs, and books). Learn more at Quarto Formats. \n\n\nWhat editing tools can I use with Quarto?\nYou can use a wide variety of tools with Quarto. We have provided documentation for writing and editing Quarto documents in VSCode, JupyterLab, RStudio IDE, or any text editor. Visit the Get Started with Quarto page to install, and then choose your tool for a brief introductory guide.\n\n\nCan I use Jupyter notebooks with Quarto?\nYes! Quarto can render Jupyter notebooks and you can use Jupyter, JupyterLab or any other .ipynb notebook editor with Quarto. You can render existing .ipynb notebooks as-is with Quarto, but adding Quarto-specific output options or a YAML header can enhance the output. Visit theJupyterLab page for more information.\n\n\nWhat programming languages are supported in Quarto?\nThe principal languages supported by Quarto are Python, R, Julia, and Observable JavaScript. Integration with Jupyter also enables the use of many other languages. \nEach Quarto document can be optionally processed by a computational engine (the engine can be manually specified or automatically detected based on the code chunks within). Current engines include Knitr (which is also used by R Markdown and supports a variety of languages including R, Python, and Julia, etc.) and Jupyter (which supports many languages including Python, Julia, and R). See the documentation on Engine Binding for additional details.\n\n\nWhat human languages are supported in Quarto?\nYou can write your Quarto documentation in your human language of choice. The lang document option is used to identify the main language of the document using IETF language tags (following the BCP 47 standard), such as en or en-GB. \n\n\nCan I use Quarto to develop proprietary content?\nYes! The copyright on Quarto does not cover the original content that you generate using Quarto. Using Quarto to create original content does not place any restrictions, legally, on the license that you choose for the original content that you create, nor does it “reach through” to affect software that you might be writing documentation for with Quarto.\n\n\nBut doesn’t the GPL cover exported HTML documents when they include styles or functionalities from Quarto?\nIt covers the styles or functionalities themselves. It does not cover your original content because your original content is not a derivative work of the Quarto styles or functionalities.\n\n\nHow can I share documents and have people comment on them?\nYou can publish Quarto content to various locations. See the user guides for publishing for details on using Quarto Pub, GitHub Pages, Netlify, Posit Connect, and other services with Quarto. Once documents are published you can use  hypothes.is, Utterances, or Giscus for commenting. Learn more in the documentation on commenting.\n\n\nCan I do collaborative editing with Quarto?\nThere is not yet anything specific for collaborative editing in Quarto. You can collaborate on .qmd files in the same way you currently do for any text or code files. \nPosit Workbench allows for Project Sharing for interactive editing and collaboration on the same document.\n\n\nWhere can I publish Quarto websites?\nThere are a wide variety of ways to publish Quarto websites. Website content is by default written to the \\_site sub-directory (you can customize this using the output-dir option). Publishing is simply a matter of copying the output directory to a web server or web hosting service.\nThe publishing documentation describes several convenient options for Quarto website deployment including Posit Connect, Netlify, GitHub Pages, Firebase, Site44, and Amazon S3. We’ll mostly defer to the documentation provided by those various services, but will note any Quarto website specific configuration required.\n\n\nDoes Posit Connect support Quarto?\nYes! You can publish Quarto content to Posit Connect v2021.08.0 or later. Quarto has to be enabled as documented in the Posit Connect admin guide. Connect’s user documentation refers to Quarto.org docs on how to publish from the RStudio IDE. To publish Python-based Quarto content, you can use the rsconnect-python CLI from various locations, including VSCode, JupyterLab or the terminal.\n\n\nWho are the developers of Quarto?\nDevelopment of Quarto is sponsored by Posit, PBC. The same core team works on both Quarto and R Markdown:\n\nCarlos Scheidegger (@cscheid)\nCharles Teague (@dragonstyle)\nChristophe Dervieux (@cderv)\nJ.J. Allaire (@jjallaire)\nYihui Xie (@yihui)\n\nHere is the full contributors list. Quarto is open source and we welcome contributions in our github repository as well! https://github.com/quarto-dev/quarto-cli.\n\n\nWhy the name Quarto?\nWe wanted to use a name that had meaning in the history of publishing and landed on Quarto, which is the format of a book or pamphlet produced from full sheets printed with eight pages of text, four to a side, then folded twice to produce four leaves. The earliest known European printed book is a Quarto, the Sibyllenbuch, believed to have been printed by Johannes Gutenberg in 1452–53.\n\n\nWhere can I report bugs or request features?\nThanks for finding something and sharing with us! You can file an issue in the Quarto repository https://github.com/quarto-dev/quarto-cli/issues.\n\n\nWhere can I ask questions and discuss using Quarto with others?\nThe best place to ask questions and see what questions other people have is in Quarto discussions (https://github.com/quarto-dev/quarto-cli/discussions)."
  },
  {
    "objectID": "about.html#github-activity",
    "href": "about.html#github-activity",
    "title": "About",
    "section": "GitHub Activity",
    "text": "GitHub Activity\n\n\n\n\nLoading my github calendar data."
  },
  {
    "objectID": "about.html#social-media",
    "href": "about.html#social-media",
    "title": "About",
    "section": "Social Media",
    "text": "Social Media\n\nTweets \nTweets by chernofffaces\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSieh dir diesen Beitrag auf Instagram an\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEin Beitrag geteilt von Chernoff Faces (@chernoff_faces)"
  },
  {
    "objectID": "contact.html#redaktionell-verantwortlich-malte-hückstädt-schelfmarkt-1-19055-schwerin",
    "href": "contact.html#redaktionell-verantwortlich-malte-hückstädt-schelfmarkt-1-19055-schwerin",
    "title": "Imprint",
    "section": "Redaktionell verantwortlich Malte Hückstädt Schelfmarkt 1 19055 Schwerin",
    "text": "Redaktionell verantwortlich Malte Hückstädt Schelfmarkt 1 19055 Schwerin\nDatenschutzerklärung\nDie folgenden Hinweise geben einen einfachen Überblick darüber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie persönlich identifiziert werden können. Ausführliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgeführten Datenschutzerklärung.\n\nDatenerfassung auf dieser Website\nWer ist verantwortlich für die Datenerfassung auf dieser Website?\nDie Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten können Sie dem Abschnitt “Hinweis zur Verantwortlichen Stelle” in dieser Datenschutzerklärung entnehmen.\n\nWie erfassen wir Ihre Daten?\nIhre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z. B. um Daten handeln, die Sie in ein Kontaktformular eingeben.\nAndere Daten werden automatisch oder nach Ihrer Einwilligung beim Besuch der Website durch unsere IT- Systeme erfasst. Das sind vor allem technische Daten (z. B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs). Die Erfassung dieser Daten erfolgt automatisch, sobald Sie diese Website betreten.\n\nWofür nutzen wir Ihre Daten?\nEin Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gewährleisten. Andere Daten können zur Analyse Ihres Nutzerverhaltens verwendet werden.\n\nWelche Rechte haben Sie bezüglich Ihrer Daten?\nSie haben jederzeit das Recht, unentgeltlich Auskunft über Herkunft, Empfänger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben außerdem ein Recht, die Berichtigung oder Löschung dieser Daten zu verlangen. Wenn Sie eine Einwilligung zur Datenverarbeitung erteilt haben, können Sie diese Einwilligung jederzeit für die Zukunft widerrufen. Außerdem haben Sie das Recht, unter bestimmten Umständen die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Des Weiteren steht Ihnen ein Beschwerderecht bei der zuständigen Aufsichtsbehörde zu. Hierzu sowie zu weiteren Fragen zum Thema Datenschutz können Sie sich jederzeit an uns wenden.\nAnalyse-Tools und Tools von Drittanbietern\nBeim Besuch dieser Website kann Ihr Surf-Verhalten statistisch ausgewertet werden. Das geschieht vor allem mit sogenannten Analyseprogrammen.\nDetaillierte Informationen zu diesen Analyseprogrammen finden Sie in der folgenden Datenschutzerklärung.\nHosting\nDiese Website wird extern gehostet. Die personenbezogenen Daten, die auf dieser Website erfasst werden, werden auf den Servern des Hosters / der Hoster gespeichert. Hierbei kann es sich v. a. um IP-Adressen, Kontaktanfragen, Meta- und Kommunikationsdaten, Vertragsdaten, Kontaktdaten, Namen, Websitezugriffe und sonstige Daten, die über eine Website generiert werden, handeln.\nDas externe Hosting erfolgt zum Zwecke der Vertragserfüllung gegenüber unseren potenziellen und bestehenden Kunden (Art. 6 Abs. 1 lit. b DSGVO) und im Interesse einer sicheren, schnellen und effizienten Bereitstellung unseres Online-Angebots durch einen professionellen Anbieter (Art. 6 Abs. 1 lit. f DSGVO). Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endgerät des Nutzers (z. B. Device-Fingerprinting) im Sinne des TTDSG umfasst. Die Einwilligung ist jederzeit widerrufbar.\nUnser(e) Hoster wird bzw. werden Ihre Daten nur insoweit verarbeiten, wie dies zur Erfüllung seiner Leistungspflichten erforderlich ist und unsere Weisungen in Bezug auf diese Daten befolgen.\n\nWir setzen folgende(n) Hoster ein:\nGitHub, Inc.\n88 Colin P Kelly Jr St San Francisco\nCA 94107\nUnited States of America\n\nAllgemeine Hinweise und Pflichtinformationen\nDatenschutz\nDie Betreiber dieser Seiten nehmen den Schutz Ihrer persönlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend den gesetzlichen Datenschutzvorschriften sowie dieser Datenschutzerklärung.\nWenn Sie diese Website benutzen, werden verschiedene personenbezogene Daten erhoben. Personenbezogene Daten sind Daten, mit denen Sie persönlich identifiziert werden können. Die vorliegende Datenschutzerklärung erläutert, welche Daten wir erheben und wofür wir sie nutzen. Sie erläutert auch, wie und zu welchem Zweck das geschieht. Wir weisen darauf hin, dass die Datenübertragung im Internet (z. B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich.\n\nHinweis zur verantwortlichen Stelle\nDie verantwortliche Stelle für die Datenverarbeitung auf dieser Website ist:\nMalte Hückstädt\nSchelfmarkt 1\n19055 Schwerin\nTelefon: +49 176215040464 \nE-Mail: deaddatascientists@gmail.com\n\nVerantwortliche Stelle ist die natürliche oder juristische Person, die allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z. B. Namen, E-Mail-Adressen o. Ä.) entscheidet.\n\nSpeicherdauer\nSoweit innerhalb dieser Datenschutzerklärung keine speziellere Speicherdauer genannt wurde, verbleiben Ihre personenbezogenen Daten bei uns, bis der Zweck für die Datenverarbeitung entfällt. Wenn Sie ein berechtigtes Löschersuchen geltend machen oder eine Einwilligung zur Datenverarbeitung widerrufen, werden Ihre Daten gelöscht, sofern wir keine anderen rechtlich zulässigen Gründe für die Speicherung Ihrer personenbezogenen Daten haben (z. B. steuer- oder handelsrechtliche Aufbewahrungsfristen); im letztgenannten Fall erfolgt die Löschung nach Fortfall dieser Gründe.\n\nAllgemeine Hinweise zu den Rechtsgrundlagen der Datenverarbeitung auf dieser Website\nSofern Sie in die Datenverarbeitung eingewilligt haben, verarbeiten wir Ihre personenbezogenen Daten auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO bzw. Art. 9 Abs. 2 lit. a DSGVO, sofern besondere Datenkategorien nach Art. 9 Abs. 1 DSGVO verarbeitet werden. Im Falle einer ausdrücklichen Einwilligung in die Übertragung personenbezogener Daten in Drittstaaten erfolgt die Datenverarbeitung außerdem auf Grundlage von Art. 49 Abs. 1 lit. a DSGVO. Sofern Sie in die Speicherung von Cookies oder in den Zugriff auf Informationen in Ihr Endgerät (z. B. via Device-Fingerprinting) eingewilligt haben, erfolgt die Datenverarbeitung zusätzlich auf Grundlage von § 25 Abs. 1 TTDSG. Die Einwilligung ist jederzeit widerrufbar. Sind Ihre Daten zur Vertragserfüllung oder zur Durchführung vorvertraglicher Maßnahmen erforderlich, verarbeiten wir Ihre Daten auf Grundlage des Art. 6 Abs. 1 lit. b DSGVO. Des Weiteren verarbeiten wir Ihre Daten, sofern diese zur Erfüllung einer rechtlichen Verpflichtung erforderlich sind auf Grundlage von Art. 6 Abs. 1 lit. c DSGVO. Die Datenverarbeitung kann ferner auf Grundlage unseres berechtigten Interesses nach Art. 6 Abs. 1 lit. f DSGVO erfolgen. Über die jeweils im Einzelfall einschlägigen Rechtsgrundlagen wird in den folgenden Absätzen dieser Datenschutzerklärung informiert.\n\nWierruf Ihrer Einwilligung zur Datenverarbeitung\nViele Datenverarbeitungsvorgänge sind nur mit Ihrer ausdrücklichen Einwilligung möglich. Sie können eine bereits erteilte Einwilligung jederzeit widerrufen. Die Rechtmäßigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unberührt.\n\nWiderspruchsrecht gegen die Datenerhebung in besonderen Fällen sowie gegen Direktwerbung (Art. 21 DSGVO)\n\nWenn die Datenverarbeitung auf Grundlage von art. 6 abs. 1 lit. E oder f DSGVO erfolgt, haben sie jederzeit das recht, aus Gründen, die sich aus ihrer besonderen Situation ergeben, gegen die Verarbeitung ihrer personenbezogenen daten Widerspruch einzulegen; dies gilt auch für ein auf diese Bestimmungen gestütztes Profiling. Die jeweilige Rechtsgrundlage, auf denen eine Verarbeitung beruht, entnehmen sie dieser Datenschutzerklärung. Wenn sie Widerspruch einlegen, werden wir ihre betroffenen personenbezogenen daten nicht mehr verarbeiten, es sei denn, wir können zwingende schutzwürdige Gründe für die Verarbeitung nachweisen, die ihre Interessen, Rechte und Freiheiten überwiegen oder die Verarbeitung dient der Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen (Widerspruch nach art. 21 abs. 1 DSGVO).\nWerden ihre personenbezogenen daten verarbeitet, um Direktwerbung zu betreiben, so haben sie das recht, jederzeit Widerspruch gegen die Verarbeitung sie betreffender personenbezogener daten zum Zwecke derartiger Werbung einzulegen; dies gilt auch für das Profiling, soweit es mit solcher Direktwerbung in Verbindung steht. Wenn sie widersprechen, werden ihre personenbezogenen daten anschließend nicht mehr zum Zwecke der Direktwerbung verwendet (Widerspruch nach art. 21 abs. 2 DSGVO).\n\nBeschwerderecht bei der zuständigen Aufsichtsbehörde\nIm Falle von Verstößen gegen die DSGVO steht den Betroffenen ein Beschwerderecht bei einer Aufsichtsbehörde, insbesondere in dem Mitgliedstaat ihres gewöhnlichen Aufenthalts, ihres Arbeitsplatzes oder des Orts des mutmaßlichen Verstoßes zu. Das Beschwerderecht besteht unbeschadet anderweitiger verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe.\n\nRecht auf Datenübertragbarkeit\nSie haben das Recht, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erfüllung eines Vertrags automatisiert verarbeiten, an sich oder an einen Dritten in einem gängigen, maschinenlesbaren Format aushändigen zu lassen. Sofern Sie die direkte Übertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.\nAuskunft, Löschung und Berichtigung\nSie haben im Rahmen der geltenden gesetzlichen Bestimmungen jederzeit das Recht auf unentgeltliche Auskunft über Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empfänger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung oder Löschung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten können Sie sich jederzeit an uns wenden.\n\nRecht auf Einschränkung der Verarbeitung\nSie haben das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Hierzu können Sie sich jederzeit an uns wenden. Das Recht auf Einschränkung der Verarbeitung besteht in folgenden Fällen:\n\nWenn Sie die Richtigkeit Ihrer bei uns gespeicherten personenbezogenen Daten bestreiten, benötigen wir in der Regel Zeit, um dies zu überprüfen. Für die Dauer der Prüfung haben Sie das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\nWenn die Verarbeitung Ihrer personenbezogenen Daten unrechtmäßig geschah/geschieht, können Sie statt der Löschung die Einschränkung der Datenverarbeitung verlangen.\nWenn wir Ihre personenbezogenen Daten nicht mehr benötigen, Sie sie jedoch zur Ausübung, Verteidigung oder Geltendmachung von Rechtsansprüchen benötigen, haben Sie das Recht, statt der Löschung die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\nWenn Sie einen Widerspruch nach Art. 21 Abs. 1 DSGVO eingelegt haben, muss eine Abwägung zwischen Ihren und unseren Interessen vorgenommen werden. Solange noch nicht feststeht, wessen Interessen überwiegen, haben Sie das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\n\nWenn Sie die Verarbeitung Ihrer personenbezogenen Daten eingeschränkt haben, dürfen diese Daten – von ihrer Speicherung abgesehen – nur mit Ihrer Einwilligung oder zur Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen oder zum Schutz der Rechte einer anderen natürlichen oder juristischen Person oder aus Gründen eines wichtigen öffentlichen Interesses der Europäischen Union oder eines Mitgliedstaats verarbeitet werden.\n\nSSL- bzw. TLS-Verschlüsselung\nDiese Seite nutzt aus Sicherheitsgründen und zum Schutz der Übertragung vertraulicher Inhalte, wie zum Beispiel Bestellungen oder Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL- bzw. TLS- Verschlüsselung. Eine verschlüsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von „http://” auf „https://” wechselt und an dem Schloss-Symbol in Ihrer Browserzeile.\nWenn die SSL- bzw. TLS-Verschlüsselung aktiviert ist, können die Daten, die Sie an uns übermitteln, nicht von Dritten mitgelesen werden.\n\nWiderspruch gegen Werbe-E-Mails\nDer Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten zur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wird hiermit widersprochen. Die Betreiber der Seiten behalten sich ausdrücklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-E-Mails, vor.\nDatenerfassung auf dieser Website\nCookies\nUnsere Internetseiten verwenden so genannte „Cookies”. Cookies sind kleine Datenpakete und richten auf Ihrem Endgerät keinen Schaden an. Sie werden entweder vorübergehend für die Dauer einer Sitzung (Session-Cookies) oder dauerhaft (permanente Cookies) auf Ihrem Endgerät gespeichert. Session-Cookies werden nach Ende Ihres Besuchs automatisch gelöscht. Permanente Cookies bleiben auf Ihrem Endgerät gespeichert, bis Sie diese selbst löschen oder eine automatische Löschung durch Ihren Webbrowser erfolgt.\nCookies können von uns (First-Party-Cookies) oder von Drittunternehmen stammen (sog. Third-Party- Cookies). Third-Party-Cookies ermöglichen die Einbindung bestimmter Dienstleistungen von Drittunternehmen innerhalb von Webseiten (z. B. Cookies zur Abwicklung von Zahlungsdienstleistungen).\nCookies haben verschiedene Funktionen. Zahlreiche Cookies sind technisch notwendig, da bestimmte Webseitenfunktionen ohne diese nicht funktionieren würden (z. B. die Warenkorbfunktion oder die Anzeige von Videos). Andere Cookies können zur Auswertung des Nutzerverhaltens oder zu Werbezwecken verwendet werden.\nCookies, die zur Durchführung des elektronischen Kommunikationsvorgangs, zur Bereitstellung bestimmter, von Ihnen erwünschter Funktionen (z. B. für die Warenkorbfunktion) oder zur Optimierung der Website (z. B. Cookies zur Messung des Webpublikums) erforderlich sind (notwendige Cookies), werden auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO gespeichert, sofern keine andere Rechtsgrundlage angegeben wird. Der Websitebetreiber hat ein berechtigtes Interesse an der Speicherung von notwendigen Cookies zur technisch fehlerfreien und optimierten Bereitstellung seiner Dienste. Sofern eine Einwilligung zur Speicherung von Cookies und vergleichbaren Wiedererkennungstechnologien abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage dieser Einwilligung (Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG); die Einwilligung ist jederzeit widerrufbar.\nSie können Ihren Browser so einstellen, dass Sie über das Setzen von Cookies informiert werden und Cookies nur im Einzelfall erlauben, die Annahme von Cookies für bestimmte Fälle oder generell ausschließen sowie das automatische Löschen der Cookies beim Schließen des Browsers aktivieren. Bei der Deaktivierung von Cookies kann die Funktionalität dieser Website eingeschränkt sein.\nWelche Cookies und Dienste auf dieser Website eingesetzt werden, können Sie dieser Datenschutzerklärung entnehmen."
  },
  {
    "objectID": "contact.html#redaktionell-verantwortlich",
    "href": "contact.html#redaktionell-verantwortlich",
    "title": "Imprint",
    "section": "Redaktionell verantwortlich",
    "text": "Redaktionell verantwortlich\nMalte Hückstädt\nSchelfmarkt 1\n19055 Schwerin"
  },
  {
    "objectID": "contact.html#datenschutzerklärung",
    "href": "contact.html#datenschutzerklärung",
    "title": "Imprint",
    "section": "Datenschutzerklärung",
    "text": "Datenschutzerklärung\nDie folgenden Hinweise geben einen einfachen Überblick darüber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie persönlich identifiziert werden können. Ausführliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgeführten Datenschutzerklärung."
  },
  {
    "objectID": "contact.html#datenerfassung-auf-dieser-website",
    "href": "contact.html#datenerfassung-auf-dieser-website",
    "title": "Imprint",
    "section": "Datenerfassung auf dieser Website",
    "text": "Datenerfassung auf dieser Website\nWer ist verantwortlich für die Datenerfassung auf dieser Website?\nDie Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten können Sie dem Abschnitt “Hinweis zur Verantwortlichen Stelle” in dieser Datenschutzerklärung entnehmen.\n\nWie erfassen wir Ihre Daten?\nIhre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z. B. um Daten handeln, die Sie in ein Kontaktformular eingeben.\nAndere Daten werden automatisch oder nach Ihrer Einwilligung beim Besuch der Website durch unsere IT- Systeme erfasst. Das sind vor allem technische Daten (z. B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs). Die Erfassung dieser Daten erfolgt automatisch, sobald Sie diese Website betreten.\n\nWofür nutzen wir Ihre Daten?\nEin Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gewährleisten. Andere Daten können zur Analyse Ihres Nutzerverhaltens verwendet werden.\n\nWelche Rechte haben Sie bezüglich Ihrer Daten?\nSie haben jederzeit das Recht, unentgeltlich Auskunft über Herkunft, Empfänger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben außerdem ein Recht, die Berichtigung oder Löschung dieser Daten zu verlangen. Wenn Sie eine Einwilligung zur Datenverarbeitung erteilt haben, können Sie diese Einwilligung jederzeit für die Zukunft widerrufen. Außerdem haben Sie das Recht, unter bestimmten Umständen die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Des Weiteren steht Ihnen ein Beschwerderecht bei der zuständigen Aufsichtsbehörde zu. Hierzu sowie zu weiteren Fragen zum Thema Datenschutz können Sie sich jederzeit an uns wenden."
  },
  {
    "objectID": "contact.html#analyse-tools-und-tools-von-drittanbietern",
    "href": "contact.html#analyse-tools-und-tools-von-drittanbietern",
    "title": "Imprint",
    "section": "Analyse-Tools und Tools von Drittanbietern",
    "text": "Analyse-Tools und Tools von Drittanbietern\nBeim Besuch dieser Website kann Ihr Surf-Verhalten statistisch ausgewertet werden. Das geschieht vor allem mit sogenannten Analyseprogrammen.\nDetaillierte Informationen zu diesen Analyseprogrammen finden Sie in der folgenden Datenschutzerklärung."
  },
  {
    "objectID": "contact.html#hosting",
    "href": "contact.html#hosting",
    "title": "Imprint",
    "section": "Hosting",
    "text": "Hosting\nDiese Website wird extern gehostet. Die personenbezogenen Daten, die auf dieser Website erfasst werden, werden auf den Servern des Hosters / der Hoster gespeichert. Hierbei kann es sich v. a. um IP-Adressen, Kontaktanfragen, Meta- und Kommunikationsdaten, Vertragsdaten, Kontaktdaten, Namen, Websitezugriffe und sonstige Daten, die über eine Website generiert werden, handeln.\nDas externe Hosting erfolgt zum Zwecke der Vertragserfüllung gegenüber unseren potenziellen und bestehenden Kunden (Art. 6 Abs. 1 lit. b DSGVO) und im Interesse einer sicheren, schnellen und effizienten Bereitstellung unseres Online-Angebots durch einen professionellen Anbieter (Art. 6 Abs. 1 lit. f DSGVO). Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endgerät des Nutzers (z. B. Device-Fingerprinting) im Sinne des TTDSG umfasst. Die Einwilligung ist jederzeit widerrufbar.\nUnser(e) Hoster wird bzw. werden Ihre Daten nur insoweit verarbeiten, wie dies zur Erfüllung seiner Leistungspflichten erforderlich ist und unsere Weisungen in Bezug auf diese Daten befolgen.\n\nWir setzen folgende(n) Hoster ein:\nGitHub, Inc.\n88 Colin P Kelly Jr St San Francisco\nCA 94107\nUnited States of America"
  },
  {
    "objectID": "contact.html#allgemeine-hinweise-und-pflichtinformationen",
    "href": "contact.html#allgemeine-hinweise-und-pflichtinformationen",
    "title": "Imprint",
    "section": "Allgemeine Hinweise und Pflichtinformationen",
    "text": "Allgemeine Hinweise und Pflichtinformationen\n\nDatenschutz\nDie Betreiber dieser Seiten nehmen den Schutz Ihrer persönlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend den gesetzlichen Datenschutzvorschriften sowie dieser Datenschutzerklärung.\nWenn Sie diese Website benutzen, werden verschiedene personenbezogene Daten erhoben. Personenbezogene Daten sind Daten, mit denen Sie persönlich identifiziert werden können. Die vorliegende Datenschutzerklärung erläutert, welche Daten wir erheben und wofür wir sie nutzen. Sie erläutert auch, wie und zu welchem Zweck das geschieht. Wir weisen darauf hin, dass die Datenübertragung im Internet (z. B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich.\n\n\nHinweis zur verantwortlichen Stelle\nDie verantwortliche Stelle für die Datenverarbeitung auf dieser Website ist:\nMalte Hückstädt\nSchelfmarkt 1\n19055 Schwerin\nTelefon: +49 176215040464 \nE-Mail: deaddatascientists@gmail.com\n\nVerantwortliche Stelle ist die natürliche oder juristische Person, die allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z. B. Namen, E-Mail-Adressen o. Ä.) entscheidet.\n\n\n\nSpeicherdauer\nSoweit innerhalb dieser Datenschutzerklärung keine speziellere Speicherdauer genannt wurde, verbleiben Ihre personenbezogenen Daten bei uns, bis der Zweck für die Datenverarbeitung entfällt. Wenn Sie ein berechtigtes Löschersuchen geltend machen oder eine Einwilligung zur Datenverarbeitung widerrufen, werden Ihre Daten gelöscht, sofern wir keine anderen rechtlich zulässigen Gründe für die Speicherung Ihrer personenbezogenen Daten haben (z. B. steuer- oder handelsrechtliche Aufbewahrungsfristen); im letztgenannten Fall erfolgt die Löschung nach Fortfall dieser Gründe.\n\n\nAllgemeine Hinweise zu den Rechtsgrundlagen der Datenverarbeitung auf dieser Website\nSofern Sie in die Datenverarbeitung eingewilligt haben, verarbeiten wir Ihre personenbezogenen Daten auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO bzw. Art. 9 Abs. 2 lit. a DSGVO, sofern besondere Datenkategorien nach Art. 9 Abs. 1 DSGVO verarbeitet werden. Im Falle einer ausdrücklichen Einwilligung in die Übertragung personenbezogener Daten in Drittstaaten erfolgt die Datenverarbeitung außerdem auf Grundlage von Art. 49 Abs. 1 lit. a DSGVO. Sofern Sie in die Speicherung von Cookies oder in den Zugriff auf Informationen in Ihr Endgerät (z. B. via Device-Fingerprinting) eingewilligt haben, erfolgt die Datenverarbeitung zusätzlich auf Grundlage von § 25 Abs. 1 TTDSG. Die Einwilligung ist jederzeit widerrufbar. Sind Ihre Daten zur Vertragserfüllung oder zur Durchführung vorvertraglicher Maßnahmen erforderlich, verarbeiten wir Ihre Daten auf Grundlage des Art. 6 Abs. 1 lit. b DSGVO. Des Weiteren verarbeiten wir Ihre Daten, sofern diese zur Erfüllung einer rechtlichen Verpflichtung erforderlich sind auf Grundlage von Art. 6 Abs. 1 lit. c DSGVO. Die Datenverarbeitung kann ferner auf Grundlage unseres berechtigten Interesses nach Art. 6 Abs. 1 lit. f DSGVO erfolgen. Über die jeweils im Einzelfall einschlägigen Rechtsgrundlagen wird in den folgenden Absätzen dieser Datenschutzerklärung informiert.\n\n\nWierruf Ihrer Einwilligung zur Datenverarbeitung\nViele Datenverarbeitungsvorgänge sind nur mit Ihrer ausdrücklichen Einwilligung möglich. Sie können eine bereits erteilte Einwilligung jederzeit widerrufen. Die Rechtmäßigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unberührt.\n\n\nWiderspruchsrecht gegen die Datenerhebung in besonderen Fällen sowie gegen Direktwerbung (Art. 21 DSGVO)\nWenn die Datenverarbeitung auf Grundlage von art. 6 abs. 1 lit. E oder f DSGVO erfolgt, haben sie jederzeit das recht, aus Gründen, die sich aus ihrer besonderen Situation ergeben, gegen die Verarbeitung ihrer personenbezogenen daten Widerspruch einzulegen; dies gilt auch für ein auf diese Bestimmungen gestütztes Profiling. Die jeweilige Rechtsgrundlage, auf denen eine Verarbeitung beruht, entnehmen sie dieser Datenschutzerklärung. Wenn sie Widerspruch einlegen, werden wir ihre betroffenen personenbezogenen daten nicht mehr verarbeiten, es sei denn, wir können zwingende schutzwürdige Gründe für die Verarbeitung nachweisen, die ihre Interessen, Rechte und Freiheiten überwiegen oder die Verarbeitung dient der Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen (Widerspruch nach art. 21 abs. 1 DSGVO).\nWerden ihre personenbezogenen daten verarbeitet, um Direktwerbung zu betreiben, so haben sie das recht, jederzeit Widerspruch gegen die Verarbeitung sie betreffender personenbezogener daten zum Zwecke derartiger Werbung einzulegen; dies gilt auch für das Profiling, soweit es mit solcher Direktwerbung in Verbindung steht. Wenn sie widersprechen, werden ihre personenbezogenen daten anschließend nicht mehr zum Zwecke der Direktwerbung verwendet (Widerspruch nach art. 21 abs. 2 DSGVO).\n\nBeschwerderecht bei der zuständigen Aufsichtsbehörde\nIm Falle von Verstößen gegen die DSGVO steht den Betroffenen ein Beschwerderecht bei einer Aufsichtsbehörde, insbesondere in dem Mitgliedstaat ihres gewöhnlichen Aufenthalts, ihres Arbeitsplatzes oder des Orts des mutmaßlichen Verstoßes zu. Das Beschwerderecht besteht unbeschadet anderweitiger verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe.\n\nRecht auf Datenübertragbarkeit\nSie haben das Recht, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erfüllung eines Vertrags automatisiert verarbeiten, an sich oder an einen Dritten in einem gängigen, maschinenlesbaren Format aushändigen zu lassen. Sofern Sie die direkte Übertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.\nAuskunft, Löschung und Berichtigung\nSie haben im Rahmen der geltenden gesetzlichen Bestimmungen jederzeit das Recht auf unentgeltliche Auskunft über Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empfänger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung oder Löschung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten können Sie sich jederzeit an uns wenden.\n\nRecht auf Einschränkung der Verarbeitung\nSie haben das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Hierzu können Sie sich jederzeit an uns wenden. Das Recht auf Einschränkung der Verarbeitung besteht in folgenden Fällen:\n\nWenn Sie die Richtigkeit Ihrer bei uns gespeicherten personenbezogenen Daten bestreiten, benötigen wir in der Regel Zeit, um dies zu überprüfen. Für die Dauer der Prüfung haben Sie das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\nWenn die Verarbeitung Ihrer personenbezogenen Daten unrechtmäßig geschah/geschieht, können Sie statt der Löschung die Einschränkung der Datenverarbeitung verlangen.\nWenn wir Ihre personenbezogenen Daten nicht mehr benötigen, Sie sie jedoch zur Ausübung, Verteidigung oder Geltendmachung von Rechtsansprüchen benötigen, haben Sie das Recht, statt der Löschung die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\nWenn Sie einen Widerspruch nach Art. 21 Abs. 1 DSGVO eingelegt haben, muss eine Abwägung zwischen Ihren und unseren Interessen vorgenommen werden. Solange noch nicht feststeht, wessen Interessen überwiegen, haben Sie das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\n\nWenn Sie die Verarbeitung Ihrer personenbezogenen Daten eingeschränkt haben, dürfen diese Daten – von ihrer Speicherung abgesehen – nur mit Ihrer Einwilligung oder zur Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen oder zum Schutz der Rechte einer anderen natürlichen oder juristischen Person oder aus Gründen eines wichtigen öffentlichen Interesses der Europäischen Union oder eines Mitgliedstaats verarbeitet werden.\n\nSSL- bzw. TLS-Verschlüsselung\nDiese Seite nutzt aus Sicherheitsgründen und zum Schutz der Übertragung vertraulicher Inhalte, wie zum Beispiel Bestellungen oder Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL- bzw. TLS- Verschlüsselung. Eine verschlüsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von „http://” auf „https://” wechselt und an dem Schloss-Symbol in Ihrer Browserzeile.\nWenn die SSL- bzw. TLS-Verschlüsselung aktiviert ist, können die Daten, die Sie an uns übermitteln, nicht von Dritten mitgelesen werden.\n\nWiderspruch gegen Werbe-E-Mails\nDer Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten zur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wird hiermit widersprochen. Die Betreiber der Seiten behalten sich ausdrücklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-E-Mails, vor.\nDatenerfassung auf dieser Website\nCookies\nUnsere Internetseiten verwenden so genannte „Cookies”. Cookies sind kleine Datenpakete und richten auf Ihrem Endgerät keinen Schaden an. Sie werden entweder vorübergehend für die Dauer einer Sitzung (Session-Cookies) oder dauerhaft (permanente Cookies) auf Ihrem Endgerät gespeichert. Session-Cookies werden nach Ende Ihres Besuchs automatisch gelöscht. Permanente Cookies bleiben auf Ihrem Endgerät gespeichert, bis Sie diese selbst löschen oder eine automatische Löschung durch Ihren Webbrowser erfolgt.\nCookies können von uns (First-Party-Cookies) oder von Drittunternehmen stammen (sog. Third-Party- Cookies). Third-Party-Cookies ermöglichen die Einbindung bestimmter Dienstleistungen von Drittunternehmen innerhalb von Webseiten (z. B. Cookies zur Abwicklung von Zahlungsdienstleistungen).\nCookies haben verschiedene Funktionen. Zahlreiche Cookies sind technisch notwendig, da bestimmte Webseitenfunktionen ohne diese nicht funktionieren würden (z. B. die Warenkorbfunktion oder die Anzeige von Videos). Andere Cookies können zur Auswertung des Nutzerverhaltens oder zu Werbezwecken verwendet werden.\nCookies, die zur Durchführung des elektronischen Kommunikationsvorgangs, zur Bereitstellung bestimmter, von Ihnen erwünschter Funktionen (z. B. für die Warenkorbfunktion) oder zur Optimierung der Website (z. B. Cookies zur Messung des Webpublikums) erforderlich sind (notwendige Cookies), werden auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO gespeichert, sofern keine andere Rechtsgrundlage angegeben wird. Der Websitebetreiber hat ein berechtigtes Interesse an der Speicherung von notwendigen Cookies zur technisch fehlerfreien und optimierten Bereitstellung seiner Dienste. Sofern eine Einwilligung zur Speicherung von Cookies und vergleichbaren Wiedererkennungstechnologien abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage dieser Einwilligung (Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG); die Einwilligung ist jederzeit widerrufbar.\nSie können Ihren Browser so einstellen, dass Sie über das Setzen von Cookies informiert werden und Cookies nur im Einzelfall erlauben, die Annahme von Cookies für bestimmte Fälle oder generell ausschließen sowie das automatische Löschen der Cookies beim Schließen des Browsers aktivieren. Bei der Deaktivierung von Cookies kann die Funktionalität dieser Website eingeschränkt sein.\nWelche Cookies und Dienste auf dieser Website eingesetzt werden, können Sie dieser Datenschutzerklärung entnehmen."
  },
  {
    "objectID": "index.html#social-media",
    "href": "index.html#social-media",
    "title": "About",
    "section": "Social Media",
    "text": "Social Media\nTweets by chernofffaces"
  },
  {
    "objectID": "index_2.html",
    "href": "index_2.html",
    "title": "Recent articles",
    "section": "",
    "text": "Webscraping with Python\n\n\n\n\n\n\n\nPython\n\n\nweb-scraping\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nMalte Hückstädt\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nCorrespondence Analysis\n\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\ndata exploration\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nMalte Hückstädt\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nChernoff Faces\n\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\ndata exploration\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nMalte Hückstädt\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "recent.html",
    "href": "recent.html",
    "title": "Recent articles",
    "section": "",
    "text": "The blog Chernoff faces documents knowledge that is more or less loosely connected to my research activities or PhD. These can be both methodological and purely technical. The main purpose of the documentation is to briefly and clearly document knowledge elements that have already been developed for me personally. A very welcome side effect would be if this documentation of mine made it easier for third parties to work with R, Python, SQL etc. or to understand the application of specific methods. To this purpose, tutorials written in Quartmarkdown are regularly published on the blog, in which various data science technics and software packages are applied. To illustrate this, simple questions are practically worked on methods from frequentist and Bayesian statistics.\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression with Python\n\n\n\n\n\n\n\nPython\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\nMalte Hückstädt\n\n\n8 min\n\n\n\n\n\n\n\n\nK-Nearest Neighbors with R\n\n\n\n\n\n\n\nR\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\nMalte Hückstädt\n\n\n11 min\n\n\n\n\n\n\n\n\nAnalysis Instagram Data with R and Network Analysis\n\n\n\n\n\n\n\nR\n\n\nweb-scraping\n\n\nnetwork analysis\n\n\napify\n\n\ndata visualization\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2023\n\n\nMalte Hückstädt\n\n\n18 min\n\n\n\n\n\n\n\n\nR with GitHub Codespace\n\n\n\n\n\n\n\nR\n\n\ndevelopment environment\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nMalte Hückstädt\n\n\n6 min\n\n\n\n\n\n\n\n\nAnalyse Web of Science-Data with R\n\n\n\n\n\n\n\nR\n\n\nbibliometrics\n\n\ndata analysis\n\n\nnetwork analysis\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2023\n\n\nMalte Hückstädt\n\n\n8 min\n\n\n\n\n\n\n\n\nWebscraping with Python\n\n\n\n\n\n\n\nPython\n\n\nweb-scraping\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nMalte Hückstädt\n\n\n5 min\n\n\n\n\n\n\n\n\nCorrespondence Analysis\n\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\ndata exploration\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nMalte Hückstädt\n\n\n10 min\n\n\n\n\n\n\n\n\nChernoff Faces\n\n\n\n\n\n\n\nR\n\n\ndata visualization\n\n\ndata exploration\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nMalte Hückstädt\n\n\n9 min\n\n\n\n\n\n\n\n\nWebscraping with Python\n\n\n\n\n\n\n\nPython\n\n\nNLP\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nMalte Hückstädt\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Random Multigraphs and Aggregated Triads with Fixed Degrees\n\n\nNew combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/aggregated_triads/index.html",
    "href": "publications/aggregated_triads/index.html",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/aggregated_triads/index.html#abstract",
    "href": "publications/aggregated_triads/index.html#abstract",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "Abstract",
    "text": "Abstract\nRandom multigraphs with fixed degrees are obtained by the configuration model or by so called random stub matching. New combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges. The number of multigraphs on triads is determined for arbitrary degrees, and aggregated triads are shown to be useful for analyzing regular and almost regular multigraphs. Relationships between entropy and complexity are given and numerically illustrated for multigraphs with different number of vertices and specified average and variance for the degrees.\nNetwork Science 6(2), 232-250"
  },
  {
    "objectID": "imprint.html",
    "href": "imprint.html",
    "title": "Imprint",
    "section": "",
    "text": "Angaben gemäß § 5 TMG"
  },
  {
    "objectID": "imprint.html#kontakt",
    "href": "imprint.html#kontakt",
    "title": "Imprint",
    "section": "Kontakt",
    "text": "Kontakt\nMalte Hückstädt\nSchelfmarkt 1\n19055 Schwerin\nTelefon: +49 (0) 176215040464\nTelefax: +49 (0) 511405638\nE-Mail: deaddatascientists@gmail.com"
  },
  {
    "objectID": "imprint.html#redaktionell-verantwortlich",
    "href": "imprint.html#redaktionell-verantwortlich",
    "title": "Imprint",
    "section": "Redaktionell verantwortlich",
    "text": "Redaktionell verantwortlich\nMalte Hückstädt\nSchelfmarkt 1\n19055 Schwerin"
  },
  {
    "objectID": "imprint.html#datenschutzerklärung",
    "href": "imprint.html#datenschutzerklärung",
    "title": "Imprint",
    "section": "Datenschutzerklärung",
    "text": "Datenschutzerklärung\nDie folgenden Hinweise geben einen einfachen Überblick darüber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie persönlich identifiziert werden können. Ausführliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgeführten Datenschutzerklärung."
  },
  {
    "objectID": "imprint.html#datenerfassung-auf-dieser-website",
    "href": "imprint.html#datenerfassung-auf-dieser-website",
    "title": "Imprint",
    "section": "Datenerfassung auf dieser Website",
    "text": "Datenerfassung auf dieser Website\nWer ist verantwortlich für die Datenerfassung auf dieser Website?\nDie Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten können Sie dem Abschnitt “Hinweis zur Verantwortlichen Stelle” in dieser Datenschutzerklärung entnehmen.\n\nWie erfassen wir Ihre Daten?\nIhre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z. B. um Daten handeln, die Sie in ein Kontaktformular eingeben.\nAndere Daten werden automatisch oder nach Ihrer Einwilligung beim Besuch der Website durch unsere IT- Systeme erfasst. Das sind vor allem technische Daten (z. B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs). Die Erfassung dieser Daten erfolgt automatisch, sobald Sie diese Website betreten.\n\nWofür nutzen wir Ihre Daten?\nEin Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gewährleisten. Andere Daten können zur Analyse Ihres Nutzerverhaltens verwendet werden.\n\nWelche Rechte haben Sie bezüglich Ihrer Daten?\nSie haben jederzeit das Recht, unentgeltlich Auskunft über Herkunft, Empfänger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben außerdem ein Recht, die Berichtigung oder Löschung dieser Daten zu verlangen. Wenn Sie eine Einwilligung zur Datenverarbeitung erteilt haben, können Sie diese Einwilligung jederzeit für die Zukunft widerrufen. Außerdem haben Sie das Recht, unter bestimmten Umständen die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Des Weiteren steht Ihnen ein Beschwerderecht bei der zuständigen Aufsichtsbehörde zu. Hierzu sowie zu weiteren Fragen zum Thema Datenschutz können Sie sich jederzeit an uns wenden."
  },
  {
    "objectID": "imprint.html#analyse-tools-und-tools-von-drittanbietern",
    "href": "imprint.html#analyse-tools-und-tools-von-drittanbietern",
    "title": "Imprint",
    "section": "Analyse-Tools und Tools von Drittanbietern",
    "text": "Analyse-Tools und Tools von Drittanbietern\nBeim Besuch dieser Website kann Ihr Surf-Verhalten statistisch ausgewertet werden. Das geschieht vor allem mit sogenannten Analyseprogrammen.\nDetaillierte Informationen zu diesen Analyseprogrammen finden Sie in der folgenden Datenschutzerklärung."
  },
  {
    "objectID": "imprint.html#hosting",
    "href": "imprint.html#hosting",
    "title": "Imprint",
    "section": "Hosting",
    "text": "Hosting\nDiese Website wird extern gehostet. Die personenbezogenen Daten, die auf dieser Website erfasst werden, werden auf den Servern des Hosters / der Hoster gespeichert. Hierbei kann es sich v. a. um IP-Adressen, Kontaktanfragen, Meta- und Kommunikationsdaten, Vertragsdaten, Kontaktdaten, Namen, Websitezugriffe und sonstige Daten, die über eine Website generiert werden, handeln.\nDas externe Hosting erfolgt zum Zwecke der Vertragserfüllung gegenüber unseren potenziellen und bestehenden Kunden (Art. 6 Abs. 1 lit. b DSGVO) und im Interesse einer sicheren, schnellen und effizienten Bereitstellung unseres Online-Angebots durch einen professionellen Anbieter (Art. 6 Abs. 1 lit. f DSGVO). Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endgerät des Nutzers (z. B. Device-Fingerprinting) im Sinne des TTDSG umfasst. Die Einwilligung ist jederzeit widerrufbar.\nUnser(e) Hoster wird bzw. werden Ihre Daten nur insoweit verarbeiten, wie dies zur Erfüllung seiner Leistungspflichten erforderlich ist und unsere Weisungen in Bezug auf diese Daten befolgen.\n\nWir setzen folgende(n) Hoster ein:\nGitHub, Inc.\n88 Colin P Kelly Jr St San Francisco\nCA 94107\nUnited States of America"
  },
  {
    "objectID": "imprint.html#allgemeine-hinweise-und-pflichtinformationen",
    "href": "imprint.html#allgemeine-hinweise-und-pflichtinformationen",
    "title": "Imprint",
    "section": "Allgemeine Hinweise und Pflichtinformationen",
    "text": "Allgemeine Hinweise und Pflichtinformationen\n\nDatenschutz\nDie Betreiber dieser Seiten nehmen den Schutz Ihrer persönlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend den gesetzlichen Datenschutzvorschriften sowie dieser Datenschutzerklärung.\nWenn Sie diese Website benutzen, werden verschiedene personenbezogene Daten erhoben. Personenbezogene Daten sind Daten, mit denen Sie persönlich identifiziert werden können. Die vorliegende Datenschutzerklärung erläutert, welche Daten wir erheben und wofür wir sie nutzen. Sie erläutert auch, wie und zu welchem Zweck das geschieht. Wir weisen darauf hin, dass die Datenübertragung im Internet (z. B. bei der Kommunikation per E-Mail) Sicherheitslücken aufweisen kann. Ein lückenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht möglich.\n\n\nHinweis zur verantwortlichen Stelle\nDie verantwortliche Stelle für die Datenverarbeitung auf dieser Website ist:\nMalte Hückstädt\nSchelfmarkt 1\n19055 Schwerin\nTelefon: +49 176215040464 \nE-Mail: deaddatascientists@gmail.com\n\nVerantwortliche Stelle ist die natürliche oder juristische Person, die allein oder gemeinsam mit anderen über die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z. B. Namen, E-Mail-Adressen o. Ä.) entscheidet.\n\n\n\nSpeicherdauer\nSoweit innerhalb dieser Datenschutzerklärung keine speziellere Speicherdauer genannt wurde, verbleiben Ihre personenbezogenen Daten bei uns, bis der Zweck für die Datenverarbeitung entfällt. Wenn Sie ein berechtigtes Löschersuchen geltend machen oder eine Einwilligung zur Datenverarbeitung widerrufen, werden Ihre Daten gelöscht, sofern wir keine anderen rechtlich zulässigen Gründe für die Speicherung Ihrer personenbezogenen Daten haben (z. B. steuer- oder handelsrechtliche Aufbewahrungsfristen); im letztgenannten Fall erfolgt die Löschung nach Fortfall dieser Gründe.\n\n\nAllgemeine Hinweise zu den Rechtsgrundlagen der Datenverarbeitung auf dieser Website\nSofern Sie in die Datenverarbeitung eingewilligt haben, verarbeiten wir Ihre personenbezogenen Daten auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO bzw. Art. 9 Abs. 2 lit. a DSGVO, sofern besondere Datenkategorien nach Art. 9 Abs. 1 DSGVO verarbeitet werden. Im Falle einer ausdrücklichen Einwilligung in die Übertragung personenbezogener Daten in Drittstaaten erfolgt die Datenverarbeitung außerdem auf Grundlage von Art. 49 Abs. 1 lit. a DSGVO. Sofern Sie in die Speicherung von Cookies oder in den Zugriff auf Informationen in Ihr Endgerät (z. B. via Device-Fingerprinting) eingewilligt haben, erfolgt die Datenverarbeitung zusätzlich auf Grundlage von § 25 Abs. 1 TTDSG. Die Einwilligung ist jederzeit widerrufbar. Sind Ihre Daten zur Vertragserfüllung oder zur Durchführung vorvertraglicher Maßnahmen erforderlich, verarbeiten wir Ihre Daten auf Grundlage des Art. 6 Abs. 1 lit. b DSGVO. Des Weiteren verarbeiten wir Ihre Daten, sofern diese zur Erfüllung einer rechtlichen Verpflichtung erforderlich sind auf Grundlage von Art. 6 Abs. 1 lit. c DSGVO. Die Datenverarbeitung kann ferner auf Grundlage unseres berechtigten Interesses nach Art. 6 Abs. 1 lit. f DSGVO erfolgen. Über die jeweils im Einzelfall einschlägigen Rechtsgrundlagen wird in den folgenden Absätzen dieser Datenschutzerklärung informiert.\n\n\nWierruf Ihrer Einwilligung zur Datenverarbeitung\nViele Datenverarbeitungsvorgänge sind nur mit Ihrer ausdrücklichen Einwilligung möglich. Sie können eine bereits erteilte Einwilligung jederzeit widerrufen. Die Rechtmäßigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unberührt.\n\n\nWiderspruchsrecht gegen die Datenerhebung in besonderen Fällen sowie gegen Direktwerbung (Art. 21 DSGVO)\nWenn die Datenverarbeitung auf Grundlage von art. 6 abs. 1 lit. E oder f DSGVO erfolgt, haben sie jederzeit das recht, aus Gründen, die sich aus ihrer besonderen Situation ergeben, gegen die Verarbeitung ihrer personenbezogenen daten Widerspruch einzulegen; dies gilt auch für ein auf diese Bestimmungen gestütztes Profiling. Die jeweilige Rechtsgrundlage, auf denen eine Verarbeitung beruht, entnehmen sie dieser Datenschutzerklärung. Wenn sie Widerspruch einlegen, werden wir ihre betroffenen personenbezogenen daten nicht mehr verarbeiten, es sei denn, wir können zwingende schutzwürdige Gründe für die Verarbeitung nachweisen, die ihre Interessen, Rechte und Freiheiten überwiegen oder die Verarbeitung dient der Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen (Widerspruch nach art. 21 abs. 1 DSGVO).\nWerden ihre personenbezogenen daten verarbeitet, um Direktwerbung zu betreiben, so haben sie das recht, jederzeit Widerspruch gegen die Verarbeitung sie betreffender personenbezogener daten zum Zwecke derartiger Werbung einzulegen; dies gilt auch für das Profiling, soweit es mit solcher Direktwerbung in Verbindung steht. Wenn sie widersprechen, werden ihre personenbezogenen daten anschließend nicht mehr zum Zwecke der Direktwerbung verwendet (Widerspruch nach art. 21 abs. 2 DSGVO).\n\nBeschwerderecht bei der zuständigen Aufsichtsbehörde\nIm Falle von Verstößen gegen die DSGVO steht den Betroffenen ein Beschwerderecht bei einer Aufsichtsbehörde, insbesondere in dem Mitgliedstaat ihres gewöhnlichen Aufenthalts, ihres Arbeitsplatzes oder des Orts des mutmaßlichen Verstoßes zu. Das Beschwerderecht besteht unbeschadet anderweitiger verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe.\n\nRecht auf Datenübertragbarkeit\nSie haben das Recht, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erfüllung eines Vertrags automatisiert verarbeiten, an sich oder an einen Dritten in einem gängigen, maschinenlesbaren Format aushändigen zu lassen. Sofern Sie die direkte Übertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.\nAuskunft, Löschung und Berichtigung\nSie haben im Rahmen der geltenden gesetzlichen Bestimmungen jederzeit das Recht auf unentgeltliche Auskunft über Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empfänger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung oder Löschung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten können Sie sich jederzeit an uns wenden.\n\nRecht auf Einschränkung der Verarbeitung\nSie haben das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Hierzu können Sie sich jederzeit an uns wenden. Das Recht auf Einschränkung der Verarbeitung besteht in folgenden Fällen:\n\nWenn Sie die Richtigkeit Ihrer bei uns gespeicherten personenbezogenen Daten bestreiten, benötigen wir in der Regel Zeit, um dies zu überprüfen. Für die Dauer der Prüfung haben Sie das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\nWenn die Verarbeitung Ihrer personenbezogenen Daten unrechtmäßig geschah/geschieht, können Sie statt der Löschung die Einschränkung der Datenverarbeitung verlangen.\nWenn wir Ihre personenbezogenen Daten nicht mehr benötigen, Sie sie jedoch zur Ausübung, Verteidigung oder Geltendmachung von Rechtsansprüchen benötigen, haben Sie das Recht, statt der Löschung die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\nWenn Sie einen Widerspruch nach Art. 21 Abs. 1 DSGVO eingelegt haben, muss eine Abwägung zwischen Ihren und unseren Interessen vorgenommen werden. Solange noch nicht feststeht, wessen Interessen überwiegen, haben Sie das Recht, die Einschränkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen.\n\nWenn Sie die Verarbeitung Ihrer personenbezogenen Daten eingeschränkt haben, dürfen diese Daten – von ihrer Speicherung abgesehen – nur mit Ihrer Einwilligung oder zur Geltendmachung, Ausübung oder Verteidigung von Rechtsansprüchen oder zum Schutz der Rechte einer anderen natürlichen oder juristischen Person oder aus Gründen eines wichtigen öffentlichen Interesses der Europäischen Union oder eines Mitgliedstaats verarbeitet werden.\n\nSSL- bzw. TLS-Verschlüsselung\nDiese Seite nutzt aus Sicherheitsgründen und zum Schutz der Übertragung vertraulicher Inhalte, wie zum Beispiel Bestellungen oder Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL- bzw. TLS- Verschlüsselung. Eine verschlüsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von „http://” auf „https://” wechselt und an dem Schloss-Symbol in Ihrer Browserzeile.\nWenn die SSL- bzw. TLS-Verschlüsselung aktiviert ist, können die Daten, die Sie an uns übermitteln, nicht von Dritten mitgelesen werden.\n\nWiderspruch gegen Werbe-E-Mails\nDer Nutzung von im Rahmen der Impressumspflicht veröffentlichten Kontaktdaten zur Übersendung von nicht ausdrücklich angeforderter Werbung und Informationsmaterialien wird hiermit widersprochen. Die Betreiber der Seiten behalten sich ausdrücklich rechtliche Schritte im Falle der unverlangten Zusendung von Werbeinformationen, etwa durch Spam-E-Mails, vor.\nDatenerfassung auf dieser Website\nCookies\nUnsere Internetseiten verwenden so genannte „Cookies”. Cookies sind kleine Datenpakete und richten auf Ihrem Endgerät keinen Schaden an. Sie werden entweder vorübergehend für die Dauer einer Sitzung (Session-Cookies) oder dauerhaft (permanente Cookies) auf Ihrem Endgerät gespeichert. Session-Cookies werden nach Ende Ihres Besuchs automatisch gelöscht. Permanente Cookies bleiben auf Ihrem Endgerät gespeichert, bis Sie diese selbst löschen oder eine automatische Löschung durch Ihren Webbrowser erfolgt.\nCookies können von uns (First-Party-Cookies) oder von Drittunternehmen stammen (sog. Third-Party- Cookies). Third-Party-Cookies ermöglichen die Einbindung bestimmter Dienstleistungen von Drittunternehmen innerhalb von Webseiten (z. B. Cookies zur Abwicklung von Zahlungsdienstleistungen).\nCookies haben verschiedene Funktionen. Zahlreiche Cookies sind technisch notwendig, da bestimmte Webseitenfunktionen ohne diese nicht funktionieren würden (z. B. die Warenkorbfunktion oder die Anzeige von Videos). Andere Cookies können zur Auswertung des Nutzerverhaltens oder zu Werbezwecken verwendet werden.\nCookies, die zur Durchführung des elektronischen Kommunikationsvorgangs, zur Bereitstellung bestimmter, von Ihnen erwünschter Funktionen (z. B. für die Warenkorbfunktion) oder zur Optimierung der Website (z. B. Cookies zur Messung des Webpublikums) erforderlich sind (notwendige Cookies), werden auf Grundlage von Art. 6 Abs. 1 lit. f DSGVO gespeichert, sofern keine andere Rechtsgrundlage angegeben wird. Der Websitebetreiber hat ein berechtigtes Interesse an der Speicherung von notwendigen Cookies zur technisch fehlerfreien und optimierten Bereitstellung seiner Dienste. Sofern eine Einwilligung zur Speicherung von Cookies und vergleichbaren Wiedererkennungstechnologien abgefragt wurde, erfolgt die Verarbeitung ausschließlich auf Grundlage dieser Einwilligung (Art. 6 Abs. 1 lit. a DSGVO und § 25 Abs. 1 TTDSG); die Einwilligung ist jederzeit widerrufbar.\nSie können Ihren Browser so einstellen, dass Sie über das Setzen von Cookies informiert werden und Cookies nur im Einzelfall erlauben, die Annahme von Cookies für bestimmte Fälle oder generell ausschließen sowie das automatische Löschen der Cookies beim Schließen des Browsers aktivieren. Bei der Deaktivierung von Cookies kann die Funktionalität dieser Website eingeschränkt sein.\nWelche Cookies und Dienste auf dieser Website eingesetzt werden, können Sie dieser Datenschutzerklärung entnehmen."
  },
  {
    "objectID": "experience.html#expected-062023-ph.d.-sociology-gottfried-wilhelm-leibniz-university-of-hanover",
    "href": "experience.html#expected-062023-ph.d.-sociology-gottfried-wilhelm-leibniz-university-of-hanover",
    "title": "Experience",
    "section": "02/2019 – (expected) 06/2023: Ph.D. Sociology, Gottfried Wilhelm Leibniz University of Hanover",
    "text": "02/2019 – (expected) 06/2023: Ph.D. Sociology, Gottfried Wilhelm Leibniz University of Hanover\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nPh.D. Sociology\n\n\nUniversity:\nGottfried Wilhelm Leibniz University of Hanover\n\n\nPeriod:\n02/2019 – (expected) 06/2023\n\n\nPlace:\nHanover, DE\n\n\nThesis:\nResearch collaborations - problems and determinants of success\n\n\nGrade:\n-\n\n\n\n\n\n\nDegree:\nM.A. Sociology\n\n\nUniversity:\nBielefeld University\n\n\nPeriod:\n10/2016 – 09/2018\n\n\nPlace:\nBielefeld, DE\n\n\nThesis:\nDas Kunstmuseum – Vom bildungsbürgerlichen Tempel zum sozial diversifizierten Lern- und Erlebnisort? Eine empirische Aktualisierung der Bourdieuschen Kunstsoziologie mittels multipler Faktorenanalyse\n\n\nGrade:\n1.2\n\n\n\n\n\n\nDegree:\nB.A. Social Science\n\n\nUniversity:\nHumboldt University of Berlin\n\n\nPeriod:\n10/2013 – 09/2016\n\n\nPlace:\nBerlin, DE\n\n\nThesis:\nSoziale Ungleichheit und kultureller Geschmack – Eine empirische Überprüfung der Sozialtheorie Pierre Bourdieus mittels der multiplen Korrespondenzanalyse\n\n\nGrade:\n1.4\n\n\n\n\n\n\nDegree:\nB.A. Artistic teacher training\n\n\nUniversity:\nBerlin University of the Arts\n\n\nPeriod:\n10/2012 – 09/2015\n\n\nGrade:\n1.3\n\n\n\n\n\n\nDegree:\nMeisterschüler Fine Arts\n\n\nUniversity:\nWeißensee Academy of Art Berlin\n\n\nPeriod:\n10/2009 – 09/2010\n\n\nGrade:\nPassed\n\n\n\n\n\n\nDegree:\nDiploma Fine Arts\n\n\nUniversity:\nWeißensee Academy of Art Berlin\n\n\nPeriod:\n10/2004 – 09/2009\n\n\nGrade:\n1.6\n\n\n\n\n\n\nDegree:\nPre-degree examination Fine Arts\n\n\nUniversity:\nHanover University of Applied Sciences and Arts\n\n\nPeriod:\n10/2002 – 09/2004\n\n\nGrade:\nPassed"
  },
  {
    "objectID": "experience.html#expected-062023-ph.d.-sociology-luh-hanover",
    "href": "experience.html#expected-062023-ph.d.-sociology-luh-hanover",
    "title": "Experience",
    "section": "02/2019 – (expected) 06/2023: Ph.D. Sociology, LUH Hanover",
    "text": "02/2019 – (expected) 06/2023: Ph.D. Sociology, LUH Hanover\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nPh.D. Sociology\n\n\nUniversity:\nGottfried Wilhelm Leibniz University of Hanover\n\n\nPeriod:\n02/2019 – 06/2023\n\n\nPlace:\nHanover, DE\n\n\nThesis:\nResearch collaborations - problems and determinants of success\n\n\nGrade:\nsumma cum laude"
  },
  {
    "objectID": "experience.html#m.a.-sociology-bielefeld-university",
    "href": "experience.html#m.a.-sociology-bielefeld-university",
    "title": "Experience",
    "section": "10/2016 – 09/2018: M.A. Sociology, Bielefeld University",
    "text": "10/2016 – 09/2018: M.A. Sociology, Bielefeld University\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nM.A. Sociology\n\n\nUniversity:\nBielefeld University\n\n\nPeriod:\n10/2016 – 09/2018\n\n\nPlace:\nBielefeld, DE\n\n\nThesis:\nDas Kunstmuseum – Vom bildungsbürgerlichen Tempel zum sozial diversifizierten Lern- und Erlebnisort? Eine empirische Aktualisierung der Bourdieuschen Kunstsoziologie mittels multipler Faktorenanalyse\n\n\nGrade:\n1.2"
  },
  {
    "objectID": "experience.html#b.a.-sociology-bielefeld-university",
    "href": "experience.html#b.a.-sociology-bielefeld-university",
    "title": "Experience",
    "section": "10/2013 – 09/2016: B.A. Sociology, Bielefeld University",
    "text": "10/2013 – 09/2016: B.A. Sociology, Bielefeld University\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nB.A. Social Science\n\n\nUniversity:\nHumboldt University of Berlin\n\n\nPeriod:\n10/2013 – 09/2016\n\n\nPlace:\nBerlin, DE\n\n\nThesis:\nSoziale Ungleichheit und kultureller Geschmack – Eine empirische Überprüfung der Sozialtheorie Pierre Bourdieus mittels der multiplen Korrespondenzanalyse\n\n\nGrade:\n1.4"
  },
  {
    "objectID": "experience.html#b.a.-artistic-teacher-training",
    "href": "experience.html#b.a.-artistic-teacher-training",
    "title": "Experience",
    "section": "10/2009 – 09/2010: B.A. Artistic teacher training",
    "text": "10/2009 – 09/2010: B.A. Artistic teacher training\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nDiploma Fine Arts\n\n\nUniversity:\nWeißensee Academy of Art Berlin\n\n\nPeriod:\n10/2004 – 09/2009\n\n\nGrade:\n1.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nPre-degree examination Fine Arts\n\n\nUniversity:\nHanover University of Applied Sciences and Arts\n\n\nPeriod:\n10/2002 – 09/2004\n\n\nGrade:\nPassed"
  },
  {
    "objectID": "experience.html#b.a.-artistic-teacher-training-1",
    "href": "experience.html#b.a.-artistic-teacher-training-1",
    "title": "Experience",
    "section": "10/2009 – 09/2010: B.A. Artistic teacher training",
    "text": "10/2009 – 09/2010: B.A. Artistic teacher training\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nDiploma Fine Arts\n\n\nUniversity:\nWeißensee Academy of Art Berlin\n\n\nPeriod:\n10/2004 – 09/2009\n\n\nGrade:\n1.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nPre-degree examination Fine Arts\n\n\nUniversity:\nHanover University of Applied Sciences and Arts\n\n\nPeriod:\n10/2002 – 09/2004\n\n\nGrade:\nPassed"
  },
  {
    "objectID": "experience.html#b.a.-sociology-humboldt-university-of-berlin",
    "href": "experience.html#b.a.-sociology-humboldt-university-of-berlin",
    "title": "Experience",
    "section": "10/2013 – 09/2016: B.A. Sociology, Humboldt University of Berlin",
    "text": "10/2013 – 09/2016: B.A. Sociology, Humboldt University of Berlin\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nB.A. Social Science\n\n\nUniversity:\nHumboldt University of Berlin\n\n\nPeriod:\n10/2013 – 09/2016\n\n\nPlace:\nBerlin, DE\n\n\nThesis:\nSoziale Ungleichheit und kultureller Geschmack – Eine empirische Überprüfung der Sozialtheorie Pierre Bourdieus mittels der multiplen Korrespondenzanalyse\n\n\nGrade:\n1.4"
  },
  {
    "objectID": "experience.html#b.a.-artistic-teacher-training-2",
    "href": "experience.html#b.a.-artistic-teacher-training-2",
    "title": "Experience",
    "section": "10/2009 – 09/2010: B.A. Artistic teacher training",
    "text": "10/2009 – 09/2010: B.A. Artistic teacher training\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nDiploma Fine Arts\n\n\nUniversity:\nWeißensee Academy of Art Berlin\n\n\nPeriod:\n10/2004 – 09/2009\n\n\nGrade:\n1.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nPre-degree examination Fine Arts\n\n\nUniversity:\nHanover University of Applied Sciences and Arts\n\n\nPeriod:\n10/2002 – 09/2004\n\n\nGrade:\nPassed"
  },
  {
    "objectID": "experience.html#b.a.-artistic-teacher-training-berlin-university-of-the-arts",
    "href": "experience.html#b.a.-artistic-teacher-training-berlin-university-of-the-arts",
    "title": "Experience",
    "section": "10/2012 – 09/2015: B.A. Artistic teacher training, Berlin University of the Arts",
    "text": "10/2012 – 09/2015: B.A. Artistic teacher training, Berlin University of the Arts\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nB.A. Artistic teacher training\n\n\nUniversity:\nBerlin University of the Arts\n\n\nPeriod:\n10/2012 – 09/2015\n\n\nGrade:\n1.3"
  },
  {
    "objectID": "experience.html#b.a.-sociology-hu-berlin",
    "href": "experience.html#b.a.-sociology-hu-berlin",
    "title": "Experience",
    "section": "10/2013 – 09/2016: B.A. Sociology, HU Berlin",
    "text": "10/2013 – 09/2016: B.A. Sociology, HU Berlin\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nB.A. Social Science\n\n\nUniversity:\nHumboldt University of Berlin\n\n\nPeriod:\n10/2013 – 09/2016\n\n\nPlace:\nBerlin, DE\n\n\nThesis:\nSoziale Ungleichheit und kultureller Geschmack – Eine empirische Überprüfung der Sozialtheorie Pierre Bourdieus mittels der multiplen Korrespondenzanalyse\n\n\nGrade:\n1.4"
  },
  {
    "objectID": "experience.html#b.a.-artistic-teacher-training-udk-berlin",
    "href": "experience.html#b.a.-artistic-teacher-training-udk-berlin",
    "title": "Experience",
    "section": "10/2012 – 09/2015: B.A. Artistic teacher training, UdK Berlin",
    "text": "10/2012 – 09/2015: B.A. Artistic teacher training, UdK Berlin\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nB.A. Artistic teacher training\n\n\nUniversity:\nBerlin University of the Arts\n\n\nPeriod:\n10/2012 – 09/2015\n\n\nGrade:\n1.3"
  },
  {
    "objectID": "experience.html#meisterschüler-fine-arts-kh-berlin",
    "href": "experience.html#meisterschüler-fine-arts-kh-berlin",
    "title": "Experience",
    "section": "10/2009 – 09/2010: Meisterschüler Fine Arts, KH Berlin",
    "text": "10/2009 – 09/2010: Meisterschüler Fine Arts, KH Berlin\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nMeisterschüler Fine Arts\n\n\nUniversity:\nWeißensee Academy of Art Berlin\n\n\nPeriod:\n10/2009 – 09/2010\n\n\nGrade:\nPassed"
  },
  {
    "objectID": "experience.html#diploma-fine-arts-kh-berlin",
    "href": "experience.html#diploma-fine-arts-kh-berlin",
    "title": "Experience",
    "section": "10/2009 – 09/2010: Diploma Fine Arts, KH Berlin",
    "text": "10/2009 – 09/2010: Diploma Fine Arts, KH Berlin\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nDiploma Fine Arts\n\n\nUniversity:\nWeißensee Academy of Art Berlin\n\n\nPeriod:\n10/2004 – 09/2009\n\n\nGrade:\n1.6"
  },
  {
    "objectID": "experience.html#pre-degree-examination-fine-arts-hanover-university-of-applied-sciences-and-arts",
    "href": "experience.html#pre-degree-examination-fine-arts-hanover-university-of-applied-sciences-and-arts",
    "title": "Experience",
    "section": "10/2009 – 09/2010: Pre-degree examination Fine Arts, Hanover University of Applied Sciences and Arts",
    "text": "10/2009 – 09/2010: Pre-degree examination Fine Arts, Hanover University of Applied Sciences and Arts\n\n\n\n\n\n\n\n\n\n\n\nDegree:\nPre-degree examination Fine Arts\n\n\nUniversity:\nHanover University of Applied Sciences and Arts\n\n\nPeriod:\n10/2002 – 09/2004\n\n\nGrade:\nPassed"
  },
  {
    "objectID": "index.html#twitter",
    "href": "index.html#twitter",
    "title": "About",
    "section": "Twitter",
    "text": "Twitter\nI’m still new to Twitter, but I really like the possibility of agile, scientific communication. The often polemical way of communicating on Twitter, on the other hand, is rather alien to me. I am a friend of fact-oriented, academic communication. With this in mind, check out my Twitter account, follow and retweet tweets that seem relevant to you!\n\nTweets by chernofffaces"
  },
  {
    "objectID": "index.html#github",
    "href": "index.html#github",
    "title": "About",
    "section": "GitHub",
    "text": "GitHub\nIn the context of my work as a research assistant at the German Centre for Science and Higher Education Research (DZHW), I have been able to significantly sharpen my profile as a (social) data scientist, which I had already developed during my studies: My everyday dealings with large amounts of data sensitised me early on to the great importance of a reproducible analytical pipeline. I therefore have an adept command of various version control tools, markup and scripting languages, which enables me to work with complex data structures in a reproducible, transparent and efficient manner at all times. For me, the most central tool for a reproducible analytical pipeline is Git(Hub). If you would like to find out more about the progress of my projects, check out my GitHub repositorys."
  },
  {
    "objectID": "index.html#instagram",
    "href": "index.html#instagram",
    "title": "About",
    "section": "Instagram",
    "text": "Instagram\nWhen I’m not working on new Data Scientist topics in my free time, I work on 3D sculptures and animations. My tool of choice is the open source software Blender. I have also recently been working on image production with the help of Artificial Intelligence. Some of my work can be found on the Instagram account of Chernoff-Faces."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "links",
    "section": "",
    "text": "icon: github\n\nhref: https://github.com/maltehueckstaedt\n\nhref: https://orcid.org/0000-0002-0185-4230\n\ntext: “”\n\nhref: https://scholar.google.com/citations?hl=en&user=EJTHqH4AAAAJ\n\ntext: “”\n\nhref: https://www.researchgate.net/profile/Malte-Hueckstaedt\n\ntext: “”\n\nicon: linkedin\n\nhref: https://de.linkedin.com/in/malte-hückstädt-82aa78208"
  },
  {
    "objectID": "index.html#running",
    "href": "index.html#running",
    "title": "About",
    "section": "Running",
    "text": "Running\nWhen I’m not working on my computer, I’ve been going running in my spare time for about 10 years. The winter asions are usually weak, but in spring, summer and autumn I usually do my rounds regularly. Here are details of one of my recent runs:\n\n\nFor more information you can check my Garmin profile here."
  },
  {
    "objectID": "skills.html#programming-languages",
    "href": "skills.html#programming-languages",
    "title": "Skills",
    "section": "",
    "text": "R\n\n\n    \n\n\n\n\n\n\n\nPython\n\n\n    \n\n\n\n\n\n\n\nMplus"
  },
  {
    "objectID": "skills.html#markup-languages",
    "href": "skills.html#markup-languages",
    "title": "Skills",
    "section": "",
    "text": "(R/Quarto)Markdown\n\n\n    \n\n\n\n\n\n\n\nLaTex"
  },
  {
    "objectID": "skills.html#r-most-used-packages",
    "href": "skills.html#r-most-used-packages",
    "title": "Skills",
    "section": "",
    "text": "readxl\n\n\n    \n\n\n\n\n\n\n\ntidyverse\n\n\n    \n\n\n\n\n\n\n\nknitr\n\n\n    \n\n\n\n\n\n\n\nstringr\n\n\n    \n\n\n\n\n\n\n\nrvest"
  },
  {
    "objectID": "skills.html#regression",
    "href": "skills.html#regression",
    "title": "Skills",
    "section": "",
    "text": "Linear Regression\n\n\n    \n\n\n\n\n\n\n\nOrdinal Regression\n\n\n    \n\n\n\n\n\n\n\nRegression Trees\n\n\n    \n\n\n\n\n\n\n\nMultilevel Analysis\n\n\n    \n\n\n\n\n\n\n\nTime Series Analysis"
  },
  {
    "objectID": "skills.html#regression-1",
    "href": "skills.html#regression-1",
    "title": "Skills",
    "section": "Regression",
    "text": "Regression\n\n\n\n\nLinear Regression\n\n\n    \n\n\n\n\n\n\n\nOrdinal Regression\n\n\n    \n\n\n\n\n\n\n\nRegression Trees\n\n\n    \n\n\n\n\n\n\n\nMultilevel Analysis\n\n\n    \n\n\n\n\n\n\n\nTime Series Analysis"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "You can create Quarto Presentations with the Revealjs format in the applicable DZHW style. The best way to get a feel for the possibilities of Revealjs is to watch this demo presentation:\n\n\n\nIf you prefer to view the demo in a standalone browser you can do that here. Check out the source code for the demo to see how the slides were created.\n\n\nIn the terminal, navigate to the desired working directory. The DZHW-Quarto theme can then be installed with the following command in the terminal:\nquarto install extension maltehueckstaedt/dzhw_revealjs_template\nIf you are unable to install Quarto extensions, you probably should update Quarto.\n\n\n\nAfter you install the template, use the following code on terminal to create a new directory with all files needed:\nquarto use template maltehueckstaedt/dzhw_revealjs_template\nQuarto templates may execute code when documents are rendered. If you do not \ntrust the authors of the template, we recommend that you do not install or \nuse the template.\n ? Do you trust the authors of this template (Y/n) › Yes\n ? Directory name: › WRITE/THE/DIRECTORY/HERE/talk/\n[✓] Downloading\n[✓] Unzipping\n    Found 1 extension.\n[✓] Copying files...\n\nFiles created:\n - LICENSE.md\n - test2.qmd\n - index.html\n - images\n - quarto-dzhw-theme.Rproj\n - actors.js\n - _extensions\n - clean_title_page.html\n - template.html\n - bibliography.bib"
  },
  {
    "objectID": "software.html#how-to-install",
    "href": "software.html#how-to-install",
    "title": "Software",
    "section": "How to install",
    "text": "How to install\nIn the terminal, navigate to the desired working directory. The DZHW-Quarto theme can then be installed with the following command in the terminal:\nquarto install extension maltehueckstaedt/dzhw_revealjs_template\nIf you are unable to install Quarto extensions, you probably should update Quarto."
  },
  {
    "objectID": "software.html#how-to-use-it",
    "href": "software.html#how-to-use-it",
    "title": "Software",
    "section": "How to use it",
    "text": "How to use it\nAfter you install the template, use the following code on terminal to create a new directory with all files needed:\nquarto use template maltehueckstaedt/dzhw_revealjs_template\nQuarto templates may execute code when documents are rendered. If you do not \ntrust the authors of the template, we recommend that you do not install or \nuse the template.\n ? Do you trust the authors of this template (Y/n) › Yes\n ? Directory name: › WRITE/THE/DIRECTORY/HERE/talk/\n[✓] Downloading\n[✓] Unzipping\n    Found 1 extension.\n[✓] Copying files...\n\nFiles created:\n - LICENSE.md\n - test2.qmd\n - index.html\n - images\n - quarto-dzhw-theme.Rproj\n - actors.js\n - _extensions\n - clean_title_page.html\n - template.html\n - bibliography.bib"
  },
  {
    "objectID": "posts/Open_Alex/index.html",
    "href": "posts/Open_Alex/index.html",
    "title": "ScopusR",
    "section": "",
    "text": "Scopus\n\nlibrary(rscopus)\n#usethis::edit_r_environ()\n\n\nlibrary(rscopus)\nres = author_df(last_name = \"Muschelli\", first_name = \"John\", verbose = FALSE, general = FALSE)\n\nWarning: 'entries_to_df' is deprecated.\nUse 'gen_entries_to_df' instead.\nSee help(\"Deprecated\")"
  },
  {
    "objectID": "posts/Open_Alex/index.html#h-index",
    "href": "posts/Open_Alex/index.html#h-index",
    "title": "Open Alex",
    "section": "H-Index",
    "text": "H-Index\n\nhindex <- function(x) {\n    tx <- sort(x, decreasing = T)\n    print(sum(tx >= seq_along(tx)))\n}"
  },
  {
    "objectID": "posts/coming/Scopus/index.html",
    "href": "posts/coming/Scopus/index.html",
    "title": "ScopusR",
    "section": "",
    "text": "Scopus\n\nlibrary(rscopus)\n#usethis::edit_r_environ()\n\n\nlibrary(rscopus)\nres = author_df(last_name = \"Muschelli\", first_name = \"John\", verbose = FALSE, general = FALSE)\n\nWarning: 'entries_to_df' is deprecated.\nUse 'gen_entries_to_df' instead.\nSee help(\"Deprecated\")"
  },
  {
    "objectID": "posts/GitHub_Codespace/index.html",
    "href": "posts/GitHub_Codespace/index.html",
    "title": "R with GitHub Codespace",
    "section": "",
    "text": "A dev container is a Docker-based developer environment that allows developers to run a consistent and isolated development environment on any computer that supports Docker. Dev containers use Docker container technology to provide a standard development environment with all the necessary dependencies and tools needed for a specific project.\nDevelopers can use dev containers to quickly and easily set up a development environment without having to worry about installing and configuring software on their local computer. This saves time and avoids potential problems caused by differences in developers’ system configurations.\nDifferent development environments and programming languages can be set up in dev containers, e.g. Visual Studio Code, PyCharm, Eclipse or IntelliJ, as well as languages such as Python, Java, JavaScript, Ruby, Go or C++. Dev containers also support integration with version control systems such as Git and the ability to include dev container definitions in one’s project repository to ensure that each developer uses a consistent development environment.\nDev container technology is supported by various development environments and platforms, including GitHub Codespaces.\n\n\n\nGitHub Codespaces are a cloud-based development environment offered by GitHub. They allow developers to quickly and easily create, edit and test code in a provisioned virtual machine (VM) without having to set up their local development environment. Codespaces are integrated into the browser and can be accessed from any device with an internet connection.\nCodespaces also allows developers to create a personalised development VM by adding their preferred tools, configurations and dependencies. This allows them to better manage and scale their work by using the same development process across all devices.\nAnother important feature of Codespaces is the ability to collaborate with other developers in real time. Multiple developers can access and collaborate on the same Codespace at the same time, making collaboration on projects easier.\n\n\n\nTo work with R in a GitHub codespace, the dev container Jupyter notebook is a good choice. Jupyter notebook is a web-based interactive development environment that allows users to create and run documents containing code, text, images and visualisations. The notebook format is very popular with data scientists because it provides an easy and intuitive way to perform data analysis, train models and visualise results.\nA Jupyter notebook consists of a sequence of cells that can contain either code or Markdown text. The user can write code in a cell and then execute that cell to display the result of the code. Results can be displayed as text, tables or charts. The user can also write Markdown text in a cell to provide a description of the code, results or analysis.\nJupyter Notebook supports many programming languages, including Python, R, Julia, C++, and others.\nIn academia, particularly in the area of data analysis and modelling, Jupyter Notebook has become a standard tool. The Notebook format facilitates sharing of research and data analysis by allowing the code and results to be stored in a single document. It also has a wide range of extensions and plug-ins that enhance Notebook’s functionality, such as saving Notebooks in different formats like PDF or HTML, or integrating Github to share Notebooks with others."
  },
  {
    "objectID": "posts/GitHub_Codespace/index.html#dev-container",
    "href": "posts/GitHub_Codespace/index.html#dev-container",
    "title": "R with GitHub Codespace",
    "section": "",
    "text": "A dev container is a Docker-based developer environment that allows developers to run a consistent and isolated development environment on any computer that supports Docker. Dev containers use Docker container technology to provide a standard development environment with all the necessary dependencies and tools needed for a specific project.\nDevelopers can use dev containers to quickly and easily set up a development environment without having to worry about installing and configuring software on their local computer. This saves time and avoids potential problems caused by differences in developers’ system configurations.\nDifferent development environments and programming languages can be set up in dev containers, e.g. Visual Studio Code, PyCharm, Eclipse or IntelliJ, as well as languages such as Python, Java, JavaScript, Ruby, Go or C++. Dev containers also support integration with version control systems such as Git and the ability to include dev container definitions in one’s project repository to ensure that each developer uses a consistent development environment.\nDev container technology is supported by various development environments and platforms, including GitHub Codespaces."
  },
  {
    "objectID": "posts/GitHub_Codespace/index.html#github-codespace",
    "href": "posts/GitHub_Codespace/index.html#github-codespace",
    "title": "R with GitHub Codespace",
    "section": "",
    "text": "GitHub Codespaces are a cloud-based development environment offered by GitHub. They allow developers to quickly and easily create, edit and test code in a provisioned virtual machine (VM) without having to set up their local development environment. Codespaces are integrated into the browser and can be accessed from any device with an internet connection.\nCodespaces also allows developers to create a personalised development VM by adding their preferred tools, configurations and dependencies. This allows them to better manage and scale their work by using the same development process across all devices.\nAnother important feature of Codespaces is the ability to collaborate with other developers in real time. Multiple developers can access and collaborate on the same Codespace at the same time, making collaboration on projects easier."
  },
  {
    "objectID": "posts/GitHub_Codespace/index.html#r-and-github-codespace",
    "href": "posts/GitHub_Codespace/index.html#r-and-github-codespace",
    "title": "R with GitHub Codespace",
    "section": "",
    "text": "To work with R in a GitHub codespace, the dev container Jupyter notebook is a good choice. Jupyter notebook is a web-based interactive development environment that allows users to create and run documents containing code, text, images and visualisations. The notebook format is very popular with data scientists because it provides an easy and intuitive way to perform data analysis, train models and visualise results.\nA Jupyter notebook consists of a sequence of cells that can contain either code or Markdown text. The user can write code in a cell and then execute that cell to display the result of the code. Results can be displayed as text, tables or charts. The user can also write Markdown text in a cell to provide a description of the code, results or analysis.\nJupyter Notebook supports many programming languages, including Python, R, Julia, C++, and others.\nIn academia, particularly in the area of data analysis and modelling, Jupyter Notebook has become a standard tool. The Notebook format facilitates sharing of research and data analysis by allowing the code and results to be stored in a single document. It also has a wide range of extensions and plug-ins that enhance Notebook’s functionality, such as saving Notebooks in different formats like PDF or HTML, or integrating Github to share Notebooks with others."
  },
  {
    "objectID": "posts/GitHub_Codespace/index.html#est",
    "href": "posts/GitHub_Codespace/index.html#est",
    "title": "R with GitHub Codespace",
    "section": "",
    "text": "Ist der Codespace wieder hergestellt, können sie eine neue Datei im Codespace namen r-test.ipynb erzeugen.\n\nSie können nun Code schreiben, ausführen und debuggen, als ob Sie eine lokale Entwicklungsumgebung auf Ihrem Computer hätten. Der Vorteil eines Codespaces besteht darin, dass alle Abhängigkeiten und Einstellungen für Ihre Entwicklungsumgebung in der Cloud gespeichert werden, so dass Sie auf jedem Gerät mit Internetzugang darauf zugreifen können, ohne dass Sie eine lokale Installation durchführen müssen."
  },
  {
    "objectID": "posts/GitHub_Codespace/index.html#footnotes",
    "href": "posts/GitHub_Codespace/index.html#footnotes",
    "title": "R with GitHub Codespace",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor easier use, I recommend installing the add-on package `R Tools` in the GitHub workspace.↩︎"
  },
  {
    "objectID": "posts/GitHub_Codespace/index.html#save-files-in-your-github-repository",
    "href": "posts/GitHub_Codespace/index.html#save-files-in-your-github-repository",
    "title": "R with GitHub Codespace",
    "section": "Save files in your GitHub Repository",
    "text": "Save files in your GitHub Repository\nUm Deine Arbeiten im GitHub-Repository zu sichern, kannst Du entweder über die Menüführung rechts im Codespace Commits puschen, oder mit Ctrl+Shift+P/Cmd+Shift+P die command palette öffnen und mit View: Toggle Terminal einen Terminal öffnen. Dort kannst Du anschließend folgendermaßen vorgehen:\nTo save your work in the GitHub repository, you can either push commits via the menu on the right in the codespace, or open the command palette with Ctrl+Shift+P/Cmd+Shift+P and open a terminal with View: Toggle Terminal. You can then proceed as follows:\n\ngit add . (All changes in your working directory and subfolders will be added to the index to be included in the next commit).\ngit commit -m \"commit codespace files\" (All changes in the local Git repository of the codespace are confirmed).\ngit push (All changes in the local Git repository of the codespace are transferred to the repository on the GitHub Git server)."
  },
  {
    "objectID": "posts/Instagram_R/index.html",
    "href": "posts/Instagram_R/index.html",
    "title": "Analysis Instagram Data with R and Network Analysis",
    "section": "",
    "text": "Instagram is one of the most widely used social media platforms in the world where users share photos, videos and other content. Instagram web scraping can extract data from public Instagram profiles that may be of interest for various purposes, such as:\n\nMarketing and advertising: businesses can scan Instagram profiles of influencers promoting their products or services and analyse the number of followers and interactions to find potential partnerships.\nCompetitive analysis: By scraping Instagram profiles of competitors, companies can analyse their activities, content and engagement and draw conclusions on how to improve their own Instagram presence.\nResearch and analysis: Scientists and researchers can collect Instagram data to study trends in online communication and user behaviour.\nSocial media management: Social media managers can use Instagram data to optimise their strategies to increase interactions and engagement on the platform.\n\nHowever, it is important to note that scraping Instagram data can also raise legal and ethical concerns. Instagram has terms of service that allow scraping data from public profiles, but scraping private profiles or selling Instagram data to third parties without users’ consent may violate privacy laws.\n\n\nIt is difficult to scrape Instagram because the platform has strict security measures in place to control and protect access to its data. Instagram has also restricted its API (Application Programming Interface) to prevent data scraping and only give authorised third parties access to certain data.\nAnother reason why it is difficult to scrape Instagram is that the platform regularly changes its data structure and layout to make it harder to access its data. These changes can render web scraping tools and techniques useless as they are no longer able to access or extract the data.\nInstagram has also implemented anti-scraping technologies to block automated access to the platform. These include, for example, captchas embedded on websites to prevent automated bots from accessing the data.\nAlthough there are many Python apps that purport to collect legal scrapeable content from Instagram in an automated way, in my experience most don’t work. Now there are two ways to still collect instagram data:\n\nWrite your own Python script that allows you to scrape information from Instagram’s dynamic website.\nUse ready-made Apify functions.\n\nI chose the second option for reasons of efficiency.\n\n\n\nApify is a cloud-based web scraping and automation platform that enables users to extract, store and analyse data from various websites. The platform offers a user-friendly interface and a powerful API that allows users to quickly and easily create and manage their web scraping and automation projects.\nApify offers a variety of tools and features, including pre-built scrapers optimised for different websites and a flexible scripting language that allows users to create and customise their own scrapers. In addition, Apify offers data extraction, transformation and loading capabilities, as well as auto-scaling that enables users to process large volumes of data quickly and efficiently.\nThe platform is designed for individuals who want to access and use web data for various purposes, such as marketing, research, analytics, price comparison and more. Apify is free of charge, especially for scraping small amounts of data, and is therefore an attractive alternative for private use to the time-consuming writing of your own web scraping scripts.1"
  },
  {
    "objectID": "posts/Bibliometrie_pt1/index.html",
    "href": "posts/Bibliometrie_pt1/index.html",
    "title": "Analyse Web of Science-Data with R",
    "section": "",
    "text": "Coming Soon"
  },
  {
    "objectID": "posts/Instagram_R/index.html#webscraping-instagram",
    "href": "posts/Instagram_R/index.html#webscraping-instagram",
    "title": "Analysis Instagram Data with R and Network Analysis",
    "section": "",
    "text": "It is difficult to scrape Instagram because the platform has strict security measures in place to control and protect access to its data. Instagram has also restricted its API (Application Programming Interface) to prevent data scraping and only give authorised third parties access to certain data.\nAnother reason why it is difficult to scrape Instagram is that the platform regularly changes its data structure and layout to make it harder to access its data. These changes can render web scraping tools and techniques useless as they are no longer able to access or extract the data.\nInstagram has also implemented anti-scraping technologies to block automated access to the platform. These include, for example, captchas embedded on websites to prevent automated bots from accessing the data.\nAlthough there are many Python apps that purport to collect legal scrapeable content from Instagram in an automated way, in my experience most don’t work. Now there are two ways to still collect instagram data:\n\nWrite your own Python script that allows you to scrape information from Instagram’s dynamic website.\nUse ready-made Apify functions.\n\nI chose the second option for reasons of efficiency."
  },
  {
    "objectID": "posts/Instagram_R/index.html#apfiy",
    "href": "posts/Instagram_R/index.html#apfiy",
    "title": "Analysis Instagram Data with R and Network Analysis",
    "section": "",
    "text": "Apify is a cloud-based web scraping and automation platform that enables users to extract, store and analyse data from various websites. The platform offers a user-friendly interface and a powerful API that allows users to quickly and easily create and manage their web scraping and automation projects.\nApify offers a variety of tools and features, including pre-built scrapers optimised for different websites and a flexible scripting language that allows users to create and customise their own scrapers. In addition, Apify offers data extraction, transformation and loading capabilities, as well as auto-scaling that enables users to process large volumes of data quickly and efficiently.\nThe platform is designed for individuals who want to access and use web data for various purposes, such as marketing, research, analytics, price comparison and more. Apify is free of charge, especially for scraping small amounts of data, and is therefore an attractive alternative for private use to the time-consuming writing of your own web scraping scripts.1"
  },
  {
    "objectID": "posts/Instagram_R/index.html#footnotes",
    "href": "posts/Instagram_R/index.html#footnotes",
    "title": "Analysis Instagram Data with R and Network Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEspecially in the context of Instagram, the time-consuming creation of custom web scraping scripts for dynamic websites is not very useful, as they are not very durable due to the high variability of Instagram’s website structures.↩︎\nThe networkD3 package is an R package that provides an implementation of the D3.js library for creating interactive network visualisations. With this package, you can create interactive and engaging network visualisations and export them in HTML format.↩︎\nSince the peer “Nike” has an overwhelming number of followers compared to all other peers, the corresponding node would cover the entire graph. To address this, the number of followers is divided by 40.000.↩︎"
  },
  {
    "objectID": "posts/Instagram_R/index.html#data-collection-1",
    "href": "posts/Instagram_R/index.html#data-collection-1",
    "title": "Analysis Instagram Data with R and Network Analysis",
    "section": "Data collection",
    "text": "Data collection\nThe web scraping process is simple and intuitive after logging in to Apify, takes a moment and requires no further explanation. After the scraping process, the data can be downloaded in various formats. In view of the nesting of the data, I decided to use the json-format. To load the data of the Posts into R, I use the rjson-package.\n\n#install.packages(\"rjson\")\nlibrary(rjson)\nlibrary(tidyverse)\nsb_list &lt;- fromJSON(file=\"./data/sucuk_bratwurst.json\")\n\nThe data entry of professional affiliations is not reliably listed on Instagram and must therefore be entered manually."
  },
  {
    "objectID": "posts/Instagram_R/index.html#data-preparation",
    "href": "posts/Instagram_R/index.html#data-preparation",
    "title": "Analysis Instagram Data with R and Network Analysis",
    "section": "Data preparation",
    "text": "Data preparation\nThe following code loads the list of Instagram posts (sb_list) and extracts the usernames of all tagged people in each post. The specific procedure is as follows:\n\nAn empty list is created (TaggedUsers).\nThe loop iterates through each post in the list sb_list\nExtracts the tagged usernames from the list of posts into the list taggedUsersUsernames\nAnd finally writes them to the TaggedUsers list\n\n\n# Create an empty list to store the usernames of all tagged people for each post\nTaggedUsers &lt;- list()\n\n# Iterate through each list in your main list\nfor (post in sb_list) {\n  \n  # Extract all usernames of the tagged users from the current post list\n  taggedUsersUsernames &lt;- list()\n  for (taggedUser in post$taggedUsers) {\n    taggedUsersUsernames &lt;- c(taggedUsersUsernames, taggedUser$username)\n  }\n  \n  # Add the usernames of all tagged users to the TaggedUsers list\n  TaggedUsers[[length(TaggedUsers) + 1]] &lt;- taggedUsersUsernames\n}\n\nAs I am interested in relationships between network peers of the agency, I remove all posts in which no person or only one person has been tagged.\n\n# Removing lists with less than 2 items\nTaggedUsers &lt;- TaggedUsers %&gt;% \n  discard(~ length(.x) &lt; 2)\n\nThe following creates an empty data frame called df to store information about the relationships between tagged users in Instagram posts. The code goes through each list in the TaggedUsers-list and creates an edge list of all possible combinations of tagged users in each post. The edge list is then converted into a data frame containing the two tagged users and the post they appear in. This is achieved with the as.data.frame()-function.\nThen the column names of the data frame edges_df are changed to “from”, “to” and “post”. The “from”- and “to”-columns contain the user names, while the “post”-column contains the number of the post from which the users were extracted.\nThe rbind()-function adds one rows to the data frame for each instagram post entry in each iteration of the loop. In this way, we generate a data frame that contains the relationships between all tagged users in all Instagram posts.\n\n# Initialisation of an empty data frame\ndf &lt;- data.frame(from = character(),\n                 to = character(),\n                 stringsAsFactors = FALSE)\n\n# Loop through each post\nfor(i in 1:length(TaggedUsers)) {\n  # Create an edge list for this post\n  edges &lt;- t(combn(TaggedUsers[[i]], 2))\n  \n  # Convert the edge list into a data frame object.\n  edges_df &lt;- as.data.frame(edges, stringsAsFactors = FALSE)\n  names(edges_df) &lt;- c(\"from\", \"to\")\n  edges_df$post &lt;- as.character(i)\n\n  # Add the edge list to the overall data frame\n  df &lt;- rbind(df, edges_df)\n}\n\nSince the different users appear in similar or the same constellations in different posts, there are multiple mentions of combinations of people. We would like to count these in a further step. To do this, we group the columns from and to and count the number of resulting grouped combinations of the characteristic expressions on the columns from and to.\n\n# Aggregate the edge list by \"from\", \"to\" and \"post\" and count the frequency\ndf_count &lt;- df %&gt;%\n  group_by(from, to) %&gt;%\n  summarise(weight = n()) %&gt;%\n  ungroup()\n\nThe resulting data frame looks like this:\n\nlibrary(kableExtra)\nkbl(head(df_count))\n\n\n\n\nfrom\nto\nweight\n\n\n\n\nkaleidoscopemagazine\nspaziomaiocchi\n5\n\n\nkaleidoscopemagazine\nhawlinservices\n1\n\n\nkaleidoscopemagazine\ncapsule.global\n3\n\n\nkaleidoscopemagazine\ntorvs\n3\n\n\nkaleidoscopemagazine\ngebrueder\n1\n\n\nkaleidoscopemagazine\nkevkoko\n1\n\n\n\n\n\n\n\nWe are still missing the information on the professional affiliation and the number of the followers of the peers. Since we don’t want to scrape an unnecessarily large amount of information, but only that of the most important peers of the design agency, we need to know the corresponding names of the peers.\nTo reduce the complexity of the subsequent network analysis, we filter out those connections from the data that occur less than three times. In a further step, we create an igraph object with the function graph_from_data_frame() : graph_from_data_frame()is a function in the R package “igraph” that creates a directed or undirected graph from a data frame matrix. The function expects a data frame matrix as input, with the first and second columns of the matrix containing the name of the start node and the name of the target node, respectively. The optional third to nth columns can be used as attribute information for edges.\nTo extract the names of the nodes from the igraph object, we extract the vertices of the graph (V()) and then their names (names()) and transform the resulting vector into a data frame format. We export this with the write.xlsx() function, open the corresponding table in Excel and copy the content of the column ..\n\n# filter\ndata &lt;- df_count %&gt;% filter(weight&gt;3)\n\nlibrary(igraph)\n#to igraph\nigraph_obj &lt;-  graph_from_data_frame(data)\nnode_names &lt;-  igraph_obj %&gt;% V %&gt;% names %&gt;% as.data.frame()\n\nlibrary(xlsx)\n# write.xlsx(node_names, file = \"node_names.xlsx\")\n\nIn a further step, we feed the content of the column into the Instagram Followers Count Scraper. To do this, we press the green “Try for free” button at the top left and then press “Bulk edit”:\n\n\n\n\n\nIn the window that now opens, we copy the content from our Excel file and press “Set”. Then, back in the main window, we press the “save and start” button at the bottom. The scraping process takes a few moments again. We export the results as a json file.\nIn a further step, we load the json file into our working environment with fromJSON(). We create an empty list (user_info) and a for-loop. The latter iteratively goes through the list sb_follower_list and extracts the username and number of followers for each peer of the design agency and writes the corresponding information into a list of data frame lines called user_info. Finally, do.call() is used to combine all data frame lines in user_info into a single data frame called user_info_df.\n\nsb_follower_list &lt;- fromJSON(file=\"./data/sucuk_bratwurst_follower.json\")\n\n# Erstelle eine leere Liste\nuser_info &lt;- list()\n\n# Wende eine Funktion auf jede Liste in der Liste an und speichere die Ergebnisse in der leeren Liste\nfor (i in 1:length(sb_follower_list)) {\n  user_info[[i]] &lt;- data.frame(userName = sb_follower_list[[i]]$userName,\n                               followersCount = sb_follower_list[[i]]$followersCount)\n}\n\n# Kombiniere alle Ergebnisse in einem Dataframe\nuser_info_df &lt;- do.call(rbind, user_info)\n\nSince we want to create our network anacysis with the package network3D2, we gather all the data and put it into the desired form with igraph_to_networkD3().\nIn a first step, we store the information about the peers (nodes) and their connections to each other (edges) in an object called net_3d.\nIn a further step, we bind the information about the number of followers and the type of peers to the nodes.\n\nlibrary(networkD3)\nnet_3d &lt;- igraph_to_networkD3(igraph_obj)\n\nlibrary(xlsx)\ntype &lt;- read.xlsx(\"./data/type.xlsx\",1) %&gt;% select(type)\nnet_3d$nodes$type &lt;- type$type\nnet_3d$nodes &lt;- rownames_to_column(net_3d$nodes)\n\nuser_info_df &lt;- rename(user_info_df, name = userName)\nnet_3d$nodes &lt;- right_join(user_info_df, net_3d$nodes, by = \"name\")\nnet_3d$nodes &lt;- net_3d$nodes %&gt;% mutate(rowname=as.numeric(rowname)) %&gt;% arrange(rowname)"
  },
  {
    "objectID": "posts/Instagram_R/index.html#data-collection",
    "href": "posts/Instagram_R/index.html#data-collection",
    "title": "Analysis Instagram Data with R and Network Analysis",
    "section": "Data collection",
    "text": "Data collection\nThe web scraping process is simple and intuitive after logging in to Apify, takes a moment and requires no further explanation. After the scraping process, the data can be downloaded in various formats. In view of the nesting of the data, I decided to use the json-format. To load the data of the Posts into R, I use the rjson-package.\n\n#install.packages(\"rjson\")\nlibrary(rjson)\nlibrary(tidyverse)\nsb_list &lt;- fromJSON(file=\"./data/sucuk_bratwurst.json\")\n\nThe data entry of professional affiliations is not reliably listed on Instagram and must therefore be entered manually."
  },
  {
    "objectID": "posts/Instagram_R/index.html#analysis",
    "href": "posts/Instagram_R/index.html#analysis",
    "title": "Analysis Instagram Data with R and Network Analysis",
    "section": "Analysis",
    "text": "Analysis\nNow we are ready, we can create a nice interactive graph using the forceNetwork() function. The following meanings should be noted:\n\nThe edge thickness is indicates the frequency of two-way connections.\nThe node size is set by the number of followers of the peers.3\nThe node colour indicates the type of peers.\n\nNote: forceNetwork() animates an interactive visualisation of network graphs. The function uses a physics-based simulation to determine the layout of the network. Here, the nodes are considered as particles that act on each other through forces such as attraction and repulsion and are held in an equilibrated position. The result is an animated and interactive representation of the network that allows the user to interact with the network by hovering, clicking and dragging the nodes.\n\nnet_3d$nodes$followersCount &lt;- net_3d$nodes$followersCount/40000\nforceNetwork(Links = net_3d$links, Nodes = net_3d$nodes,\n             Source = 'source', \n             Target = 'target', \n             NodeID = 'name', \n             Value = \"value\", # Frequency of reciprocal connection\n             Group = 'type', # Peer type\n             opacity = 0.8,\n             Nodesize= \"followersCount\", # Number of followers of the peers\n             legend = T,\n             linkDistance = 80,\n             linkColour = \"#666\",\n             zoom = T,\n             opacityNoHover = .3,\n             colourScale = JS(\"d3.scaleOrdinal(d3.schemeCategory10);\"))\n\n\n\n\n\n\nInterpretation\nWe can see that the design agency sucuk und bratwurst gmbh is connected to different types of peers who have exceedingly different reach on Instagram: On the one hand Nike (biggest, blue node) with 290 million followers and on the other hand many smaller peers (artists, musicians and others) with 1-3 thousand followers. We can also see that the members of sucuk and bratwurst gmbh (red nodes in the centre of the graph) are the central elements of the network, which is only intuitive, since we are after all analysing their Instagram network, of which they themselves form the centre. The graph also shows that sucuk and bratwurst gmbh are predominantly connected to high-reach companies from the textile industry (Nike, Adidas, Alexander Wang, Footlooker) and their employees, which marks the clear, economic orientation of the company. At the same time, they also have connections to more or less successful German musicians (Rin, Yung Hurn), publications (Zeit, Vice) and artists (Julian Sebastian, Igor Botur). In this respect, the network analysis makes it clear that sucuk and bratwurst gmbh are definitely addressed as a young, established design agency, which, if the data is to be believed, has a very high social capital (Bourdieu 1983).\n\nNode degree\nThe node degree indicates the number of edges that are connected to a node. The node degree is an important indicator because it provides information about how many connections a node has in the network (Luke 2015).\n\nd &lt;- degree(igraph_obj) %&gt;% as.data.frame() %&gt;% \n  rename_with(~ \"degree\", .cols = \".\") %&gt;% rownames_to_column(\"name\")\n\nd &lt;- inner_join(d, net_3d$nodes, by = \"name\") %&gt;%\n  arrange(desc(degree)) %&gt;% # \n  mutate(gruppe = factor(type, levels = unique(type))) %&gt;% \n  mutate(name = reorder(name, degree)) %&gt;% \n  top_n(30,degree)\n\nggplot(d, aes(x = name, y = degree, fill=type)) +\n  geom_bar(stat = \"identity\") + \n  labs(x = \"name\", y = \"degree\")+\n  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))\n\n\n\n\nUnsurprisingly, the highest node degree comes from the agency itself (home) and members of the agency itself. The artists nebenallgeier and beediktfff are also highly integrated into the network of sucuk und bratwurst gmbh.\n\n\nCentrality\nCentrality is a measure of the importance of a node in the network. There are different types of centrality, such as degree centrality, closeness centrality or betweenness centrality (Kolaczyk and Csárdi 2020). The choice of the centrality metric depends on the specific question. In the following, Betweenness Centrality is calculated: This measure shows how often a node is on the shortest path between other nodes in the network. Nodes with higher betweenness centrality have greater control over communication in the network (McNulty 2022).\n\ncb &lt;- data.frame(centr_betw=centr_betw(igraph_obj)$res,\n                 name=igraph_obj %&gt;% V %&gt;% names)%&gt;% \n  top_n(10,centr_betw)%&gt;%\n  arrange(desc(centr_betw))\n\nkable(cb)\n\n\n\n\ncentr_betw\nname\n\n\n\n\n561.0491\ndenis_olgac\n\n\n479.8883\nlukasolgac\n\n\n467.2978\nalessandro.belliero\n\n\n301.3423\ndavid__goenner\n\n\n171.9985\nnevenallgeier\n\n\n125.8868\nmax_filesize\n\n\n109.5167\nsucukundbratwurst\n\n\n94.0000\namp_yourself\n\n\n76.5000\nisamayaffrench\n\n\n46.0000\ndivision.recordings\n\n\n\n\n\n\n\nOnce again, we can see that intuitively, the members of sucuk und bratwurst gmbh have the greatest betweenness centrality and thus have greater control over the communication in their network.\n\n\nClustering coefficient\nThe clustering coefficient is a measure of how strongly the neighbours of a node in a network are connected to each other. It therefore measures the probability that, for example, two people who share a friend also become friends, thus forming a closed triangle (Luke 2015).\nA high clustering coefficient indicates that the nodes in a network are organised in strong clusters (groups of nodes that are strongly connected to each other). A low value, on the other hand, indicates that there are few connections between a node’s neighbours and thus the network is less structured (Luke 2015).\nAs a rule, the clustering coefficient lies between 0 and 1. A value of 1 means that all neighbours of a node are connected to each other and thus there is a fully networked cluster. A value of 0 means that there are no connections between the neighbours of a node and thus there is no cluster (Stegbauer 2010).\n\n transitivity(igraph_obj) %&gt;% round(.,2)\n\n[1] 0.36\n\n\nA value of 0.36 therefore means that the network under consideration is relatively strongly structured by relationship clusters.\n\n\nDensity\nIn a network analysis, the density indicates how many edges there are in the network compared to the maximum possible number of edges. A density of 0 means that there are no edges in the network, while a density of 1 means that all possible edges are present in the network. A high density thus indicates more interconnectedness within the network, while a low density indicates less interconnectedness or fragmentation of the network (Luke 2015).\n\n graph.density(igraph_obj) %&gt;% round(.,2)\n\n[1] 0.05\n\n\nA density of 0.05 means that only 5% of all possible edges in the network actually exist. The network under consideration therefore has a relatively low number of connections compared to all possible connections that could exist between the nodes.\n\n\nDiameter\nThe diameter of a network is the longest shortest connection between two nodes in the network. The diameter is an important parameter for the efficiency of a network. A small diameter means that the nodes in the network are connected quickly and efficiently. A large diameter, on the other hand, means that it is more difficult and time-consuming to get from one node to another, which may indicate that the network may not be as effective or well connected as it could be (Luke 2015).\n\n diameter(igraph_obj) %&gt;% round(.,2)\n\n[1] 39\n\n\nA diameter (Diameter) of 39 means that the maximum shortest distance between two nodes in the network is 39 edges (links). This means that there is at least one pair of nodes for which the shortest path between them comprises 39 edges. In other words, when trying to get from one node to another in this network, it takes an average of 39 steps to reach the destination. A high diameter value may indicate low connectivity of the network."
  },
  {
    "objectID": "posts/Bibliometrie_pt1/index.html#the-bibliometrix-package",
    "href": "posts/Bibliometrie_pt1/index.html#the-bibliometrix-package",
    "title": "Analyse Web of Science-Data with R",
    "section": "The bibliometrix-Package",
    "text": "The bibliometrix-Package\nBibliometrix is an R package specifically designed for the bibliometric analysis of scientific publications. The package offers a variety of functions that allow you to perform complex analyses to identify trends in research, recognise the most important authors and journals or visualise networks of co-authors or citations.\nBibliometrix offers a comprehensive data import function that allows you to retrieve data from various sources such as Web of Science, Scopus or PubMed and import them into R.\nBibliometrix features include analysing publication production, identifying key authors and works cited, analysing citation networks, performing co-author and co-citation analyses, identifying thematic clusters and much more.\nBibliometrix is a powerful bibliometrics package that offers a wealth of features to perform complex analyses and visualise research results. It is a valuable tool for librarians, scientists and researchers interested in the bibliometric analysis of scientific publications."
  },
  {
    "objectID": "posts/Bibliometrie_pt1/index.html#analysis",
    "href": "posts/Bibliometrie_pt1/index.html#analysis",
    "title": "Analyse Web of Science-Data with R",
    "section": "Analysis",
    "text": "Analysis\nWe start with the analysis of EXC 1086. We load the package bibliometrix and pass the file EXC_1086.bib to the function convert2df(). The function supports various data formats such as BibTeX, RIS and ISI web-of-science files and can automatically convert them into a structured R data frame. The function also allows you to select only certain variables or fields from the source files and import them into the R data frame.1\n\nlibrary(bibliometrix) \nfile &lt;- \"./data/EXC_1086.bib\"\nEXC_1086 &lt;- convert2df(file = file, format = \"bibtex\")\n\n\nConverting your wos collection into a bibliographic dataframe\n\n\nWarning:\nIn your file, some mandatory metadata are missing. Bibliometrix functions may not work properly!\n\nPlease, take a look at the vignettes:\n- 'Data Importing and Converting' (https://www.bibliometrix.org/vignettes/Data-Importing-and-Converting.html)\n- 'A brief introduction to bibliometrix' (https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html)\n\n\nMissing fields:  CR \nDone!\n\n\nGenerating affiliation field tag AU_UN from C1:  Done!\n\nfile &lt;- \"./data/EXC_115.bib\"\nEXC_115 &lt;- convert2df(file = file, format = \"bibtex\")\n\n\nConverting your wos collection into a bibliographic dataframe\n\n\nWarning:\nIn your file, some mandatory metadata are missing. Bibliometrix functions may not work properly!\n\nPlease, take a look at the vignettes:\n- 'Data Importing and Converting' (https://www.bibliometrix.org/vignettes/Data-Importing-and-Converting.html)\n- 'A brief introduction to bibliometrix' (https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html)\n\n\nMissing fields:  CR \nDone!\n\n\nGenerating affiliation field tag AU_UN from C1:  Done!\n\n\n\nEXC 1086\nThe resulting dataframe contains 384 publications and rows and 59 columns. For an overview of the meaning of the columns, the following PDF is useful: https://www.bibliometrix.org/documents/Field_Tags_bibliometrix.pdf\n\nlibrary(igraph)\n\n# Extract authors from the \"AU\" column\nauthors &lt;- strsplit(as.character(EXC_1086$AU), \";\")\n\n# Extract unique authors\nall_authors &lt;- unique(unlist(authors))\n\n# Create edge list\nedges &lt;- data.frame(from = character(), to = character(), stringsAsFactors = FALSE)\n\nfor (i in 1:length(authors)) {\n  if (length(authors[[i]]) &gt; 1) {\n    pairs &lt;- t(combn(authors[[i]], 2))\n    edge_data &lt;- data.frame(from = pairs[, 1], to = pairs[, 2], stringsAsFactors = FALSE)\n    edges &lt;- rbind(edges, edge_data)\n  }\n}\n\n# Create network graph\nnetwork_graph &lt;- graph_from_data_frame(edges, directed = FALSE)\nlibrary(GGally)\nsimple &lt;- simplify(network_graph)\ng_1086 &lt;-  ggnet2(simple, \n       mode = \"fruchtermanreingold\",  \n       node.size = 1,\n       node.color = \"red\", \n       node.alpha = .3,\n       size = 30,\n       edge.size = .1, \n       edge.color = \"grey\")+\n  ggtitle(\"EXC 1086\")\n\n\n\nEXC 115\nThe resulting dataframe contains 263 publications and rows and 56 columns. For an overview of the meaning of the columns, the following PDF is useful: https://www.bibliometrix.org/documents/Field_Tags_bibliometrix.pdf\n\nlibrary(igraph)\n\n# Extract authors from the \"AU\" column\nauthors &lt;- strsplit(as.character(EXC_115$AU), \";\")\n\n# Extract unique authors\nall_authors &lt;- unique(unlist(authors))\n\n# Create edge list\nedges &lt;- data.frame(from = character(), to = character(), stringsAsFactors = FALSE)\n\nfor (i in 1:length(authors)) {\n  if (length(authors[[i]]) &gt; 1) {\n    pairs &lt;- t(combn(authors[[i]], 2))\n    edge_data &lt;- data.frame(from = pairs[, 1], to = pairs[, 2], stringsAsFactors = FALSE)\n    edges &lt;- rbind(edges, edge_data)\n  }\n}\n\n# Create network graph\nnetwork_graph &lt;- graph_from_data_frame(edges, directed = FALSE)\n\nsimple &lt;- simplify(network_graph)\ng_115 &lt;-  ggnet2(simple, \n       mode = \"fruchtermanreingold\",  \n       node.size = 1,\n       node.color = \"blue\", \n       node.alpha = .3,\n       size = 30,\n       edge.size = .1, \n       edge.color = \"grey\")+\n  ggtitle(\"EXC 115\")\n\nlibrary(cowplot)\nplot_grid(g_115, g_1086, ncol = 2)\n\n\n\n\n\n\n\n\nUm zwei Publikationsnetzwerke zu vergleichen, können Sie verschiedene Kennzahlen und Metriken betrachten, die aus den Netzwerken berechnet werden können. Hier sind einige mögliche Ansätze:\n\nVergleich der Größe und Dichte der Netzwerke: Sie können die Anzahl der Knoten (Autoren) und Kanten (Koautorschaften) in den beiden Netzwerken vergleichen, um zu sehen, wie groß und dicht sie sind.\nVergleich der zentralen Knoten: Sie können die zentralen Knoten (Autoren) in beiden Netzwerken identifizieren und vergleichen, um zu sehen, ob es Überschneidungen gibt oder ob bestimmte Autoren in einem Netzwerk zentraler sind als im anderen.\nVergleich der Clusterstruktur: Sie können die Clusterstruktur in beiden Netzwerken untersuchen und vergleichen, um zu sehen, ob es Unterschiede gibt. Dazu können Sie z.B. die Gemeinschaftsstruktur mit Methoden wie modularity oder Walktrap analysieren.\nVergleich der Netzwerkmetriken: Sie können verschiedene Netzwerkmetriken wie Degree-Zentralität, Closeness-Zentralität oder Betweenness-Zentralität berechnen und vergleichen, um zu sehen, ob es Unterschiede zwischen den beiden Netzwerken gibt.\nVergleich der Koautorschaftsbeziehungen: Sie können die Koautorschaftsbeziehungen in beiden Netzwerken vergleichen, um zu sehen, ob bestimmte Autoren in einem Netzwerk häufiger zusammenarbeiten als im anderen."
  },
  {
    "objectID": "posts/Bibliometrie_pt1/index.html#footnotes",
    "href": "posts/Bibliometrie_pt1/index.html#footnotes",
    "title": "Analyse Web of Science-Data with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSince our Bibfiles do not contain information on cited references, the warning Missing fields: CR is issued.↩︎"
  },
  {
    "objectID": "software.html#themes",
    "href": "software.html#themes",
    "title": "Software",
    "section": "",
    "text": "You can create Quarto Presentations with the Revealjs format in the applicable DZHW style. The best way to get a feel for the possibilities of Revealjs is to watch this demo presentation:\n\n\n\nIf you prefer to view the demo in a standalone browser you can do that here. Check out the source code for the demo to see how the slides were created.\n\n\nIn the terminal, navigate to the desired working directory. The DZHW-Quarto theme can then be installed with the following command in the terminal:\nquarto install extension maltehueckstaedt/dzhw_revealjs_template\nIf you are unable to install Quarto extensions, you probably should update Quarto.\n\n\n\nAfter you install the template, use the following code on terminal to create a new directory with all files needed:\nquarto use template maltehueckstaedt/dzhw_revealjs_template\nQuarto templates may execute code when documents are rendered. If you do not \ntrust the authors of the template, we recommend that you do not install or \nuse the template.\n ? Do you trust the authors of this template (Y/n) › Yes\n ? Directory name: › WRITE/THE/DIRECTORY/HERE/talk/\n[✓] Downloading\n[✓] Unzipping\n    Found 1 extension.\n[✓] Copying files...\n\nFiles created:\n - LICENSE.md\n - test2.qmd\n - index.html\n - images\n - quarto-dzhw-theme.Rproj\n - actors.js\n - _extensions\n - clean_title_page.html\n - template.html\n - bibliography.bib"
  },
  {
    "objectID": "software.html#shinyapps",
    "href": "software.html#shinyapps",
    "title": "Software",
    "section": "ShinyApps",
    "text": "ShinyApps\n\nLocate Your Cluster!\nWith the ShinyApp “Locate Your Cluster” you can check the functionality of your research collaboration in five functionality dimensions (Difference, Relationship, Commitment, Communication, Fairness) and compare your team with other research clusters. The reference point for your data is the information provided by approximately 5000 PIs and spokespersons of DFG research collaborations. Have Fun!\n\n\nPower in the Field of Art School Professors\nThe ShinyApp “Power in the Field of Art School Professors” offers you additional information on the analyses and data of the article”Power in the Field of Art School Professors: The Role of Symbolic, Social, Cultural and Economic Capital” (under review). In this ShinyApp, four tabs are provided:\n\nCloud of the art schools\nPositions of the Professors\nDistribution of the inputvariables\nInputdata\n\nEach tab is accompanied by information and instructions. The ShinyApp is thus simple and extremely intuitive to use."
  },
  {
    "objectID": "experience.html#since-062022-research-associate---dzhw",
    "href": "experience.html#since-062022-research-associate---dzhw",
    "title": "Experience",
    "section": "",
    "text": "Employer:\nGerman Centre for Higher Education Research and Science Studies (DZHW)\n\n\nPeriod:\nSince 06/2022\n\n\nPlace:\nHanover, DE\n\n\nRole:\nResearch Associate\n\n\nProject:\nHierarchies in Cooperative Research (third-party funding-application preparation, DFG, Individual Research Grants)\n\n\nProject content:\nHierarchies have the potential to both enhance and effectiveness of research teams. The research project Hierarchies in Collaborative Research aims to identify the determinants of hierarchies in collaborative research. research, the determinants of whether, why and when different forms of and when different forms of hierarchy can impair the effectiveness of teams. effectiveness of teams (mediated by cooperation problems). impair the effectiveness of teams. The data basis for the study will be the survey data of data from staff members of Collaborative Research Centres and Excellence clusters will be analysed.\n\n\nResponsibilities:\nPhase 1: Preparation of the proposal\n\nDevelopment of a project idea\nDevelopment of a short concept\nReview of the international state of research\nDevelopment of a project concept\n\nPhase 2: Writing the proposal\n\nPresentation of the initial situation\nPresentation of the work programme\nPresentation of the state of research"
  },
  {
    "objectID": "experience.html#research-associate---dzhw",
    "href": "experience.html#research-associate---dzhw",
    "title": "Experience",
    "section": "",
    "text": "Employer:\nGerman Centre for Higher Education Research and Science Studies (DZHW)\n\n\nPeriod:\n01/2022 – 05/2022\n\n\nPlace:\nHanover, DE\n\n\nRole:\nResearch Associate\n\n\nProject:\nDomain Data Protocols for Empirical Educational Research a Contribution to Standardizing and Increasing the Quality of Research Data Management (DDP)\n\n\nProject content:\nThe collaborative project DDP aimed to develop so-called domain data protocols for handling research data in educational research. Domain data protocols are public and referencable sample standard protocols for data management. They are intended to support researchers in empirical educational research in producing quality-assured and re-usable data.\n\n\nResponsibilities:\n\nProofreading of domain data protocols\nPretests von Domain-Data-Protokollen"
  },
  {
    "objectID": "experience.html#research-associate---dzhw-1",
    "href": "experience.html#research-associate---dzhw-1",
    "title": "Experience",
    "section": "",
    "text": "Employer:\nGerman Centre for Higher Education Research and Science Studies (DZHW)\n\n\nPeriod:\n02/2019 – 05/2022\n\n\nPlace:\nHanover, DE\n\n\nRole:\nResearch Associate\n\n\nProject:\nDeterminants and effects of cooperation in homogeneous and heterogeneous research clusters (DEKiF)\n\n\nProject content:\nThe joint project DEKiF investigated the question of which internal cooperation problems occur in (inter-)disciplinary research clusters, which causes and framework conditions are decisive for this and how cooperation problems affect the success of research clusters. At the same time, it was determined which strategies research clusters use to solve or prevent problems that arise.\n\n\nResponsibilities:\n\nNarrowing down and reviewing the state of international research\nIdentification of relevant research gaps\nDevelopment and concretisation of research questions and hypothesis\nPreparation, implementation and evaluation of a large-scale web survey (survey of approx. 15,000 researchers participating in DFG research clusters)\nProgramming and chaining of R-functions for the automated collection of contact data from the target persons of the DEKiF web survey by means of web scraping\nDevelopment of measurement instruments\nCreation of programming templates\nPreparation and implementation of pre-tests\nImplementation and supervision of the field phase\nCleaning and preparation of survey- and metadata\nConducting survey error analyses\nCarrying out descriptive, explorative and advanced, inference statistical analyses\nPreparation of methodological reports\nAnonymisation and preparation of the survey- and metadata for publication of a scientific use file\nPreparation and publication of technical papers\nCompilation and framing of technical papers for a cumulative dissertation publication"
  },
  {
    "objectID": "experience.html#research-associate---dzhw-2",
    "href": "experience.html#research-associate---dzhw-2",
    "title": "Experience",
    "section": "",
    "text": "Employer:\nGerman Centre for Higher Education Research and Science Studies (DZHW)\n\n\nPeriod:\n11/2018 – 01/2019\n\n\nPlace:\nHanover, DE\n\n\nRole:\nResearch Associate\n\n\nProject:\nContinuous operation of the Research Data Centre for Higher Education and Science Research (FDZ-DZHW)\n\n\nResponsibilities:\nEditing and anonymisation of guided interview transcripts of the research project Performance Evaluation in Appointment Procedures - the changing tradition of hiring professors in academia (LiBerTas)\n\n\nPeriod:\n10/2010 – 09/2012\n\n\nRole:\nFreelance artist and graphic designer"
  },
  {
    "objectID": "posts/lin_reg_python/index.html",
    "href": "posts/lin_reg_python/index.html",
    "title": "Linear Regression with Python",
    "section": "",
    "text": "The basic assumption of linear regression is that there is a linear relationship between one or more independent variable(s) and a dependent variable. This means that a proportional change in the independent variables causes a proportional change in the dependent variable. This linear relationship is represented by a regression function, which is mathematically adjusted to minimise the deviation between the observed data points and the predicted values. Minimising the deviation between the observed data points and the predicted values is usually achieved using the Ordinary Least Squares (OLS) method. The aim of the OLS procedure is to estimate the parameters of the regression line so that they best fit the input data (Döring and Bortz 2016).\nOnce estimated, the regression function can be used to make predictions for the dependent variable based on the values of the independent variables. This makes linear regression a powerful tool for identifying and predicting relationships in quantitative data."
  },
  {
    "objectID": "skills.html#languages",
    "href": "skills.html#languages",
    "title": "Skills",
    "section": "",
    "text": "German\n\n\n    \n\n\n\n\n\n\n\nEnglish"
  },
  {
    "objectID": "skills.html#version-control",
    "href": "skills.html#version-control",
    "title": "Skills",
    "section": "",
    "text": "Git(Hub)"
  },
  {
    "objectID": "skills.html#web-applications",
    "href": "skills.html#web-applications",
    "title": "Skills",
    "section": "",
    "text": "Shiny"
  },
  {
    "objectID": "skills.html#distributions",
    "href": "skills.html#distributions",
    "title": "Skills",
    "section": "",
    "text": "Anaconda"
  },
  {
    "objectID": "skills.html#source-code-editors",
    "href": "skills.html#source-code-editors",
    "title": "Skills",
    "section": "",
    "text": "VS Code\n\n\n    \n\n\n\n\n\n\n\nRstudio\n\n\n    \n\n\n\n\n\n\n\nAtom (†)"
  },
  {
    "objectID": "skills.html#other-software",
    "href": "skills.html#other-software",
    "title": "Skills",
    "section": "",
    "text": "Zotero\n\n\n    \n\n\n\n\n\n\n\nyEd\n\n\n    \n\n\n\n\n\n\n\nPandoc\n\n\n    \n\n\n\n\n\n\n\nMS Office\n\n\n    \n\n\n\n\n\n\n\nSQL\n\n\n    \n\n\n\n\n\n\n\nDocker"
  },
  {
    "objectID": "skills.html#classification",
    "href": "skills.html#classification",
    "title": "Skills",
    "section": "",
    "text": "Logistic Regression\n\n\n    \n\n\n\n\n\n\n\nClassification Trees\n\n\n    \n\n\n\n\n\n\n\nSupport Vector Machines"
  },
  {
    "objectID": "skills.html#clustering",
    "href": "skills.html#clustering",
    "title": "Skills",
    "section": "",
    "text": "Partitioning Cluster Analysis\n\n\n    \n\n\n\n\n\n\n\nHierarchical Cluster Analysis\n\n\n    \n\n\n\n\n\n\n\nLatent Class Analysis"
  },
  {
    "objectID": "skills.html#dimensionality-reduction",
    "href": "skills.html#dimensionality-reduction",
    "title": "Skills",
    "section": "",
    "text": "(Multiple) Corres-pondence Analysis\n\n\n    \n\n\n\n\n\n\n\nMultidimensional Scaling\n\n\n    \n\n\n\n\n\n\n\nExploratory/Confirmatory  Factor Analysis\n\n\n    \n\n\n\n\n\n\n\nPrincipal Component Analysis"
  },
  {
    "objectID": "skills.html#ensemble-methods",
    "href": "skills.html#ensemble-methods",
    "title": "Skills",
    "section": "",
    "text": "Random Forest"
  },
  {
    "objectID": "skills.html#other-methods",
    "href": "skills.html#other-methods",
    "title": "Skills",
    "section": "",
    "text": "Structural Equation Modeling\n\n\n    \n\n\n\n\n\n\n\nWeb Scraping\n\n\n    \n\n\n\n\n\n\n\n(Permutational)Analysis of Variance\n\n\n    \n\n\n\n\n\n\n\nGeocoding\n\n\n    \n\n\n\n\n\n\n\nNetwork Analysis\n\n\n    \n\n\n\n\n\n\n\nNatural LanguageProcessing"
  },
  {
    "objectID": "publications.html#journal-artikel-peer-reviewed",
    "href": "publications.html#journal-artikel-peer-reviewed",
    "title": "Publications",
    "section": "",
    "text": "Hückstädt, M. (2023). ‘Ten reasons why research collaborations succeed—a random forest approach’, Scientometrics, 128/3: 1923–50. DOI: https://doi.org/10.1007/s11192-022-04629-7\nWeinmann, C., Hückstädt, M., Meißner, F., & Vowe, G. (2023). ‘How do researchers perceive problems in research collaboration? Results from a large-scale study of German scientists’, Frontiers in Research Metrics and Analytics, 8. DOI: https://doi.org/10.3389/frma.2023.1106482\nHückstädt, M., Leisten, L.M. (2023). Input, process, output - inhibitors and promoters of collaboration Problems (under Review).\nHückstädt, M. (2022). ‘Coopetition between frenemies–interrelations and effects of seven collaboration problems in research clusters’, Scientometrics, 127/9: 5191–224. DOI: 10.1007/s11192-022-04472-w. DOI: https://doi.org/10.1007/s11192-022-04472-w\nKleimann, B., & Hückstädt, M. (2021). Selection criteria in professorial recruiting as indicators of institutional similarity? A comparison of German universities and universities of applied sciences.Quality in Higher Education (online first). https://doi.org/10.1080/13538322.2021.1889760\nKleimann, B., & Hückstädt, M. (2018). Auswahlkriterien in Berufungsverfahren: Universitäten und Fachhochschulen im Vergleich. Beiträge zur Hochschulforschung, 40 (2/2018), 20-47."
  },
  {
    "objectID": "posts/lin_reg_python/index.html#split-data",
    "href": "posts/lin_reg_python/index.html#split-data",
    "title": "Linear Regression with Python",
    "section": "Split Data",
    "text": "Split Data\nThe independent variables (UVs) are stored in X, dropping the column “sales” and the unnamed column “Unnamed: 0”. The dependent variable (AV) we want to predict is stored in y. The data is split into training and testing data, with 20% of the data used for testing. This is done using the function train_test_split(). The argument random_state is used to control random number generation when splitting data into training and test sets. random_state is a number or numerical value used as a so-called”seed” for the random number generator. If you set a specific value for random_state, the division of the data will always be the same each time you run your code. This is useful to ensure that your results are reproducible.\n\nX = mark_data.drop(columns=[\"sales\",'Unnamed: 0'])  # UVs\ny = mark_data[\"sales\"]  # AV\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "posts/lin_reg_python/index.html#fitting-the-linear-regression-model",
    "href": "posts/lin_reg_python/index.html#fitting-the-linear-regression-model",
    "title": "Linear Regression with Python",
    "section": "Fitting the linear regression model",
    "text": "Fitting the linear regression model\nA linear regression model is fitted using the package statsmodels (sm). sm.OLS() stands for Ordinary Least Squares, and fit() fits the model to the training data. Furthermore, the model summary is printed, which provides comprehensive information about the regression model. This includes statistics such as R-squared, F-statistics, and coefficients for each independent variable.\n\nmodel = sm.OLS(y_train, X_train).fit()\nprint(model.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                  sales   R-squared (uncentered):                   0.982\nModel:                            OLS   Adj. R-squared (uncentered):              0.982\nMethod:                 Least Squares   F-statistic:                              2935.\nDate:                Thu, 12 Oct 2023   Prob (F-statistic):                   1.28e-137\nTime:                        12:13:10   Log-Likelihood:                         -365.83\nNo. Observations:                 160   AIC:                                      737.7\nDf Residuals:                     157   BIC:                                      746.9\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nyoutube        0.0531      0.001     36.467      0.000       0.050       0.056\nfacebook       0.2188      0.011     20.138      0.000       0.197       0.240\nnewspaper      0.0239      0.008      3.011      0.003       0.008       0.040\n==============================================================================\nOmnibus:                       11.405   Durbin-Watson:                   1.895\nProb(Omnibus):                  0.003   Jarque-Bera (JB):               15.574\nSkew:                          -0.432   Prob(JB):                     0.000415\nKurtosis:                       4.261   Cond. No.                         13.5\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nInterpreation of the coefficients\nThe YouTube coefficient indicates how a one unit increase in advertising spend in the context of YouTube affects turnover, holding constant the other variables in the model. In this case, the estimated increase in revenue is approximately 0.0531 units for each additional unit of spending on YouTube. This suggests that advertising spend on YouTube has a positive impact on sales.\nThe Facebook coefficient indicates how a one unit increase in ad spend on Facebook affects revenue, holding constant the other variables in the model. The estimated increase in turnover is approximately 0.2188 units for each additional unit of spending on Facebook. This shows that ad spend on Facebook has a stronger positive impact on revenue than ad spend on YouTube.\nThe Newspaper coefficient indicates the impact of a one unit increase in newspaper ad spend on sales, holding the other variables in the model constant. The estimated increase in turnover is about 0.0239 units for each additional unit of expenditure in newspapers. Note that this coefficient is smaller than the coefficients for YouTube and Facebook, suggesting that advertising spend in newspapers has the smallest impact on turnover.\n\n\nChecking the assumptions of linear regression\nA look at the section below the regression coefficients provides us with important information on the prerequisite tests of the regression. Thus, (1) the significant omnibus test indicates a first violation of the prerequisites of linear regression: The residuals of the regression model are not normally distributed.\nThe Durbin-Watson value of 1.895 indicates a slight positive autocorrelation between the residuals, which is normally not critical. However, the Jarque-Bera test and its low p-value indicate that the residuals are not normally distributed Finally, the condition number of 13.5 indicates possible multicollinearity between the independent variables in the model, which could indicate that some of these variables are highly correlated with each other.\nOverall, these metrics signal that some of the model assumptions are not met, so adjustments to the model or transformations of the data may be needed to improve model performance and ensure that the results are stable."
  },
  {
    "objectID": "posts/lin_reg_python/index.html#predictions",
    "href": "posts/lin_reg_python/index.html#predictions",
    "title": "Linear Regression with Python",
    "section": "Predictions",
    "text": "Predictions\nIn order to assess the accuracy and quality of the regression model, (1) the explanatory power, (2) the fit of the predictions of the regression model to the actual data and (3) the average absolute error are calculated.\n\n# Vorhersagen\ny_pred = model.predict(X_test)\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\nr_squared = r2_score(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\n\nprint(f'R-squared: {r_squared}')\n\nR-squared: 0.8542036745015231\n\nprint(f'Mean Squared Error (MSE): {mse}')\n\nMean Squared Error (MSE): 6.626672623118433\n\nprint(f'Root Mean Squared Error (RMSE): {rmse}')\n\nRoot Mean Squared Error (RMSE): 2.5742324337787434\n\nprint(f'Mean Absolute Error (MAE): {mae}')\n\nMean Absolute Error (MAE): 2.0801090639089828\n\n\nThe R-squared value indicates how well the model explains the variation in the dependent variable (turnover). An R² value of 0.8542 means that the model explains about 85.42% of the variance in turnover. This is a good explanatory power and shows that the model fits the turnover data quite well.\nThe MSE is a measure of the mean squared deviation between the observed and predicted values. An MSE of 6.6267 means that the mean squared error between the observed and predicted turnover values is about 6.6267 units. A low MSE is desirable as it indicates a more accurate prediction.\nThe RMSE is the square root of the MSE and has the same units as the dependent variable. An RMSE of 2.5742 means that the average deviation between the observed and predicted turnover values is about 2.5742 units. A lower RMSE indicates that the model’s predictions are more accurate.\nThe MAE is a measure of the average absolute deviation between the observed and predicted values. An MAE of 2.0801 means that the average absolute deviation between the observed and predicted turnover values is approximately 2.0801 units. The MAE is also a measure of the accuracy of the model, with lower values being better.\nIn summary, the results mean that the model performs well in predicting turnover. The R-squared value is high and the error metrics such as MSE, RMSE and MAE are relatively low, indicating that the model’s predictions match the actual data quite well. This indicates that the model is able to predict turnover fairly accurately."
  },
  {
    "objectID": "posts/KNN_R/index.html",
    "href": "posts/KNN_R/index.html",
    "title": "K-Nearest Neighbors with R",
    "section": "",
    "text": "K-Nearest Neighbours (K-NN) (Kramer 2013) is a widely used algorithm in the field of machine learning and data analysis. It is often used for tasks such as classification and regression. The basic approach of the K-NN algorithm is to identify the \\(K\\) nearest neighbours of a given data point in a dataset and make a prediction for the given point based on the classifications or values of these neighbours.\nHere is a basic description of the K-NN algorithm:\nDataset: First, a dataset is required, which consists of a collection of data points. Each data point has features that define its position in a geometric space.\n\nDistance calculation: To find the nearest neighbours for a given data point, a distance metric is used, often the Euclidean distance. This metric measures the distance between the feature vectors of the data points.\nSelection of K: The user must specify a number K, which indicates how many nearest neighbours should be considered. K is a positive integer.\nIdentification of K nearest neighbours: For a given data point, the K data points are selected from the data set that have the shortest distance to the given point.\nPrediction: In a classification task, the class membership of the data point is predicted based on the majority of the K nearest neighbours. This means that the class that occurs most frequently in the K nearest neighbours is selected. In a regression task, the value of the data point is estimated based on the average or weighted averages of the K nearest neighbours.\nEvaluation and fitting: The algorithm can be applied to test data to evaluate its performance. Choosing the right K value can have a significant impact on the results, so it is important to choose K carefully.\n\nK-NN is a simple, non-parametric algorithm that does not require explicit model fitting. However, it is sensitive to the choice of K and can be computationally expensive in large datasets or with many features. Nevertheless, it is a useful algorithm for various use cases, especially when the data distribution is non-linear or complex.\nIn order to apply the K-NN method in an exemplary way, the following question will be examined to find out which influences lead to research teams creating a climate of cooperation characterised by trust to the highest degree. This is a relevant question because it has been repeatedly shown that a climate of cooperation characterised by trust is an important factor for the success of research teams (e.g. Hückstädt 2023)."
  },
  {
    "objectID": "posts/KNN_R/index.html#recoding",
    "href": "posts/KNN_R/index.html#recoding",
    "title": "K-Nearest Neighbors with R",
    "section": "Recoding",
    "text": "Recoding\nWe load the DEKiF-data into the workspace and recode the Trust-Variable (problm_9).1 We code as high trust those cases that indicate that the collaboration within their research team is characterised by trust to a high extent, and as no high trust those cases where this is not the case.\n\nlibrary(tidyverse)\ndf &lt;- read_rds(\"/Volumes/MALTE_USB/dekif_df.rds\")\n\ndf &lt;- df %&gt;% mutate(trust=case_when(\n  problm_9&lt;5 ~ \"no high trust\",\n  problm_9==5 ~ \"high trust\"\n)) %&gt;% mutate(trust=as.factor(trust))\n\ntable(df$trust)\n\n\n   high trust no high trust \n         1590          2022"
  },
  {
    "objectID": "posts/KNN_R/index.html#selection",
    "href": "posts/KNN_R/index.html#selection",
    "title": "K-Nearest Neighbors with R",
    "section": "Selection",
    "text": "Selection\n\ndf_knn &lt;- df %&gt;% select(trust,\n                        n_fachl_zuord,\n                        n_PIs,\n                        entsch_inhlt_knflkt,\n                        entsch_ressrc_knflkt)"
  },
  {
    "objectID": "posts/KNN_R/index.html#data-selection",
    "href": "posts/KNN_R/index.html#data-selection",
    "title": "K-Nearest Neighbors with R",
    "section": "Data Selection",
    "text": "Data Selection\nAs can be seen, in terms of internal trust relationships, DFG teams’ cooperation is similarly often perceived as being characterised by trust to the highest degree as not or less trusting.\nWhich variables could be responsible for the fact that the cooperation climate is not always perceived as trustworthy to the highest degree?\n\n\nDisciplinary heterogeneity\n\nCommunication barriers: Different disciplines have different disciplinary cultures. If team members have difficulty communicating in an understandable way, this can affect trust as misunderstandings and miscommunication can occur.\nDifferences in working methods: Team members from different disciplines may have different ways of working and methods. This can lead to conflict and reduce trust if team members have doubts about the effectiveness or quality of others’ work.\nPrejudices and stereotypes: In interdisciplinary teams, prejudices and stereotypes about certain disciplines might occur, which can affect the trust of team members. If members of a discipline are mistakenly seen as less competent, this can reduce confidence in their abilities (Shrum, Chompalov, and Genuth 2001; Hückstädt 2023).\n\n\n\nNumber of members\n\nCoordination and decision-making: As the number of team members in a team increases, coordination and decision-making can become increasingly complex. Each member may have different ideas, priorities and ways of working, which can lead to conflicts or delays, which in turn reduce the trust relationships among members.\nResponsibilities: Each member has responsibility for certain aspects of the project. However, if the number of members in a team is too high, it can be difficult to establish clear responsibilities and ensure that each team member is performing their tasks effectively. Diffusion of responsibility can induce free-rider behaviour, which can also negatively affect trust relationships between members.\nResource allocation: The number of members can also influence the allocation of resources such as research funds, laboratory resources and research staff. If there are too many members, the limited resources risk being shared inefficiently, which can lead to friction and eventually to a loss of trust (Defila, Di Giulio, and Scheuermann 2006).\n\n\n\nFrictions\nDecisions regarding the distribution of resources or content can lead to conflicts, for example if they are not reached on the basis of consensus. The tensions resulting from unacceptable decisions can lead to a loss of trust in the team if some members are overreached in the allocation of resources without comprehensible justification.\nTo specify the aforementioned input variables in the context of the K-NN, we use the select() function to select the dichothomised trust variable coded above, as well as the number of subjects in a research team, the number of researchers in a research team, the extent of conflicts arising in the context of resource allocation or content-related decisions.\n\ndf_knn &lt;- df %&gt;% select(trust,\n                        n_fachl_zuord,\n                        n_PIs,\n                        entsch_inhlt_knflkt,\n                        entsch_ressrc_knflkt) %&gt;% na.omit()\n\nlibrary(kableExtra)\nkbl(head(df_knn),format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\ntrust\nn_fachl_zuord\nn_PIs\nentsch_inhlt_knflkt\nentsch_ressrc_knflkt\n\n\n\n\nhigh trust\n3\n20\n1\n1\n\n\nno high trust\n4\n37\n2\n2\n\n\nno high trust\n3\n9\n4\n2\n\n\nno high trust\n1\n28\n2\n2\n\n\nhigh trust\n3\n24\n1\n2\n\n\nno high trust\n2\n32\n3\n2"
  },
  {
    "objectID": "posts/KNN_R/index.html#model-evaluation",
    "href": "posts/KNN_R/index.html#model-evaluation",
    "title": "K-Nearest Neighbors with R",
    "section": "Model Evaluation",
    "text": "Model Evaluation\n\npredictions &lt;- predict(best_model, test,type = \"class\")\n# Calculate confusion matrix\ncm &lt;- confusionMatrix(predictions, test$trust)\ncm\n\nConfusion Matrix and Statistics\n\n               Reference\nPrediction      high trust no high trust\n  high trust           146           109\n  no high trust        140           232\n                                          \n               Accuracy : 0.6029          \n                 95% CI : (0.5634, 0.6414)\n    No Information Rate : 0.5439          \n    P-Value [Acc &gt; NIR] : 0.001649        \n                                          \n                  Kappa : 0.1925          \n                                          \n Mcnemar's Test P-Value : 0.057279        \n                                          \n            Sensitivity : 0.5105          \n            Specificity : 0.6804          \n         Pos Pred Value : 0.5725          \n         Neg Pred Value : 0.6237          \n             Prevalence : 0.4561          \n         Detection Rate : 0.2329          \n   Detection Prevalence : 0.4067          \n      Balanced Accuracy : 0.5954          \n                                          \n       'Positive' Class : high trust      \n                                          \n\n\nThe No Information Rate is your best guess given no information beyond the overall distribution of the classes you are trying to predict. In this case, we know from our table that most our applicants (70%) did not apply for an internal role. So our best guess with no other information is to pick the majority class.\nIf we just pick the majority class, we will be correct 70% of the time."
  },
  {
    "objectID": "posts/KNN_R/index.html#footnotes",
    "href": "posts/KNN_R/index.html#footnotes",
    "title": "K-Nearest Neighbors with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee for an overview of the measuring instruments: https://metadata.fdz.dzhw.eu/public/files/instruments/ins-decquant-ins1$-1.0.0/attachments/Codebuch_Questionnaire_Final.EN.pdf↩︎"
  },
  {
    "objectID": "posts/KNN_R/index.html#data-selection-1",
    "href": "posts/KNN_R/index.html#data-selection-1",
    "title": "K-Nearest Neighbors with R",
    "section": "Data Selection",
    "text": "Data Selection\nTo specify the aforementioned input variables in the context of the K-NN, we use the select() function to select the dichothomised trust variable coded above, as well as the number of subjects in a research team, the number of researchers in a research team, the extent of conflicts arising in the context of resource allocation or content-related decisions.\n\ndf_knn &lt;- df %&gt;% select(trust,\n                        n_fachl_zuord,\n                        n_PIs,\n                        entsch_inhlt_knflkt,\n                        entsch_ressrc_knflkt) %&gt;% na.omit()\n\nlibrary(kableExtra)\nkbl(head(df_knn),format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\ntrust\nn_fachl_zuord\nn_PIs\nentsch_inhlt_knflkt\nentsch_ressrc_knflkt\n\n\n\n\nhigh trust\n3\n20\n1\n1\n\n\nno high trust\n4\n37\n2\n2\n\n\nno high trust\n3\n9\n4\n2\n\n\nno high trust\n1\n28\n2\n2\n\n\nhigh trust\n3\n24\n1\n2\n\n\nno high trust\n2\n32\n3\n2\n\n\n\n\n\nSince K-NN is a distance-based algorithm, the results are affected by the scaling of the input variables. To avoid this, we scale our data in the following:\n\ndf_knn &lt;- df_knn %&gt;% mutate_if(is.numeric, scale)\n\ndf_knn %&gt;%\n  select_if(is.numeric) %&gt;%\n  gather() %&gt;% \n  ggplot(aes(x=value)) + \n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"blue\",binwidth = 1.5, alpha=.2)+\n  geom_density(alpha=.2, fill=\"green\", adjust = 3) +\n  facet_wrap(~key, scales = \"free\")+\n    theme_minimal() +\n    labs(x = \"Value\", y = \"Frequency\") +\n    ggtitle(\"Histograms and Density Plots of Numerical Variables\")"
  },
  {
    "objectID": "posts/KNN_R/index.html#recoding-inputvariables",
    "href": "posts/KNN_R/index.html#recoding-inputvariables",
    "title": "K-Nearest Neighbors with R",
    "section": "Recoding Inputvariables",
    "text": "Recoding Inputvariables\nSince K-NN is a distance-based algorithm, the results are affected by the scaling of the input variables. To avoid this, we scale our data in the following and look at the distributions of the input variables using the ggpplot() function.\n\ndf_knn &lt;- df_knn %&gt;% mutate_if(is.numeric, scale)\n\ndf_knn %&gt;%\n  select_if(is.numeric) %&gt;%\n  gather() %&gt;% \n  ggplot(aes(x=value)) + \n  geom_histogram(aes(y=..density..), colour=\"black\", fill=\"blue\",binwidth = 1.5, alpha=.2)+\n  geom_density(alpha=.2, fill=\"green\", adjust = 3) +\n  facet_wrap(~key, scales = \"free\")+\n    theme_minimal() +\n    labs(x = \"Value\", y = \"Frequency\") +\n    ggtitle(\"Histograms and Density Plots of Numerical Variables\")"
  },
  {
    "objectID": "posts/NLP_R/index.html",
    "href": "posts/NLP_R/index.html",
    "title": "Webscraping with Python",
    "section": "",
    "text": "A Bag of Words is a form of vectorized text representation that describes the multiplicity of words in a text corpus. Thereby, a Bag of Words only considers the number, but not the grammar or the order in which words are implemented in a text corpus. This is also the name-giving core property of the Bag of Words: `The model is only interested in whether and in which frequency specific words occur in a text corpus, but not where they are implemented in which grammatical way in a text corpus. Accordingly, the Bag of Words approach is often used to classify documents where the (frequency of) occurrence of specific words can be used as a classification criterion e.g. of documents (Albrecht, Ramachandran, and Winkler 2020)."
  },
  {
    "objectID": "posts/NLP_R/index.html#footnotes",
    "href": "posts/NLP_R/index.html#footnotes",
    "title": "Webscraping with Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe collected data can also be loaded here as .xlsx-file.↩︎\nDiese müssen ggf. separat mit der Funktion nltk.download('stopwords') geladen werden.↩︎"
  },
  {
    "objectID": "posts/NLP_R/NLP_Lession1.html",
    "href": "posts/NLP_R/NLP_Lession1.html",
    "title": "Natural language processing with Python",
    "section": "",
    "text": "A Bag of Words is a form of vectorized text representation that describes the multiplicity of words in a text corpus. Thereby, a Bag of Words only considers the number, but not the grammar or the order in which words are implemented in a text corpus. This is also the name-giving core property of the Bag of Words: `The model is only interested in whether and in which frequency specific words occur in a text corpus, but not where they are implemented in which grammatical way in a text corpus. Accordingly, the Bag of Words approach is often used to classify documents where the (frequency of) occurrence of specific words can be used as a classification criterion e.g. of documents (Albrecht, Ramachandran, and Winkler 2020)."
  },
  {
    "objectID": "posts/NLP_R/NLP_Lession1.html#footnotes",
    "href": "posts/NLP_R/NLP_Lession1.html#footnotes",
    "title": "Natural language processing with Python",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe collected data can also be loaded here as .xlsx-file.↩︎\nDiese müssen ggf. separat mit der Funktion nltk.download('stopwords') geladen werden.↩︎"
  }
]